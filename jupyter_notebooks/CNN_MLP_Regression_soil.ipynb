{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:22:36.808410Z",
     "start_time": "2022-03-12T09:22:36.692299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install soiltexture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T08:10:55.728740Z",
     "start_time": "2022-03-11T08:10:23.548572Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install -c conda-forge --yes --prefix {sys.prefix} h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:57:25.084981Z",
     "start_time": "2022-03-12T08:57:18.236069Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import soiltexture\n",
    "import tsfresh\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xarray as xr\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:57:25.122772Z",
     "start_time": "2022-03-12T08:57:25.100608Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath_ts = r'C:/Users/USER/Desktop/Master_Irrigation/03_GIS/soil_classification/download_ts/grassland_DE/'\n",
    "files = glob(filepath_ts + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:58:42.407946Z",
     "start_time": "2022-03-12T08:58:09.464833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load all files into one Dataframe\n",
    "gdf = pd.concat([gpd.read_file(x, ignore_index=True) for x in files])\n",
    "\n",
    "# Set datetime type\n",
    "gdf['date'] = gdf['date'].astype('datetime64[ns]')\n",
    "gdf['date_y'] = gdf['date_y'].astype('datetime64[ns]')\n",
    "\n",
    "# Add day of year column\n",
    "gdf['day_of_year'] = gdf['date'].dt.day_of_year\n",
    "print(len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with Lucas topsoil properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:58:42.878156Z",
     "start_time": "2022-03-12T08:58:42.523880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load LUCAS TOPSOIL DATABASE\n",
    "df_lts = pd.read_csv(r'C:/Users/USER/Desktop/Master_Irrigation/03_GIS/soil_classification/LTS_grassland_DE.csv').drop(['Unnamed: 0', 'geometry'], axis=1)\n",
    "gdf_lts = gpd.GeoDataFrame(df_lts, geometry=gpd.points_from_xy(df_lts.GPS_LONG, df_lts.GPS_LAT))\n",
    "\n",
    "# Join LTS data to S1, S2, Era5 Data\n",
    "gdf = pd.merge(gdf, gdf_lts, on='POINT_ID', how='left', suffixes=('', '_y')).rename({'geometry_y' : 'geometry'})\n",
    "print(len(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:58:43.217980Z",
     "start_time": "2022-03-12T08:58:43.125258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mask rows without particel size distribution\n",
    "gdf = gdf[(gdf.sand > 0) & (gdf.silt > 0) & (gdf.clay > 0)]\n",
    "print(len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify soil textures by sand, silt , clay content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:59:05.821401Z",
     "start_time": "2022-03-12T08:58:43.463899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create USDA classification\n",
    "USDA_class = list()\n",
    "FAO_class = list()\n",
    "INTERNATIONAL_class = list()\n",
    "ISSS_class = list()\n",
    "for index, row in gdf.iterrows():\n",
    "    USDA_class.append(soiltexture.getTexture(row.sand, row.clay, classification='USDA'))\n",
    "    FAO_class.append(soiltexture.getTexture(row.sand, row.clay, classification='FAO'))\n",
    "    INTERNATIONAL_class.append(soiltexture.getTexture(row.sand, row.clay, classification='INTERNATIONAL'))\n",
    "    ISSS_class.append(soiltexture.getTexture(row.sand, row.clay, classification='ISSS'))\n",
    "                      \n",
    "gdf['USDA'] = USDA_class\n",
    "gdf['FAO'] = FAO_class\n",
    "gdf['INTERNATIONAL'] = INTERNATIONAL_class\n",
    "gdf['ISSS'] = ISSS_class\n",
    "print(len(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:59:06.179092Z",
     "start_time": "2022-03-12T08:59:06.131960Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf['USDA'].value_counts(),gdf['FAO'].value_counts(),gdf['INTERNATIONAL'].value_counts(),gdf['ISSS'].value_counts(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil textures categorical to numerical classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:59:06.727330Z",
     "start_time": "2022-03-12T08:59:06.479684Z"
    }
   },
   "outputs": [],
   "source": [
    "FAO_to_numerical = {'FAO':     {'medium': int(0), 'coarse': int(1), 'fine' : int(2)}}\n",
    "gdf['FAO_nr'] = gdf.replace(FAO_to_numerical)['FAO']\n",
    "gdf['FAO_nr'] = gdf['FAO_nr']\n",
    "\n",
    "USDA_to_numerical = {'USDA':     {'sandy loam': int(0), 'loam': int(1), 'silt loam' : int(2),'loamy sand': int(3), 'silty clay loam': int(4), 'clay loam' : int(5),'sand': int(6), 'clay': int(7), 'silty clay' : int(8), 'sandy clay loam' : int(8)}}\n",
    "gdf['USDA_nr'] = gdf.replace(USDA_to_numerical)['USDA']\n",
    "gdf['USDA_nr'] = gdf['USDA_nr']\n",
    "print(len(gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize locations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pbpython.com/categorical-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T08:59:10.636658Z",
     "start_time": "2022-03-12T08:59:07.049580Z"
    }
   },
   "outputs": [],
   "source": [
    "longitude = list(dict.fromkeys(gdf.geometry.x))\n",
    "latitude = list(dict.fromkeys(gdf.geometry.y))\n",
    "point_id = list(dict.fromkeys(gdf.POINT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:00:22.041829Z",
     "start_time": "2022-03-12T09:00:09.911376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize Points\n",
    "ax = gdf.plot(figsize=(10, 10), alpha=0.5, edgecolor='k', )\n",
    "cx.add_basemap(ax=ax, source=cx.providers.Esri.WorldImagery, crs=gdf.crs, zoom = 'auto')\n",
    "#for x, y, label in zip(longitude, latitude, [str(x) for x in point_id]):\n",
    "#    ax.annotate(label, xy=(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Rainfall data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Für Morning Aquisition also 5 uhr ist 0 day der tag vor dem tag und für abend aquisition also 17 uhr ist 0 day der gleiche tag\n",
    "    0 day\n",
    "    1 day\n",
    "    2 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:08:59.216943Z",
     "start_time": "2022-03-12T09:01:09.419138Z"
    }
   },
   "outputs": [],
   "source": [
    "RADOLAN = xr.open_dataset('D:/jupy_data/precipitation_1km.nc')\n",
    "pp_24h, pp_48h, pp_72h = list(), list(), list()\n",
    "midday = dt.time(12,0)\n",
    "\n",
    "for index, row in gdf.iterrows():\n",
    "    pp_loc = RADOLAN.sel(latitude=row.geometry.y, longitude=row.geometry.x, method='nearest').sel(time=slice(row.date.date() - pd.Timedelta('3d'), row.date.date())).precipitation_1km\n",
    "    if row.date.time() > midday:\n",
    "        try:\n",
    "            pp_24h.append(pp_loc.isel(time=3).values)\n",
    "            pp_48h.append(pp_loc.isel(time=slice(2,4)).sum().values)  \n",
    "            pp_72h.append(pp_loc.isel(time=slice(1,4)).sum().values)  \n",
    "        except:\n",
    "            pp_24h.append(np.nan)\n",
    "            pp_48h.append(np.nan)\n",
    "            pp_72h.append(np.nan)\n",
    "    else:\n",
    "        try:\n",
    "            pp_24h.append(pp_loc.isel(time=2).values)\n",
    "            pp_48h.append(pp_loc.isel(time=slice(1,3)).sum().values)  \n",
    "            pp_72h.append(pp_loc.isel(time=slice(0,3)).sum().values)  \n",
    "        except:\n",
    "            pp_24h.append(np.nan)\n",
    "            pp_48h.append(np.nan)\n",
    "            pp_72h.append(np.nan)\n",
    "gdf['pp_24h'] = pp_24h\n",
    "gdf['pp_48h'] = pp_48h\n",
    "gdf['pp_72h'] = pp_72h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:57:37.957662Z",
     "start_time": "2022-03-12T09:57:34.207890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset Data\n",
    "gdf['longitude'] = gdf['geometry'].x\n",
    "gdf['latitude'] = gdf['geometry'].y\n",
    "df_sub = gdf.loc[:,['POINT_ID','date', 'VV','VH','angle', 'orbit', 'platform', 'NDVI','day_of_year', 'FAO_nr', \n",
    "                          'USDA_nr','minimum_2m_air_temperature','pp_24h', 'pp_48h', 'pp_72h', 'silt', 'clay', 'sand']]\n",
    "\n",
    "df_sub['minimum_2m_air_temperature'] = df_sub['minimum_2m_air_temperature'] -273.15\n",
    "#Drop frozen grounds\n",
    "df_sub = df_sub[df_sub['minimum_2m_air_temperature'] > 1]\n",
    "# Drop rows (axis=0) with na values\n",
    "df_sub.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "# One Hot Encoding\n",
    "df_sub['FAO_nr'] = df_sub['FAO_nr'].astype('int64')\n",
    "df_sub['FAO'] = df_sub['FAO_nr']\n",
    "\n",
    "df_sub['USDA_nr'] = df_sub['USDA_nr'].astype('int64')\n",
    "df_sub['USDA'] = df_sub['USDA_nr']\n",
    "\n",
    "df_sub.loc[:,['pp_24h', 'pp_48h', 'pp_72h']] = df_sub.loc[:,['pp_24h', 'pp_48h', 'pp_72h']].astype('float64')\n",
    "\n",
    "df_sub = pd.DataFrame(pd.get_dummies(df_sub, columns=['platform', 'orbit', 'FAO_nr', 'USDA_nr']))\n",
    "print(len(df_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlizers\n",
    "Maybe not necesarry with 30m mean values from s1 s2 era5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T20:37:57.690869Z",
     "start_time": "2022-03-10T20:37:57.662902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop lower and upper 1% of data to eliminate outliers\n",
    "for column in ['VV', 'VH', 'NDVI']:\n",
    "    df_sub = df_sub[df_sub[column].gt(gdf[column].quantile(0.01)) & df_sub[column].lt(df_sub[column].quantile(0.99))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for all values together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:59:00.992621Z",
     "start_time": "2022-03-12T09:59:00.807662Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "df_sub.loc[:,['VV', 'VH', 'angle', 'NDVI', 'day_of_year', 'minimum_2m_air_temperature','pp_24h', 'pp_48h', 'pp_72h']\n",
    "          ] = scaler.fit_transform(df_sub.loc[:,['VV', 'VH', 'angle', 'NDVI', 'day_of_year', \n",
    "                                                 'minimum_2m_air_temperature','pp_24h', 'pp_48h', 'pp_72h']]) \n",
    "\n",
    "df_sub.dropna(how='any', inplace=True)\n",
    "df_sub.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for percentage values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:59:02.795805Z",
     "start_time": "2022-03-12T09:59:02.764766Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.loc[:,['sand', 'silt', 'clay']] = df_sub.loc[:,['sand', 'silt', 'clay']] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:59:04.800965Z",
     "start_time": "2022-03-12T09:59:04.769924Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for every location indepentently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "df_sub.loc[:,['VV', 'VH', 'angle', 'NDVI', 'day_of_year', 'minimum_2m_air_temperature','pp_24h', 'pp_48h', 'pp_72h']] = df_sub.groupby('POINT_ID')[['VV', 'VH', 'angle', 'NDVI', 'day_of_year', 'minimum_2m_air_temperature','pp_24h', 'pp_48h', 'pp_72h']].agg(scaler.fit_transform()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/structured_data/time_series normalize only with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-07T19:47:13.857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "df_sub['VV'] = df_sub.groupby('POINT_ID')['VV'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['VH'] = df_sub.groupby('POINT_ID')['VH'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['angle'] = df_sub.groupby('POINT_ID')['angle'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['NDVI'] = df_sub.groupby('POINT_ID')['NDVI'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['day_of_year'] = df_sub.groupby('POINT_ID')['day_of_year'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['dewpoint_2m_temperature'] = df_sub.groupby('POINT_ID')['dewpoint_2m_temperature'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['maximum_2m_air_temperature'] = df_sub.groupby('POINT_ID')['maximum_2m_air_temperature'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['mean_2m_air_temperature'] = df_sub.groupby('POINT_ID')['mean_2m_air_temperature'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['minimum_2m_air_temperature'] = df_sub.groupby('POINT_ID')['minimum_2m_air_temperature'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['surface_pressure'] = df_sub.groupby('POINT_ID')['surface_pressure'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['total_precipitation'] = df_sub['total_precipitation'].apply(lambda x: (x - df_sub['total_precipitation'].mean()) / df_sub['total_precipitation'].std())\n",
    "df_sub['u_component_of_wind_10m'] = df_sub.groupby('POINT_ID')['u_component_of_wind_10m'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df_sub['v_component_of_wind_10m'] = df_sub.groupby('POINT_ID')['v_component_of_wind_10m'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "def normalization(x):\n",
    "    mean = x.mean()\n",
    "# Transform to percentage particel distribution\n",
    "#df_sub['sand'] = df_sub['sand'] / 100\n",
    "#df_sub['clay'] = df_sub['clay'] / 100\n",
    "#df_sub['silt'] = df_sub['silt'] / 100\n",
    "df_sub.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot normalized Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:59:19.584385Z",
     "start_time": "2022-03-12T09:59:16.561753Z"
    }
   },
   "outputs": [],
   "source": [
    "df_std = (df_sub.iloc[:,2:11] - df_sub.iloc[:,2:11].mean()) / df_sub.iloc[:,2:11].std()\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(df_sub.iloc[:,2:11].keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:59:35.405178Z",
     "start_time": "2022-03-12T09:59:35.373941Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T12:54:17.029163Z",
     "start_time": "2022-03-12T12:54:16.998135Z"
    }
   },
   "outputs": [],
   "source": [
    "# create new classes for sand\n",
    "df_sub['sand_class'] = df_sub['sand']\n",
    "df_sub.loc[df_sub['sand'].between(0,.15), 'sand_class'] = 0\n",
    "df_sub.loc[df_sub['sand'].between(.15,.3), 'sand_class'] = 1\n",
    "df_sub.loc[df_sub['sand'].between(.3,.45), 'sand_class'] = 2\n",
    "df_sub.loc[df_sub['sand'].between(.6,.75), 'sand_class'] = 4\n",
    "df_sub.loc[df_sub['sand'].between(.75,.9), 'sand_class'] = 5\n",
    "df_sub.loc[df_sub['sand'].between(.9,1), 'sand_class'] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T20:14:03.832554Z",
     "start_time": "2022-03-11T20:07:49.361190Z"
    }
   },
   "outputs": [],
   "source": [
    "from tsfresh import extract_relevant_features\n",
    "df_features = df_sub[['POINT_ID', 'date', 'VV', 'VH', 'angle', 'NDVI', 'day_of_year',\n",
    "       'minimum_2m_air_temperature', 'pp_24h', 'pp_48h', 'pp_72h',\n",
    "       'platform_A', 'platform_B', 'orbit_ASCENDING', 'orbit_DESCENDING']]\n",
    "\n",
    "y_features = df_sub[['POINT_ID','USDA']].drop_duplicates()\n",
    "y_features = y_features.set_index('POINT_ID')['USDA']\n",
    "\n",
    "relevant_features = extract_relevant_features(df_features, y_features, column_id='POINT_ID', column_sort='date')\n",
    "relevant_features['USDA'] = y_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion tree full features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T20:42:47.461723Z",
     "start_time": "2022-03-11T20:42:47.414859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(relevant_features, y_features, test_size=.3)\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test),labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys()) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression full features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:31:40.475518Z",
     "start_time": "2022-03-11T18:31:32.121587Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter=6000, class_weight='balanced')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## less features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:10:22.312012Z",
     "start_time": "2022-03-11T18:09:17.787194Z"
    }
   },
   "outputs": [],
   "source": [
    "from tsfresh import extract_relevant_features\n",
    "df_features_small = df_sub[(df_sub['platform_A'] == 1 ) & (df_sub['orbit_ASCENDING'] == 1 )]\n",
    "df_features_small = df_features_small[['POINT_ID', 'date', 'VV', 'VH', 'angle', 'NDVI', 'day_of_year',\n",
    "       'minimum_2m_air_temperature', 'pp_24h', 'pp_48h', 'pp_72h']]\n",
    "\n",
    "y_features_small = df_sub[['POINT_ID','FAO']].drop_duplicates()\n",
    "y_features_small = y_features_small.set_index('POINT_ID')['FAO']\n",
    "\n",
    "relevant_features_small = extract_relevant_features(df_features_small, y_features_small, column_id='POINT_ID', column_sort='date')\n",
    "relevant_features_small['FAO'] = y_features_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### desicion tree less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:13:35.458482Z",
     "start_time": "2022-03-11T18:13:35.418027Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(relevant_features_small, y_features_small, test_size=.4)\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descion Tree USDA full rows\n",
    "https://keras.io/examples/structured_data/deep_neural_decision_forests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T09:48:16.838725Z",
     "start_time": "2022-03-12T09:48:16.818726Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T20:02:57.842237Z",
     "start_time": "2022-03-11T20:02:57.028021Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = df_sub[df_sub['date'].dt.year.isin([2018])].values\n",
    "X = df_sub[['VV', 'VH', 'NDVI','angle', 'day_of_year', 'platform_A', 'platform_B', \n",
    "            'orbit_ASCENDING', 'orbit_DESCENDING', 'minimum_2m_air_temperature',\n",
    "            'pp_24h', 'pp_48h', 'pp_72h']].values\n",
    "#y = df_sub[['FAO_nr_0', 'FAO_nr_1', 'FAO_nr_2']].values #One Hot encoded\n",
    "#y = df_sub['FAO'].values\n",
    "y = df_sub['USDA'].values\n",
    "\n",
    "# transform the dataset BALANCE dataset\n",
    "#oversample = SMOTE()\n",
    "#X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "## Split Training Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.35)\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Multi Time Series with irregular time intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn multiple classifiers comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:01:26.540316Z",
     "start_time": "2022-03-12T14:01:26.524723Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:30:50.340312Z",
     "start_time": "2022-03-12T13:30:50.287162Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = df_sub[df_sub['date'].dt.year.isin([2018])].values\n",
    "X = df_sub[['VV', 'VH', 'NDVI','angle', 'day_of_year', 'platform_A', 'platform_B', \n",
    "            'orbit_ASCENDING', 'orbit_DESCENDING','minimum_2m_air_temperature',\n",
    "            'pp_24h', 'pp_48h', 'pp_72h']].values\n",
    "\n",
    "y = df_sub['USDA'].values\n",
    "\n",
    "## Split Training Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:01:28.065557Z",
     "start_time": "2022-03-12T14:01:26.941201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Decision tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "print(classification_report(y_test, decision_tree.predict(X_test), labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:40:06.768375Z",
     "start_time": "2022-03-12T14:28:26.838198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Desicion Tree Hyperparameter Tuning\n",
    "param_dict = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : range(1,10),\n",
    "    'min_samples_split' : range(1,10),\n",
    "    'min_samples_leaf' : range(1,5)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(decision_tree,\n",
    "                   param_grid = param_dict,\n",
    "                   cv=10,\n",
    "                   verbose=1,\n",
    "                   n_jobs=-1)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:01:28.512894Z",
     "start_time": "2022-03-12T14:01:28.512894Z"
    }
   },
   "outputs": [],
   "source": [
    "# RandomForest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "print(classification_report(y_test, random_forest.predict(X_test), labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:01:28.512894Z",
     "start_time": "2022-03-12T14:01:28.512894Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "param_dict = {\n",
    "    'n_estimators' : [50,100,300],\n",
    "    'max_depth' : range(1,10),\n",
    "    'min_features' : range(1,X_train.shape[1]),\n",
    "    'min_samples_split' : range(1,6)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(random_forest,\n",
    "                   param_grid = param_dict,\n",
    "                   cv=10,\n",
    "                   verbose=1,\n",
    "                   n_jobs=-1)\n",
    "grid.fit(x_train,y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:44:28.738937Z",
     "start_time": "2022-03-12T13:41:47.354731Z"
    }
   },
   "outputs": [],
   "source": [
    "# GradientBoosting XGBoost Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test), labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:47:34.069596Z",
     "start_time": "2022-03-12T13:47:33.951778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test), labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:01:20.902864Z",
     "start_time": "2022-03-12T13:47:36.678387Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVM Support Vector Machines\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test), labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T14:01:26.024802Z",
     "start_time": "2022-03-12T14:01:21.321722Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "classifier = AdaBoostClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classification_report(y_test, classifier.predict(X_test), labels = list(USDA_to_numerical['USDA'].values()), target_names = list(USDA_to_numerical['USDA'].keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Functional API with 3 outputs - sand, clay, silt distribution size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T11:30:17.651716Z",
     "start_time": "2022-03-12T11:30:17.636312Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import datetime\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T20:23:56.760906Z",
     "start_time": "2022-03-07T20:23:56.745297Z"
    }
   },
   "source": [
    "one epoch = one forward pass and one backward pass of all the training examples\n",
    "batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T11:27:09.028811Z",
     "start_time": "2022-03-12T11:27:09.013823Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    metrices https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T10:11:40.374078Z",
     "start_time": "2022-03-12T10:11:40.327443Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = df_sub[df_sub['date'].dt.year.isin([2018])].values\n",
    "X = df_sub[['VV', 'VH', 'NDVI','angle', 'day_of_year', 'platform_A', 'platform_B', \n",
    "            'orbit_ASCENDING', 'orbit_DESCENDING','minimum_2m_air_temperature',\n",
    "            'pp_24h', 'pp_48h', 'pp_72h']].values\n",
    "\n",
    "y1 = df_sub['sand'].values\n",
    "y2 = df_sub['silt'].values\n",
    "y3 = df_sub['clay'].values\n",
    "\n",
    "y = df_sub[['sand', 'silt', 'clay']].values\n",
    "\n",
    "## Split Training Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T10:12:31.697412Z",
     "start_time": "2022-03-12T10:12:31.681758Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T10:22:39.285805Z",
     "start_time": "2022-03-12T10:22:39.270177Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Define model layers.\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    first_dense = Dense(128, activation='relu')(input_layer)\n",
    "    # Y1 output will be fed from the first dense\n",
    "    y1_output = Dense(1, name='sand')(first_dense)    \n",
    "    second_dense = Dense(128 ,activation='relu')(first_dense)\n",
    "    # Y2 output will be fed from the second dense\n",
    "    y2_output = Dense(1,name='silt')(second_dense)\n",
    "    third_dense = Dense(128,activation='relu')(second_dense)\n",
    "    y3_output = Dense(1,name='clay')(third_dense)\n",
    "    # Define the model with the input layer and a list of output layers\n",
    "    model = Model(inputs=input_layer,outputs=[y1_output, y2_output, y3_output])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T12:21:26.115213Z",
     "start_time": "2022-03-12T12:21:25.520145Z"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model()# Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "optimizer = RMSprop(.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss={'sand': 'mae', 'silt': 'mae', 'clay': 'mae'},\n",
    "metrics={'sand':tf.keras.metrics.MeanAbsoluteError(),\n",
    "        'silt':tf.keras.metrics.MeanAbsoluteError(),\n",
    "        'clay':tf.keras.metrics.MeanAbsoluteError()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T11:49:51.702024Z",
     "start_time": "2022-03-12T11:49:51.686397Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_improving = EarlyStopping(monitor='val_silt_loss', patience=10)#####+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs\\\\fit_30\\\\'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T12:21:36.923596Z",
     "start_time": "2022-03-12T12:21:30.724655Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, np.split(y_train, 3, axis=1),\n",
    "                    epochs=30, batch_size=71581, validation_split = 0.2)\n",
    "#validation_data=(X_test, np.split(y_test, 3, axis=1)), callbacks=[tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T12:22:49.520723Z",
     "start_time": "2022-03-12T12:22:49.489448Z"
    }
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API with one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T12:34:40.419099Z",
     "start_time": "2022-03-12T12:34:40.403477Z"
    }
   },
   "outputs": [],
   "source": [
    "## The Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.nn import relu\n",
    "#from tensorflow.nn import linear\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:23:43.354152Z",
     "start_time": "2022-03-12T13:23:43.323086Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = df_sub[df_sub['date'].dt.year.isin([2018])].values\n",
    "\n",
    "X = df_sub[(df_sub['platform_A'] == 1) & (df_sub['orbit_ASCENDING'] == 1)][['VV', 'VH', 'NDVI','angle', 'day_of_year', \n",
    "            'minimum_2m_air_temperature', 'pp_24h', 'pp_48h', 'pp_72h']].values\n",
    "\n",
    "y = df_sub[(df_sub['platform_A'] == 1) & (df_sub['orbit_ASCENDING'] == 1)]['sand'].values\n",
    "## Split Training Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:23:46.677504Z",
     "start_time": "2022-03-12T13:23:45.211719Z"
    }
   },
   "outputs": [],
   "source": [
    "#Architecture \n",
    "def build_model(optimizer):\n",
    "    model = Sequential([\n",
    "        layers.Dense(24, activation=relu, input_shape=(X_train.shape[1],)), # densly (fully connected) hidden layer\n",
    "        layers.Dense(12, activation=relu), # denly hidden layer\n",
    "        layers.Dense(1, activation='linear') #output layer\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'mse', # https://keras.io/api/losses/\n",
    "        optimizer = optimizer, #https://keras.io/api/optimizers/#available-optimizers\n",
    "        metrics = ['mse']) \n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(optimizer = Adam(.001))\n",
    "\n",
    "# stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "stop_improving = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    epochs = EPOCHS,\n",
    "    batch_size=X_train.shape[0],\n",
    "    validation_split = 0.2, \n",
    "    verbose = 1,\n",
    "    callbacks = [stop_improving]) # Calls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T12:42:23.798130Z",
     "start_time": "2022-03-12T12:42:23.782482Z"
    }
   },
   "outputs": [],
   "source": [
    "len(y_test), len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:23:49.351972Z",
     "start_time": "2022-03-12T13:23:48.935512Z"
    }
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "## Make Predictions\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('True Values [% sand]')\n",
    "plt.ylabel('Predictions [% sand]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plot_predict_scatter = plt.plot([-100,100], [-100,100])\n",
    "print(plot_predict_scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API with one output and feature extraction as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new classes for sand\n",
    "df_sub['sand_class'] = df_sub['sand']\n",
    "df_sub.loc[df_sub['sand'].between(0,.15), 'sand_class'] = 0\n",
    "df_sub.loc[df_sub['sand'].between(.15,.3), 'sand_class'] = 1\n",
    "df_sub.loc[df_sub['sand'].between(.3,.45), 'sand_class'] = 2\n",
    "df_sub.loc[df_sub['sand'].between(.6,.75), 'sand_class'] = 4\n",
    "df_sub.loc[df_sub['sand'].between(.75,.9), 'sand_class'] = 5\n",
    "df_sub.loc[df_sub['sand'].between(.9,1), 'sand_class'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:03:04.042516Z",
     "start_time": "2022-03-12T12:56:43.338477Z"
    }
   },
   "outputs": [],
   "source": [
    "from tsfresh import extract_relevant_features\n",
    "df_features = df_sub[['POINT_ID', 'date', 'VV', 'VH', 'angle', 'NDVI', 'day_of_year',\n",
    "       'minimum_2m_air_temperature', 'pp_24h', 'pp_48h', 'pp_72h',\n",
    "       'platform_A', 'platform_B', 'orbit_ASCENDING', 'orbit_DESCENDING']]\n",
    "\n",
    "y_features = df_sub[['POINT_ID','sand_class']].drop_duplicates()\n",
    "y_features = y_features.set_index('POINT_ID')['sand_class']\n",
    "\n",
    "relevant_features = extract_relevant_features(df_features, y_features, column_id='POINT_ID', column_sort='date')\n",
    "relevant_features['sand_class'] = y_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:03:10.586650Z",
     "start_time": "2022-03-12T13:03:10.555434Z"
    }
   },
   "outputs": [],
   "source": [
    "y_features = df_sub[['POINT_ID','sand']].drop_duplicates()\n",
    "y_features = y_features['sand']\n",
    "relevant_features.drop('sand_class', inplace=True, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(relevant_features, y_features, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:05:16.183585Z",
     "start_time": "2022-03-12T13:05:16.130439Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:07:33.717582Z",
     "start_time": "2022-03-12T13:07:32.282791Z"
    }
   },
   "outputs": [],
   "source": [
    "#Architecture \n",
    "def build_model(optimizer):\n",
    "    model = Sequential([\n",
    "        layers.Dense(541, activation=relu, input_shape=[X_train.shape[1]]), # densly (fully connected) hidden layer\n",
    "        layers.Dense(64, activation=relu), # denly hidden layer\n",
    "        layers.Dense(1, activation='linear') #output layer\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'mae', # https://keras.io/api/losses/\n",
    "        optimizer = optimizer, #https://keras.io/api/optimizers/#available-optimizers\n",
    "        metrics = ['mae', 'mse', 'accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(optimizer = RMSprop(.001))\n",
    "\n",
    "# stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "stop_improving = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2, \n",
    "    verbose = 1,\n",
    "    callbacks = [stop_improving]) # Calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T13:07:55.359714Z",
     "start_time": "2022-03-12T13:07:55.222104Z"
    }
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "## Make Predictions\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('True Values [% sand]')\n",
    "plt.ylabel('Predictions [% sand]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([-200,2000])\n",
    "plt.ylim([-200,2000])\n",
    "plot_predict_scatter = plt.plot([-100,100], [-100,100])\n",
    "print(plot_predict_scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-12T11:54:56.271686Z",
     "start_time": "2022-03-12T11:53:56.026030Z"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit_30/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification with imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To-Do Balance Dataset\n",
    "!!Check!! Use only not winter values\n",
    "!!Check!! Find 24h rain 48 rain and 72h rainamount before acquisition\n",
    "use Tensorboard stuff https://www.tensorflow.org/tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T15:42:01.613279Z",
     "start_time": "2022-03-11T15:42:01.597661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show imbalanced\n",
    "df_sub['FAO'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:52:58.670120Z",
     "start_time": "2022-03-11T18:52:58.632603Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = df_sub[df_sub['date'].dt.year.isin([2018])].values\n",
    "X = df_sub[['VV', 'VH', 'NDVI','angle', 'day_of_year', 'platform_A', 'platform_B', \n",
    "            'orbit_ASCENDING', 'orbit_DESCENDING','minimum_2m_air_temperature',\n",
    "            'pp_24h', 'pp_48h', 'pp_72h']].values\n",
    "y = df_sub[['FAO_nr_0', 'FAO_nr_1', 'FAO_nr_2']].values #One Hot encoded\n",
    "#y = df_sub['FAO'].values\n",
    "\n",
    "## Split Training Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Balancing \n",
    "\n",
    "    SMOTE balancing decrease results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T16:17:40.485483Z",
     "start_time": "2022-03-11T16:17:39.690798Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform the dataset BALANCE dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T19:38:58.230301Z",
     "start_time": "2022-03-10T19:38:58.218348Z"
    }
   },
   "outputs": [],
   "source": [
    "totals = df_sub['FAO'].value_counts().values\n",
    "totalMean = totals.mean()\n",
    "weights = {i: totalMean / count for i, count in enumerate(totals)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ouput dense layer always softmax for multiclass (mutual exclusiv)\n",
    "    loss also categorical_crossentropy for multiclass\n",
    "    optimizer = rmsprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Keras, Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T16:05:51.955282Z",
     "start_time": "2022-03-11T16:05:51.944509Z"
    }
   },
   "source": [
    "### Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:39:26.051150Z",
     "start_time": "2022-03-11T18:39:25.917088Z"
    }
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    return model\n",
    "\n",
    "# Compile model\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if from feature extraction method use this to ensure one hot encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:38:55.513746Z",
     "start_time": "2022-03-11T18:38:55.498098Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:26:49.436219Z",
     "start_time": "2022-03-11T18:26:46.572084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "# stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "stop_improving = EarlyStopping(monitor='accuracy', patience=5)\n",
    "\n",
    "model.fit(x=X_train, y=y_train, epochs=100, verbose=1, validation_data=(X_test, y_test), callbacks=[stop_improving])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T16:27:39.941696Z",
     "start_time": "2022-03-11T16:27:13.671844Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=X.shape[0], verbose=1) #verbose=0 debugging=off\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T18:40:30.336226Z",
     "start_time": "2022-03-11T18:39:57.594584Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=X_train.shape[0], verbose=1) #verbose=0 debugging=off\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn logistic regression\n",
    "    V3 \n",
    "    less features dont make better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T16:48:22.964896Z",
     "start_time": "2022-03-11T16:48:22.939035Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = df_sub[df_sub['date'].dt.year.isin([2018])].values\n",
    "#X = df_sub[['VV', 'VH', 'NDVI', 'day_of_year', 'platform_A', 'platform_B', \n",
    "#            'orbit_ASCENDING', 'orbit_DESCENDING','minimum_2m_air_temperature',\n",
    "#            'pp_24h', 'pp_48h', 'pp_72h']].values\n",
    "X = df_sub[['VV', 'NDVI','platform_A', 'platform_B', \n",
    "            'orbit_ASCENDING', 'orbit_DESCENDING']].values\n",
    "#y = df_sub[['FAO_nr_0', 'FAO_nr_1', 'FAO_nr_2']].values #One Hot encoded\n",
    "y = df_sub['FAO'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T16:46:51.019872Z",
     "start_time": "2022-03-11T16:46:51.004277Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter=350, class_weight='balanced')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:23:41.026253Z",
     "start_time": "2022-03-07T10:23:41.022258Z"
    }
   },
   "outputs": [],
   "source": [
    "features = ['VV', 'VH', 'NDVI', 'platform_A', 'platform_B', 'orbit_ASCENDING', 'orbit_DESCENDING', 'day_of_year', 'FAO_nr']\n",
    "y_label = 'FAO_nr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:23:43.062504Z",
     "start_time": "2022-03-07T10:23:43.038435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe Statistics \n",
    "train_stats = df_sub[features].describe()\n",
    "train_stats.pop(y_label)\n",
    "train_stats = train_stats.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:23:43.938434Z",
     "start_time": "2022-03-07T10:23:43.918461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into Train and Test \n",
    "df = df_sub[features].reset_index(drop=True)\n",
    "train_df = df.sample(frac = 0.8, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "train_labels = train_df.pop(y_label)\n",
    "test_labels = test_df.pop(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T10:24:02.961628Z",
     "start_time": "2022-03-07T10:23:45.116702Z"
    }
   },
   "outputs": [],
   "source": [
    "# The Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.nn import relu\n",
    "#from tensorflow.nn import linear\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "#Architecture \n",
    "def build_model(optimizer):\n",
    "    model = Sequential([\n",
    "        layers.Dense(128, activation=relu, input_shape=[len(train_df.keys())]), # densly (fully connected) hidden layer\n",
    "        layers.Dense(64, activation=relu), # denly hidden layer\n",
    "        layers.Dense(1, activation='linear') #output layer\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'mse', # https://keras.io/api/losses/\n",
    "        optimizer = optimizer, #https://keras.io/api/optimizers/#available-optimizers\n",
    "        metrics = ['mae', 'mse', 'accuracy']) \n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(optimizer = Adam(.001))\n",
    "\n",
    "#Inspect Model\n",
    "model_summary = model.summary()\n",
    "\n",
    "\n",
    "## Train the model\n",
    "# Callbacks\n",
    "\"\"\"\n",
    "class Calls(Callback):\n",
    "    #Print dot every epoch while training\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\"\"\"\n",
    "# stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "stop_improving = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "    x = train_df,\n",
    "    y = train_labels,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = 0.2, \n",
    "    verbose = 0,\n",
    "    callbacks = [stop_improving]) # Calls()\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "\n",
    "def plot_history(hitsory):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [m^3/m^3]')\n",
    "    plt.plot(hist['epoch'], hist['mae'],\n",
    "            label = 'Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mae'],\n",
    "            label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,hist['mae'].max() + hist['mae'].max() * 0.5])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [m^3/m^3^2]')\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "            label = 'Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "            label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,hist['mse'].max() + hist['mse'].max() * 0.5])\n",
    "\n",
    "plot_history(history)\n",
    "print(f'metrics_names : {model.metrics_names}')\n",
    "loss, mae, mse, accuracy = model.evaluate(test_df, test_labels, verbose = 0)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} m^3/m^3\".format(mae))\n",
    "print(\"Testing set Mean squared Error: {:5.2f} m^3/m^3\".format(mse))\n",
    "print(\"Testing set Accuracy: {:5.2f} m^3/m^3\".format(accuracy))\n",
    "\n",
    "## Make Predictions\n",
    "test_predictions = model.predict(test_df).flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values m^3/m^3')\n",
    "plt.ylabel('Predictions m^3/m^3')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plot_predict_scatter = plt.plot([-100,100], [-100,100])\n",
    "print(plot_predict_scatter)\n",
    "\n",
    "plt.figure()\n",
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error m^3/m^3')\n",
    "plot_error_hist = plt.ylabel('Count')\n",
    "print(plot_error_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T11:22:59.924864Z",
     "start_time": "2022-03-06T11:22:58.351972Z"
    }
   },
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T20:38:48.744998Z",
     "start_time": "2022-03-10T20:38:48.698266Z"
    }
   },
   "outputs": [],
   "source": [
    "plotterAccLoss = AccLossPlotter(graphs=['acc', 'loss'], save_graph=True)\n",
    "plotterConfusion = ConfusionMatrixPlotter(X_val=X_test, classes=['0', '1', '2'], Y_val=y_test)\n",
    "classificationReport = ClassificationReport(X_val=X_test, classes=['0', '1', '2'], Y_val=y_test)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='categorical_crossentropy', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.patches as mpatches  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class AccLossPlotter(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Plot training Accuracy and Loss values on a Matplotlib graph. \n",
    "    The graph is updated by the 'on_epoch_end' event of the Keras Callback class\n",
    "\n",
    "    Adapted from: https://github.com/chasingbob/keras-visuals/blob/master/visual_callbacks.py\n",
    "\n",
    "    # Arguments\n",
    "        graphs: list with some or all of ('acc', 'loss')\n",
    "        save_graph: Save graph as an image on Keras Callback 'on_train_end' event \n",
    "    \"\"\"\n",
    "    def __init__(self, graphs=['acc', 'loss'], save_graph=False):\n",
    "        self.graphs = graphs\n",
    "        self.num_subplots = len(graphs)\n",
    "        self.save_graph = save_graph\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.epoch_count = 0\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_count += 1\n",
    "        self.val_acc.append(logs.get('val_categorical_accuracy'))\n",
    "        self.acc.append(logs.get('categorical_accuracy'))\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        epochs = [x for x in range(self.epoch_count)]\n",
    "\n",
    "        count_subplots = 0\n",
    "        \n",
    "        if 'acc' in self.graphs:\n",
    "            count_subplots += 1\n",
    "            plt.subplot(self.num_subplots, 1, count_subplots)\n",
    "            plt.title('Accuracy')\n",
    "            plt.plot(epochs, self.val_acc, color='r')\n",
    "            plt.plot(epochs, self.acc, color='b')\n",
    "            plt.ylabel('accuracy')\n",
    "\n",
    "            red_patch = mpatches.Patch(color='red', label='Val')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='Train')\n",
    "\n",
    "            plt.legend(handles=[red_patch, blue_patch], loc=4)\n",
    "\n",
    "        if 'loss' in self.graphs:\n",
    "            count_subplots += 1\n",
    "            plt.subplot(self.num_subplots, 1, count_subplots)\n",
    "            plt.title('Loss')\n",
    "            plt.plot(epochs, self.val_loss, color='r')\n",
    "            plt.plot(epochs, self.loss, color='b')\n",
    "            plt.ylabel('loss')\n",
    "\n",
    "            red_patch = mpatches.Patch(color='red', label='Val')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='Train')\n",
    "\n",
    "            plt.legend(handles=[red_patch, blue_patch], loc=4)\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        if self.save_graph:\n",
    "            plt.savefig('training_acc_loss.png')\n",
    "\n",
    "class ConfusionMatrixPlotter(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Plot the confusion matrix on a graph and update after each epoch\n",
    "\n",
    "    Adapted from: https://github.com/chasingbob/keras-visuals/blob/master/visual_callbacks.py\n",
    "\n",
    "    # Arguments\n",
    "        X_val: The input values \n",
    "        Y_val: The expected output values\n",
    "        classes: The categories as a list of string names\n",
    "        normalize: True - normalize to [0,1], False - keep as is\n",
    "        cmap: Specify matplotlib colour map\n",
    "        title: Graph Title\n",
    "    \"\"\"\n",
    "    def __init__(self, X_val, Y_val, classes, normalize=False, cmap=plt.cm.Blues, title='Confusion Matrix'):\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.title = title\n",
    "        self.classes = classes\n",
    "        self.normalize = normalize\n",
    "        self.cmap = cmap\n",
    "        plt.ion()\n",
    "        plt.figure()\n",
    "        plt.title(self.title)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):    \n",
    "        plt.clf()\n",
    "        pred = self.model.predict(self.X_val)\n",
    "        max_pred = np.argmax(pred, axis=1)\n",
    "        max_y = np.argmax(self.Y_val, axis=1)\n",
    "        cnf_mat = confusion_matrix(max_y, max_pred)\n",
    "   \n",
    "        if self.normalize:\n",
    "            cnf_mat = cnf_mat.astype('float') / cnf_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cnf_mat.max() / 2.\n",
    "        for i, j in itertools.product(range(cnf_mat.shape[0]), range(cnf_mat.shape[1])):\n",
    "            plt.text(j, i, cnf_mat[i, j],                                          \n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cnf_mat[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.imshow(cnf_mat, interpolation='nearest', cmap=self.cmap)\n",
    "\n",
    "        # Labels\n",
    "        tick_marks = np.arange(len(self.classes))\n",
    "        plt.xticks(tick_marks, self.classes, rotation=45)\n",
    "        plt.yticks(tick_marks, self.classes)\n",
    "        plt.colorbar()                                                                       \n",
    "        plt.tight_layout()                                                    \n",
    "        plt.ylabel('True label')                                              \n",
    "        plt.xlabel('Predicted label')                                         \n",
    "        plt.show()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "\n",
    "class ClassificationReport(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Print the scikit-learn classification_report after each epoch\n",
    "\n",
    "    # Arguments\n",
    "        X_val: The input values \n",
    "        Y_val: The expected output values\n",
    "        classes: The categories as a list of string names\n",
    "    \"\"\"\n",
    "    def __init__(self, X_val, Y_val, classes, normalize=False):\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.classes = classes\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):   \n",
    "        pred = self.model.predict(self.X_val)\n",
    "        max_pred = np.argmax(pred, axis=1)\n",
    "        max_y = np.argmax(self.Y_val, axis=1)\n",
    "        print(classification_report(max_y, max_pred, target_names=self.classes))\n",
    "\n",
    "# stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "stop_improving = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T11:23:02.977742Z",
     "start_time": "2022-03-06T11:23:02.946727Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T11:21:27.846128Z",
     "start_time": "2022-03-06T11:21:27.842133Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T11:21:54.520518Z",
     "start_time": "2022-03-06T11:21:54.505126Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - MLP - train model with regression problematic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T17:00:55.280624Z",
     "start_time": "2022-03-03T17:00:55.238735Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mlp_lr_sm(paths, features, y_label, networks, stations, climates,use_date_as_var, ndvi_low, ndvi_high, allow_frozen_state,loss_function, optimizer, metrics, save_to):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Import modules\n",
    "    from glob import glob\n",
    "    import datetime as dt\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    ## Initialize data\n",
    "    # Files with data\n",
    "    paths = glob(paths)\n",
    "    paths.sort()\n",
    "\n",
    "    # Create Station id linked with filepath\n",
    "    ids = [x.split('\\\\')[-1].split('_')[0] for x in paths]\n",
    "    files = dict(zip(ids,paths))\n",
    "\n",
    "    # Load all files into one Dataframe\n",
    "    gdf = gpd.tools.util.pd.concat(map(gpd.read_file, paths), ignore_index=True)\n",
    "    print(f'Rows before preprocessing {len(gdf)}')\n",
    "    \n",
    "    \n",
    "    ## Preprocessing\n",
    "    # Set type of date column to datetime object\n",
    "    gdf.date = gdf.date.astype('datetime64[ns]')\n",
    "    \n",
    "    #Drop rows without Sentinel 2 data\n",
    "    gdf.dropna(how='any',subset=['NDVI'], inplace=True)\n",
    "    \n",
    "    # Convert int to timedelta in days \n",
    "    s2_timedelta = [dt.timedelta(days=x) for x in gdf.s2_distance]  \n",
    "    gdf.s2_distance = s2_timedelta\n",
    "    \n",
    "    #Remove uneseccary columns\n",
    "    gdf.drop(labels = ['CloudMask'], axis = 1, inplace = True)\n",
    "\n",
    "    # Clean Nan Values within subset columns (major variables to inspect)\n",
    "    gdf.dropna(how='any', subset=['soil_moisture', 'VV'], inplace=True)\n",
    "\n",
    "    # Drop lower and upper 1% of data to eliminate outliers\n",
    "    gdf = gdf[gdf.soil_moisture.gt(gdf.soil_moisture.quantile(0.01)) & gdf.soil_moisture.lt(gdf.soil_moisture.quantile(0.99))]\n",
    "    gdf = gdf[gdf.VV.gt(gdf.VV.quantile(0.01)) & gdf.VV.lt(gdf.VV.quantile(0.99))]\n",
    "    gdf = gdf[gdf.NDVI.gt(gdf.NDVI.quantile(0.01)) & gdf.NDVI.lt(gdf.NDVI.quantile(0.99))]\n",
    "\n",
    "    # Remove rows where ndvi is older than 30days\n",
    "    gdf = gdf[gdf.s2_distance.gt(dt.timedelta(days=-7)) & gdf.s2_distance.lt(dt.timedelta(days=7))]\n",
    "    \n",
    "    # Remove NDVI values outside range because to high ndvi means to dense vegetation and vv is disturbed too much \n",
    "    if None not in [ndvi_low, ndvi_high]:\n",
    "        gdf = gdf[gdf.NDVI.between(ndvi_low,ndvi_high)]\n",
    "    \n",
    "    # Remove rows where Temperature is below Zero 273,15 K because VV is in frozen soils not trustworth\n",
    "    if allow_frozen_state == 'no':\n",
    "        gdf = gdf[gdf.minimum_2m_air_temperature.gt(273,15)]\n",
    "\n",
    "    # Choose only specififed Network\n",
    "    if networks is not None:\n",
    "        gdf = gdf[gdf['network'].isin(networks)]\n",
    "    \n",
    "    # Choose only specified Stations\n",
    "    if stations is not None:\n",
    "        gdf = gdf[gdf['station'].isin(stations)]\n",
    "        \n",
    "    # Choose only specified climate after Köpping https://en.wikipedia.org/wiki/K%C3%B6ppen_climate_classification\n",
    "    if climates is not None:\n",
    "        gdf = gdf[gdf['climate'].isin(climates)]\n",
    "    print(f'Rows after preprocessing {len(gdf)}')\n",
    "    \n",
    "    # Create a Variable from date convert the timestamps to delta, perform a cumulative sum on the dt feature to get a monotonically increasing value\n",
    "    # For now use dayofyear \n",
    "    if use_date_as_var == True:\n",
    "        #date_timedelta = [dt.timedelta(x) for x in gdf['date']]\n",
    "        #date_timedelta_cumsum = np.cumsum(date_timedelta)\n",
    "        gdf['day_of_year'] = gdf.date.dt.day_of_year\n",
    "        \n",
    "        \n",
    "    ## One Hot Encoding\n",
    "    gdf = pd.get_dummies(gdf, columns=[\"platform\", \"orbit\"])\n",
    "    \n",
    "    ## Statistics\n",
    "    #KDE Plot\n",
    "    #plot_kde = sns.pairplot(gdf[features], diag_kind = 'kde')\n",
    "    #print(plot_kde)\n",
    "    \n",
    "    # Dataframe Statistics \n",
    "    train_stats = gdf[features].describe()\n",
    "    train_stats.pop(y_label)\n",
    "    train_stats = train_stats.transpose()\n",
    "    #print(train_stats)\n",
    "    \n",
    "    \n",
    "    ## Split into Train and Test \n",
    "    df = pd.DataFrame(gdf[features]).reset_index(drop=True)\n",
    "    df['sand'] = df['sand'] / 100\n",
    "    train_df = df.sample(frac = 0.8, random_state=0)\n",
    "    test_df = df.drop(train_df.index)\n",
    "    \n",
    "    train_labels = train_df.pop(y_label)\n",
    "    test_labels = test_df.pop(y_label)\n",
    "    scaler = MinMaxScaler()\n",
    "    print(train_labels)\n",
    "    features.remove(y_label)\n",
    "    train_df[features] = scaler.fit_transform(X = train_df[features])\n",
    "    test_df[features] = scaler.fit_transform(X = test_df[features])\n",
    "    \n",
    "    n_train_df = train_df\n",
    "    n_test_df = test_df\n",
    "    \n",
    "    ## Normalize data\n",
    "    def norm(x):\n",
    "        return (x - train_stats['mean']) / train_stats['std']\n",
    "    \n",
    "    #n_train_df = norm(train_df)\n",
    "    #n_test_df = norm(test_df)\n",
    "\n",
    "    ## The Model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.nn import relu\n",
    "    #from tensorflow.nn import linear\n",
    "    from tensorflow.keras.optimizers import RMSprop\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from keras.callbacks import Callback\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    import tensorflow as tf\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    \n",
    "    #Architecture \n",
    "    def build_model(optimizer):\n",
    "        model = Sequential([\n",
    "            layers.Dense(128, activation=relu, input_shape=[len(n_train_df.keys())]), # densly (fully connected) hidden layer\n",
    "            layers.Dense(64, activation=relu), # denly hidden layer\n",
    "            layers.Dense(1, activation='linear') #output layer\n",
    "        ])\n",
    "\n",
    "        if optimizer == 'RMSprop':\n",
    "            optimizer = RMSprop(0.001)\n",
    "        \n",
    "        if optimizer == 'Adam':\n",
    "            optimizer = Adam(.001)\n",
    "        model.compile(\n",
    "            loss = loss_function, # https://keras.io/api/losses/\n",
    "            optimizer = optimizer, #https://keras.io/api/optimizers/#available-optimizers\n",
    "            metrics = metrics) \n",
    "\n",
    "        return model\n",
    "    \n",
    "    model = build_model(optimizer)\n",
    "    \n",
    "    #Inspect Model\n",
    "    model_summary = model.summary()\n",
    "    \n",
    "    \n",
    "    ## Train the model\n",
    "    # Callbacks\n",
    "    \"\"\"\n",
    "    class Calls(Callback):\n",
    "        #Print dot every epoch while training\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            if epoch % 100 == 0: print('')\n",
    "            print('.', end='')\n",
    "    \"\"\"\n",
    "    # stop the training when there is no improvement in the loss for three consecutive epochs.\n",
    "    stop_improving = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    EPOCHS = 1000\n",
    "\n",
    "    history = model.fit(\n",
    "        x = n_train_df,\n",
    "        y = train_labels,\n",
    "        epochs = EPOCHS,\n",
    "        validation_split = 0.2, \n",
    "        verbose = 0,\n",
    "        callbacks = [stop_improving]) # Calls()\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def plot_history(hitsory):\n",
    "        hist = pd.DataFrame(history.history)\n",
    "        hist['epoch'] = history.epoch\n",
    "\n",
    "        plt.figure()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Abs Error [m^3/m^3]')\n",
    "        plt.plot(hist['epoch'], hist['mae'],\n",
    "                label = 'Train Error')\n",
    "        plt.plot(hist['epoch'], hist['val_mae'],\n",
    "                label = 'Val Error')\n",
    "        plt.legend()\n",
    "        plt.ylim([0,hist['mae'].max() + hist['mae'].max() * 0.5])\n",
    "\n",
    "        plt.figure()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Square Error [m^3/m^3^2]')\n",
    "        plt.plot(hist['epoch'], hist['mse'],\n",
    "                label = 'Train Error')\n",
    "        plt.plot(hist['epoch'], hist['val_mse'],\n",
    "                label = 'Val Error')\n",
    "        plt.legend()\n",
    "        plt.ylim([0,hist['mse'].max() + hist['mse'].max() * 0.5])\n",
    "\n",
    "    plot_history(history)\n",
    "    print(f'metrics_names : {model.metrics_names}')\n",
    "    loss, mae, mse, accuracy = model.evaluate(n_test_df, test_labels, verbose = 0)\n",
    "    print(\"Testing set Mean Abs Error: {:5.2f} m^3/m^3\".format(mae))\n",
    "    print(\"Testing set Mean squared Error: {:5.2f} m^3/m^3\".format(mse))\n",
    "    print(\"Testing set Accuracy: {:5.2f} m^3/m^3\".format(accuracy))\n",
    "\n",
    "    ## Make Predictions\n",
    "    test_predictions = model.predict(n_test_df).flatten()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(test_labels, test_predictions)\n",
    "    plt.xlabel('True Values m^3/m^3')\n",
    "    plt.ylabel('Predictions m^3/m^3')\n",
    "    plt.axis('equal')\n",
    "    plt.axis('square')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plot_predict_scatter = plt.plot([-100,100], [-100,100])\n",
    "    print(plot_predict_scatter)\n",
    "    \n",
    "    plt.figure()\n",
    "    error = test_predictions - test_labels\n",
    "    plt.hist(error, bins=25)\n",
    "    plt.xlabel('Prediction Error m^3/m^3')\n",
    "    plot_error_hist = plt.ylabel('Count')\n",
    "    print(plot_error_hist)\n",
    "    \n",
    "    #Save Model\n",
    "    if save_to is not None:\n",
    "        model.save(save_to)\n",
    "\n",
    "    return model, gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T16:50:48.701853Z",
     "start_time": "2022-03-03T16:50:48.693863Z"
    }
   },
   "outputs": [],
   "source": [
    "features_s1 = ['platform', 'orbit', 'VV', 'VH', 'angle', 'img_id'] #latform & orbit are categorical data\n",
    "\n",
    "features_s2 = ['Aerosols', 'Blue', 'Green', 'Red', 'RedEdge1', 'RedEdge2', 'RedEdge3', 'RedEdge4', 'NIR', \n",
    "               'WaterVapor', 'Cirrus', 'SWIR1', 'SWIR2', 'CloudMask', 'NDVI', 'date_y']\n",
    "\n",
    "features_era5 = ['dewpoint_2m_temperature', 'maximum_2m_air_temperature', 'mean_2m_air_temperature',\n",
    "                 'minimum_2m_air_temperature', 'surface_pressure', 'total_precipitation', \n",
    "                 'u_component_of_wind_10m', 'v_component_of_wind_10m']\n",
    "\n",
    "features_ismn = ['ismn_id','soil_moisture', 'soil_moisture_flag', 'soil_moisture_orig_flag', 'network', \n",
    "                 'station', 'clay', 'sand', 'silt', 'oc', 'climate', 'elevation', 'instrument']\n",
    "\n",
    "features_4all = ['date','day_of_year', 'geometry']\n",
    "\n",
    "features_selected = ['day_of_year', 'VV', 'VH', 'angle', 'platform_A', 'platform_B', 'orbit_ASCENDING','orbit_DESCENDING',\n",
    "                     'NDVI', 'soil_moisture']\n",
    "\n",
    "networks_all = {'Germany' : ['TERENO'],'Austria' : ['WEGERENET']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T17:03:06.722797Z",
     "start_time": "2022-03-03T17:00:58.020985Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, gdf = mlp_lr_sm(\n",
    "    paths = 'C://Users/USER/Desktop/Master_Irrigation/03_GIS/ground_trouth/CNN_data/*',\n",
    "    features = features_selected + ['sand'],\n",
    "    y_label = 'sand',\n",
    "    networks = None,\n",
    "    stations = None,\n",
    "    climates = None,\n",
    "    use_date_as_var = True,\n",
    "    ndvi_low = 0,\n",
    "    ndvi_high = 0.4, \n",
    "    allow_frozen_state = 'no',\n",
    "    loss_function = 'mae',       # 'mae', 'sparse_categorical_crossentropy'\n",
    "    optimizer =  'Adam',    # Ftrl adam, Adamax Nadam  'RMSprop',  SGD  RMSprop  Adam  Adadelta  Adagrad         \n",
    "    metrics = ['mae', 'mse', 'accuracy'],\n",
    "    save_to = r'C:\\Users\\USER\\Desktop\\Master_Irrigation\\03_GIS\\cnn_models_soil\\soil_ssm'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T12:41:43.046954Z",
     "start_time": "2022-02-23T12:39:02.703270Z"
    }
   },
   "outputs": [],
   "source": [
    "model_30m, gdf_30m = mlp_lr_sm(\n",
    "    paths = 'C://Users/USER/Desktop/Master_Irrigation/03_GIS/ground_trouth/CNN_data_30m/*',\n",
    "    features = features_era5 + features_selected,\n",
    "    y_label = 'soil_moisture',\n",
    "    networks = None,\n",
    "    stations = None,\n",
    "    climates = None,\n",
    "    use_date_as_var = True,\n",
    "    ndvi_low = 0,\n",
    "    ndvi_high = 0.7, \n",
    "    allow_frozen_state = 'no',\n",
    "    loss_function = 'mae',       # 'mae', 'sparse_categorical_crossentropy'\n",
    "    optimizer =  'Adam',    # Ftrl adam, Adamax Nadam  'RMSprop',  SGD  RMSprop  Adam  Adadelta  Adagrad         \n",
    "    metrics = ['mae', 'mse', 'accuracy'],\n",
    "    save_to = r'C:\\Users\\USER\\Desktop\\Master_Irrigation\\03_GIS\\cnn_models\\basic_ssm'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T12:43:01.101747Z",
     "start_time": "2022-02-23T12:43:01.086153Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf_30m[features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T12:36:02.600535Z",
     "start_time": "2022-02-08T12:36:02.469143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "Merzenhausen = gdf[gdf['station'] == 'Merzenhausen'].copy()\n",
    "x_Merz_values = Merzenhausen[features_selected + features_era5]\n",
    "y_Merz_label = x_Merz_values.pop('soil_moisture')\n",
    "\n",
    "train_stats = x_Merz_values.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "x_Merz_norm = norm(x_Merz_values)\n",
    "y_Merz_predict = model.predict(x_Merz_norm).flatten()\n",
    "Merzenhausen['sm_predict'] = y_Merz_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T12:36:03.587214Z",
     "start_time": "2022-02-08T12:36:03.571559Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas  # noqa\n",
    "\n",
    "pd.options.plotting.backend = 'holoviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T12:36:21.626826Z",
     "start_time": "2022-02-08T12:36:04.488694Z"
    }
   },
   "outputs": [],
   "source": [
    "Merzenhausen.hvplot.scatter(x='date', y=['soil_moisture', 'sm_predict'],groupby='date.year' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T12:36:21.789483Z",
     "start_time": "2022-02-08T12:36:21.773861Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(Merzenhausen['soil_moisture'] - Merzenhausen['sm_predict']).sum() / 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Function names with arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T21:55:11.696024Z",
     "start_time": "2022-02-04T21:55:11.680045Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction, signature\n",
    "import numpy\n",
    "\n",
    "def explain(m):\n",
    "    try:\n",
    "        return f\"{m[0]}{signature(m[1])}\"\n",
    "    except:\n",
    "        return f\"{m[0]}(???)\" # some functions don't provide signature\n",
    "\n",
    "print(*(explain(m) for m in getmembers(irrigation_detection, isfunction)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T21:38:07.697928Z",
     "start_time": "2022-03-06T21:38:07.675741Z"
    }
   },
   "outputs": [],
   "source": [
    "def USDA_classifier(sand,silt,clay):\n",
    "    soil_class = np.nan\n",
    "    soil_names = np.nan\n",
    "    soil_class_nr = np.nan\n",
    "    soil_names_nr = np.nan\n",
    "    if sand >= .86 and silt <= .14: #and clay <= .10:\n",
    "        soil_class = 'Sand'\n",
    "        soil_names = 'Sandy soils (Coarse texture)'\n",
    "        soil_class_nr = 0\n",
    "        soil_names_nr = 0\n",
    "    elif .7 <= sand <= .86 and silt <= .30: #and clay <= .15:\n",
    "        soil_class = 'Loamy Sand'\n",
    "        soil_names = 'Sandy soils (Coarse texture)'\n",
    "        soil_class_nr = 1\n",
    "        soil_names_nr = 0\n",
    "    elif .50 <= sand <= .70 and silt <= .50: #and clay <= .20:\n",
    "        soil_class = 'Sandy loam'\n",
    "        soil_names = 'Loamy soils (Moderately coarse texture)'\n",
    "        soil_class_nr = 2\n",
    "        soil_names_nr = 1\n",
    "    elif .23 <= sand <= .52 and .28 <= silt <= .50: #and .07 <= clay <= .27:\n",
    "        soil_class = 'Loam'\n",
    "        soil_names = 'Loamy soils (Medium texture)'\n",
    "        soil_class_nr = 3\n",
    "        soil_names_nr = 2\n",
    "    elif .20 <= sand <= .50 and .74 <= silt <= .88: #and clay <= .27:\n",
    "        soil_class = 'Silty loam'\n",
    "        soil_names = 'Loamy soils (Medium texture)'\n",
    "        soil_class_nr = 4\n",
    "        soil_names_nr = 2\n",
    "    elif sand <= .20  and .88 <= silt <= .100: #and clay <= .12:\n",
    "        soil_class = 'Silty'\n",
    "        soil_names = 'Loamy soils (Medium texture)'\n",
    "        soil_class_nr = 5\n",
    "        soil_names_nr = 2\n",
    "    elif .20 <= sand <= .45 and .15 <= silt <= .52: #and .27 <= clay <= .40:\n",
    "        soil_class = 'Clay loam'\n",
    "        soil_names = 'Loamy soils (Moderately fine texture)'\n",
    "        soil_class_nr = 6\n",
    "        soil_names_nr = 3\n",
    "    elif .45 <= sand <= .80 and silt <= .28: #and .20 <= clay <= .35:\n",
    "        soil_class = 'Sandy clay loam'\n",
    "        soil_names = 'Loamy soils (Moderately fine texture)'\n",
    "        soil_class_nr = 7\n",
    "        soil_names_nr = 3\n",
    "    elif sand <= .20 and .40 <= silt <= .73: #and .27 <= clay <= .40:\n",
    "        soil_class = 'Silty clay loam'\n",
    "        soil_names = 'Loamy soils (Moderately fine texture)'\n",
    "        soil_class_nr = 8\n",
    "        soil_names_nr = 3\n",
    "    elif .45 <= sand <= .60 and silt <= .20: #and .35 <= clay <= .55:\n",
    "        soil_class = 'Sandy clay'\n",
    "        soil_names = 'Clayey soils (Fine texture)'\n",
    "        soil_class_nr = 9\n",
    "        soil_names_nr = 4\n",
    "    elif sand <= .20 and .40 <= silt <= .60: #and .40 <= clay <= .60:\n",
    "        soil_class = 'Silty clay'\n",
    "        soil_names = 'Clayey soils (Fine texture)'\n",
    "        soil_class_nr = 10\n",
    "        soil_names_nr = 4\n",
    "    elif sand <= .45 and silt <= .40: #and .40 <= clay <= .100:\n",
    "        soil_class = 'Clay'\n",
    "        soil_names = 'Clayey soils (Fine texture)'\n",
    "        soil_class_nr = 11\n",
    "        soil_names_nr = 4\n",
    "    return soil_class, soil_names, soil_class_nr, soil_names_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T21:38:38.539923Z",
     "start_time": "2022-03-06T21:38:11.194411Z"
    }
   },
   "outputs": [],
   "source": [
    "soil_class, soil_names, soil_class_nr, soil_names_nr = list(), list(), list(), list()\n",
    "for index, row in gdf.iterrows():\n",
    "    soil_class_i, soil_names_i, soil_class_nr_i, soil_names_nr_i = USDA_classifier(row.sand, row.silt, row.clay)\n",
    "    soil_class.append(soil_class_i)\n",
    "    soil_names.append(soil_names_i)\n",
    "    soil_class_nr.append(soil_class_nr_i)\n",
    "    soil_names_nr.append(soil_names_nr_i)\n",
    "gdf['soil_class'] = soil_class\n",
    "gdf['soil_names'] = soil_names\n",
    "gdf['soil_class_nr'] = soil_class_nr\n",
    "gdf['soil_names_nr'] = soil_names_nr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev1]",
   "language": "python",
   "name": "conda-env-dev1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "434.85px",
    "left": "1547px",
    "right": "20px",
    "top": "120px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
