{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import moduels\n",
    "import xarray as xr\n",
    "\n",
    "#general modules\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#statistical modules\n",
    "from scipy.stats import linregress\n",
    "import scipy.stats\n",
    "from dtw import dtw,accelerated_dtw\n",
    "from scipy.signal import hilbert, butter, filtfilt\n",
    "from scipy.fftpack import fft,fftfreq,rfft,irfft,ifft\n",
    "\n",
    "#machine learning modules\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn import cluster, datasets\n",
    "\n",
    "#gridded & tabular data\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import salem\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "#Visualisation\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import geoviews as gv\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import gmaps\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "\n",
    "#projection\n",
    "from pyproj import Proj\n",
    "\n",
    "#Widgets & Ipython Stuff\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display_html\n",
    "\n",
    "#holoview\n",
    "hv.extension('matplotlib')\n",
    "hv.config.image_rtol = 0.1\n",
    "opts.defaults(opts.Scatter3D(color='Value', cmap='fire', edgecolor='black', s=50))\n",
    "renderer = hv.plotting.mpl.MPLRenderer.instance(dpi=120)\n",
    "#hv.renderer('matplotlib')\n",
    "\n",
    "#gmaps\n",
    "gmaps.configure(api_key='AIzaSyC3jBxz5pktQXdIFOQzFv6MnOYIuF_ULvc')\n",
    "\n",
    "#change modules parameter\n",
    "# pretty print all cell's output and not just the last one\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\" ##'allÄ or lst_exp = only last if unwandtet noise use ; at EOL or in for loop use _ = to assign varaible\n",
    "#check module versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths \n",
    "paths = ['C://Users//USER//Desktop//master-thesis-master//', 'D://jupy_data//', 'C://Users//USER//Desktop//Masterarbeit//DATA//master_data//', 'C:\\\\Users\\\\USER\\\\Desktop\\\\Masterarbeit\\\\DATA\\\\master_data\\\\'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ried center\n",
    "lon_c = 8.5262\n",
    "lat_c = 49.7238\n",
    "\n",
    "#Load city names\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "\n",
    "df = pd.read_csv(paths[0] + 'json//germany_city_names.csv', sep=None, encoding='utf-8')\n",
    "print(df.head())\n",
    "geometry = [Point(xy) for xy in zip(df.lon, df.lat)]\n",
    "crs = {'init': 'epsg:4326'} #http://www.spatialreference.org/ref/epsg/2263/\n",
    "geo_df = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "geo_df.to_file(driver='ESRI Shapefile', filename='germany_city_names.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = gpd.read_file('germany_city_names.shp')\n",
    "city_names_filtered = city_names[city_names['place'] == 'city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Gridded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil Moisture & Precipitation & real evapotranspiration (1km x 1km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_model_data = xr.open_dataset(paths[1] + 'xr_model_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xr_da_mean(xr_model_data.soil_moisture_1km,vec_aoi,'Soil Moisture Time Series Mean (2015.04 - 2019.12)', 'mean_ts_model_sm', cmap='plasma_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDVI & other Indizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_1c = xr.open_dataset(paths[0] + 'indizes//s2_1c.nc')\n",
    "s2_1c_ndvi_100m = xr.open_dataset(paths[0] + 'indizes//s2_1c_ndvi_100m.nc').rename_vars({'ndvi_100m' : 'NDVI'})\n",
    "l7_sr_ndvi_100m = xr.open_dataset(paths[0] + 'indizes//l7_sr_ndvi_100m.nc').rename_vars({'ndvi_100m' : 'NDVI'})\n",
    "l8_sr_ndvi_100m = xr.open_dataset(paths[0] + 'indizes//l7_sr_ndvi_100m.nc').rename_vars({'ndvi_100m' : 'NDVI'})\n",
    "ndvi_datasets = [s2_1c, s2_1c_ndvi_100m, l7_sr_ndvi_100m, l8_sr_ndvi_100m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load soil map data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_map_data = gpd.read_file(paths[3] + 'Bodenkarte_200\\\\aoi2020\\\\Boden_2020.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hauptgruppe = soil_map_data.HAUPTGRUPP.drop_duplicates().values.tolist()\n",
    "Gruppe = soil_map_data.GRUPPE.drop_duplicates().values.tolist()\n",
    "Untergruppe = soil_map_data.UNTERGRUPP.drop_duplicates().values.tolist()\n",
    "Bodeneinheit = soil_map_data.BODENEINHE.drop_duplicates().values.tolist()\n",
    "Substrat = soil_map_data.SUBSTRAT.drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {'HAUPTGRUPP': Hauptgruppe, 'GRUPPE': Gruppe, 'UNTERGRUPP': Untergruppe, 'BODENEINHE': Bodeneinheit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical data to Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_soil_map_100 = make_geocube(vector_data=soil_map_data, output_crs=\"+init=epsg:4326\", resolution=(-0.0008983152841195215, 0.0008983152841195215), categorical_enums=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load desired AOI as shapefile with geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_aoi = gpd.read_file(paths[0] + 'aoi2020//aoi_2020.shp')\n",
    "vec_ried = gpd.read_file(paths[2] + 'Ried_225_222//hessisches_ried.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the xarray dataset with salem accesor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_model_aoi = xr_model_data.salem.subset(shape=vec_aoi)\n",
    "xr_model_ried = xr_model_data.salem.subset(shape=vec_ried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xr_da_mean(xr_model_aoi.soil_moisture_1km,None,None,'Soil Moisture Time Series Mean for Nördl. Oberrheingraben(2015.04 - 2019.12)', 'mean_ts_aoi_sm', cmap='plasma_r', lw_1=4, fsc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xr_da_mean(xr_model_ried.soil_moisture_1km,vec_ried,None,'Soil Moisture Time Series Mean for Hessisches Ried(2015.04 - 2019.12)', 'mean_ts_ried_sm', cmap='plasma_r', lw_1=4, fsc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xr_da_mean(xr_model_ried.soil_moisture_1km, vec_ried, vec_clc_ried[vec_clc_ried['raster_grp'] !=2],'Soil Moisture Time Series Mean for Hessisches Ried(2015.04 - 2019.12)', 'mean_ts_ried_sm_clc', cmap='plasma_r', lw_1=4,lw_2=1, fsc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xr_da_mean(xr_model_ried.soil_moisture_1km.where(clc_100_ried < 300), vec_ried, vec_clc_ried[vec_clc_ried['raster_grp'] !=2],'Soil Moisture Time Series Mean for Hessisches Ried(2015.04 - 2019.12) \\n with agriculture areas', 'mean_ts_ried_sm_clc_f', lw_1=4,lw_2=1, fsc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs={'title' : 'Soil Moisture Mean cm³/cm³ over time (2015.04 - 2019.12)', 'titelsize' : 12}\n",
    "cbar_kwargs = {'label': 'soil moisture cm³/cm³', 'pad' : 0.1, 'shrink' : 0.5} #, 'drawedges': True\n",
    "legend_dict={'fontsize' : 15}\n",
    "\n",
    "def plot_xr_da_mean(da,vec,vec_clc_1,title, name, cmap='plasma_r', lw_1 = 4,lw_2 = 2, fsc=14): #plasma_r\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    \n",
    "    \n",
    "    #norm=plt.Normalize(0,0.6)\n",
    "    #cmap = mlp.colors.LinearSegmentedColormap.from_list(\"\", [\"lightblue\",'blue',\"red\"])\n",
    "    \n",
    "    da.groupby('latitude', 'longitude').mean('time').plot(cbar_kwargs=cbar_kwargs, cmap=plt.cm.get_cmap(cmap))\n",
    "    #kmeans_plot = gpd_kmeans_f.plot('group', markersize=18, ax=ax, legend=True,alpha=1, label='Kmean group') #, color=['white', 'blue', 'red', 'yellow', 'black']\n",
    "    try:\n",
    "        vec.geometry.boundary.plot(color=None,edgecolor='k', linewidth = lw_1, ax=ax)\n",
    "    except:\n",
    "        print('no geometry detect')\n",
    "    try:\n",
    "        vec_clc_1.geometry.boundary.plot(color=None,edgecolor='k', linewidth = lw_2, ax=ax, hatch='\\\\\\\\\\\\\\\\')\n",
    "        circ1 = mpatches.Patch(facecolor='white',alpha=1, hatch=r'\\\\\\\\',label='non agriculture land')\n",
    "        ax.legend(handles = [circ1],loc='upper right', fontsize='large')\n",
    "    except:\n",
    "        print('no geometry detect')\n",
    "        \n",
    "    city_names_filtered_1 = city_names_filtered[::]\n",
    "    city_names_filtered_1.plot(ax=ax)\n",
    "    city_names_filtered_1.apply(lambda x: ax.annotate(s=x['name'], xy=x.geometry.centroid.coords[0], ha='center', fontsize=fsc) ,axis=1) ;\n",
    "    \n",
    "    ax.set_title(title, fontsize=15,  pad=10) \n",
    "    ax.tick_params('both', labelsize=11) \n",
    "    plt.xlabel('longitude', fontsize=13,  labelpad=10)\n",
    "    plt.ylabel('latitude', fontsize=13,  labelpad=10)\n",
    "\n",
    "    F = plt.gcf()\n",
    "    Size = F.get_size_inches()\n",
    "    F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('figures//%s.png' %(name))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "xr_model_aoi.soil_moisture_1km.groupby('latitude', 'longitude').mean('time').plot(cbar_kwargs=cbar_kwargs, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "\n",
    "city_names_filtered.plot(ax=ax)\n",
    "city_names_filtered.apply(lambda x: ax.annotate(s=x['name'], xy=x.geometry.centroid.coords[0], ha='center', fontsize=12) ,axis=1) ;\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 4, ax=ax)\n",
    "\n",
    "ax.set_title('Soil Moisture Time Series Mean (2015.04 - 2019.12)', fontsize=15,  pad=10) \n",
    "ax.tick_params('both', labelsize=11) \n",
    "plt.xlabel('longitude', fontsize=13,  labelpad=10)\n",
    "plt.ylabel('latitude', fontsize=13,  labelpad=10)\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "\n",
    "plt.savefig('figures//sm_mean_aoi_RB.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clc_1000_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pandas dataframe \n",
    "stacked_df = xr_model_aoi.to_dataframe().reset_index()\n",
    "stacked_df_f = xr_model_aoi.where(clc_1000_aoi.data < 300).to_dataframe().reset_index()\n",
    "\n",
    "stacked_df['lonlat'] = stacked_df.longitude + stacked_df.latitude\n",
    "stacked_df_f['lonlat'] = stacked_df_f.longitude + stacked_df_f.latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask nan values\n",
    "mask_na_sm = np.isfinite(stacked_df.soil_moisture_1km.tolist())\n",
    "stacked_df_sm = stacked_df[mask_na_sm]\n",
    "lonlat_list = stacked_df_sm.lonlat.unique()\n",
    "\n",
    "#mask nan values\n",
    "mask_na_sm_f = np.isfinite(stacked_df_f.soil_moisture_1km.tolist())\n",
    "stacked_df_sm_f = stacked_df_f[mask_na_sm_f]\n",
    "lonlat_list_f = stacked_df_sm_f.lonlat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_df_sm_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get soil map as Y\n",
    "y = xr_soil_map_100.interp(y=xr_model_ried.latitude, x=xr_model_ried.longitude, method='nearest')\n",
    "y_Bodeneinheit = y.BODENEINHE.to_dataframe().reset_index()\n",
    "y_Hauptgruppe  = y.HAUPTGRUPP.to_dataframe().reset_index()\n",
    "y_Bodeneinheit['lonlat'] = y_Bodeneinheit.longitude + y_Bodeneinheit.latitude\n",
    "y_Hauptgruppe['lonlat']  = y_Hauptgruppe.longitude + y_Hauptgruppe.latitude\n",
    "y_Bodeneinheit = y_Bodeneinheit[y_Bodeneinheit['lonlat'].isin(lonlat_list)]\n",
    "y_Hauptgruppe = y_Hauptgruppe[y_Hauptgruppe['lonlat'].isin(lonlat_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Features\n",
    "extracted_features_sm = extract_features(stacked_df_sm, column_id=\"lonlat\", column_sort=\"time\", column_value=\"soil_moisture_1km\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Features\n",
    "extracted_features_sm_f = extract_features(stacked_df_sm_f, column_id=\"lonlat\", column_sort=\"time\", column_value=\"soil_moisture_1km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat y to features\n",
    "extracted_features_sm_e = extracted_features_sm\n",
    "#extracted_features_sm_e['Bodeneinheit'] = np.array(y_Bodeneinheit['BODENEINHE'].tolist()).astype('int')\n",
    "extracted_features_sm_e['Hauptgruppe'] = np.array(y_Hauptgruppe['HAUPTGRUPP'].tolist()).astype('int')\n",
    "\n",
    "#select only valid samples\n",
    "extracted_features_sm_s = extracted_features_sm_e[extracted_features_sm_e['Hauptgruppe'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all nan values \n",
    "impute(extracted_features_sm_s)\n",
    "#select only relevant features\n",
    "features_filtered_sm = select_features(extracted_features_sm_s.iloc[:,:-1], extracted_features_sm_s['Hauptgruppe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all nan values \n",
    "impute(extracted_features_sm)\n",
    "impute(extracted_features_sm_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_features_sm), len(extracted_features_sm_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means clustering\n",
    "X = np.array(extracted_features_sm.iloc[:,:2])\n",
    "#y_iris = np.array(extracted_features_sm_s['Hauptgruppe'])\n",
    "km3 = cluster.KMeans(n_clusters=3).fit(X)\n",
    "km4 = cluster.KMeans(n_clusters=4).fit(X)\n",
    "km5 = cluster.KMeans(n_clusters=5).fit(X)\n",
    "km7 = cluster.KMeans(n_clusters=7).fit(X)\n",
    "km3_p = km3.predict(X)\n",
    "km4_p = km4.predict(X)\n",
    "km5_p = km5.predict(X)\n",
    "km7_p = km7.predict(X)\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.subplot(141)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km3.labels_)\n",
    "plt.title(\"K=3, J=%.2f\" % km3.inertia_)\n",
    "plt.subplot(142)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km4.labels_)\n",
    "plt.title(\"K=4, J=%.2f\" % km4.inertia_)\n",
    "plt.subplot(143)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km5.labels_)#.astype(np.float))\n",
    "plt.title(\"K=5, J=%.2f\" % km5.inertia_)\n",
    "plt.subplot(144)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km7.labels_)#.astype(np.float))\n",
    "plt.title(\"K=7, J=%.2f\" % km7.inertia_)\n",
    "plt.savefig('figures//%s.png' %('kmeans_aoi_all'))#K means clustering\n",
    "\n",
    "X_f = np.array(extracted_features_sm_f.iloc[:,:20])\n",
    "#y_iris = np.array(extracted_features_sm_s['Hauptgruppe'])\n",
    "km3_f = cluster.KMeans(n_clusters=3).fit(X_f)\n",
    "km4_f = cluster.KMeans(n_clusters=4).fit(X_f)\n",
    "km5_f = cluster.KMeans(n_clusters=5).fit(X_f)\n",
    "km7_f = cluster.KMeans(n_clusters=7).fit(X_f)\n",
    "km3_f_p = km3_f.predict(X_f)\n",
    "km4_f_p = km4_f.predict(X_f)\n",
    "km5_f_p = km5_f.predict(X_f)\n",
    "km7_f_p = km7_f.predict(X_f)\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.subplot(141)\n",
    "plt.scatter(X_f[:, 0], X_f[:, 1], c=km3_f.labels_)\n",
    "plt.title(\"K=3, J=%.2f\" % km3_f.inertia_)\n",
    "plt.subplot(142)\n",
    "plt.scatter(X_f[:, 0], X_f[:, 1], c=km4_f.labels_)\n",
    "plt.title(\"K=4, J=%.2f\" % km4_f.inertia_)\n",
    "plt.subplot(143)\n",
    "plt.scatter(X_f[:, 0], X_f[:, 1], c=km5_f.labels_)#.astype(np.float))\n",
    "plt.title(\"K=5, J=%.2f\" % km5_f.inertia_)\n",
    "plt.subplot(144)\n",
    "plt.scatter(X_f[:, 0], X_f[:, 1], c=km7_f.labels_)#.astype(np.float))\n",
    "plt.title(\"K=7, J=%.2f\" % km7_f.inertia_)\n",
    "plt.savefig('figures//%s.png' %('kmeans_aoi_all_2_f'))#K means clustering\n",
    "\n",
    "#np.unique(y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get groups for classification with kmeans\n",
    "result = extracted_features_sm\n",
    "result['group'] = km7_p\n",
    "result = result['group']\n",
    "groups = stacked_df_sm.drop_duplicates('lonlat').join(result, on='lonlat', rsuffix='asd')[['longitude', 'latitude', 'group']]\n",
    "\n",
    "result_f = extracted_features_sm_f\n",
    "result_f['group'] = km7_f_p\n",
    "result_f = result_f['group']\n",
    "groups_f = stacked_df_sm_f.drop_duplicates('lonlat').join(result_f, on='lonlat', rsuffix='asd')[['longitude', 'latitude', 'group']]\n",
    "\n",
    "#create geometry\n",
    "gdf_groups_aoi = gpd.GeoDataFrame(groups, geometry=gpd.points_from_xy(groups.longitude, groups.latitude))\n",
    "gdf_groups_f_aoi = gpd.GeoDataFrame(groups_f, geometry=gpd.points_from_xy(groups_f.longitude, groups_f.latitude))\n",
    "\n",
    "#intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbar_kwargs = {'label': 'soil moisture cm³/cm³', 'pad' : 0.1, 'shrink' : 0.5} #, 'drawedges': True\n",
    "legend_kwds={'label': \"Kmeans cluster group\"}\n",
    "\n",
    "#create gpd multipoint with shaply geometry from lon & lat equal to [Point(x, y) for x, y in zip(df.Longitude, df.Latitude)]\n",
    "gpd_kmeans_ried = gpd.GeoDataFrame(gdf_groups_ried, geometry=gpd.points_from_xy(gdf_groups_ried.longitude, gdf_groups_ried.latitude))\n",
    "gpd_kmeans_f_ried = gpd.GeoDataFrame(gdf_groups_f_ried, geometry=gpd.points_from_xy(gdf_groups_f_ried.longitude, gdf_groups_f_ried.latitude))\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 2, nrows=2, dpi=200)\n",
    "ax[0,0].scatter(X[:, 0], X[:, 1], c=km5.labels_)\n",
    "ax[0,1].scatter(X_f[:, 0], X_f[:, 1], c=km5_f.labels_)#.astype(np.float))\n",
    "\n",
    "divider = make_axes_locatable(ax[1,0])\n",
    "divider_2 = make_axes_locatable(ax[1,1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "gpd_kmeans_ried.plot('group',  markersize=120, legend=True, ax=ax[1,0], alpha=1, cax=cax)\n",
    "\n",
    "gpd_kmeans_f_ried.plot('group', markersize=120, legend=True, ax=ax[1,1], alpha=1, cax=cax_2)\n",
    "\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1,0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1,1])\n",
    "#ax[.axis([7.7, 8.7, 49.55, 50.18])\n",
    "\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0,0].set_title('Soil Moisture Clustering by Feature Extraction from Time Series (5 Groups)')\n",
    "ax[0,1].set_title('Soil Moisture Clustering by Feature Extraction from Time Series on Agriculture Surfaces (5 Groups)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('kmeans_ried_all_compare_results_5'))#K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbar_kwargs = {'label': 'soil moisture cm³/cm³', 'pad' : 0.1, 'shrink' : 0.5} #, 'drawedges': True\n",
    "legend_kwds={'label': \"Kmeans cluster group\"}\n",
    "\n",
    "#create gpd multipoint with shaply geometry from lon & lat equal to [Point(x, y) for x, y in zip(df.Longitude, df.Latitude)]\n",
    "gpd_kmeans_ried = gpd.GeoDataFrame(gdf_groups_ried, geometry=gpd.points_from_xy(gdf_groups_ried.longitude, gdf_groups_ried.latitude))\n",
    "gpd_kmeans_f_ried = gpd.GeoDataFrame(gdf_groups_f_ried, geometry=gpd.points_from_xy(gdf_groups_f_ried.longitude, gdf_groups_f_ried.latitude))\n",
    "\n",
    "gpd_kmeans_aoi = gpd.GeoDataFrame(gdf_groups_aoi, geometry=gpd.points_from_xy(gdf_groups_aoi.longitude, gdf_groups_aoi.latitude))\n",
    "gpd_kmeans_f_aoi = gpd.GeoDataFrame(gdf_groups_f_aoi, geometry=gpd.points_from_xy(gdf_groups_f_aoi.longitude, gdf_groups_f_aoi.latitude))\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 2, nrows=2, dpi=200)\n",
    "ax[0,0].scatter(X[:, 0], X[:, 1], c=km5.labels_)\n",
    "ax[0,1].scatter(X_f[:, 0], X_f[:, 1], c=km5_f.labels_)#.astype(np.float))\n",
    "\n",
    "divider = make_axes_locatable(ax[1,0])\n",
    "divider_2 = make_axes_locatable(ax[1,1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "gpd_kmeans_aoi.plot('group',  markersize=60, legend=True, ax=ax[1,0], alpha=1, cax=cax)\n",
    "\n",
    "gpd_kmeans_f_aoi.plot('group', markersize=60, legend=True, ax=ax[1,1], alpha=1, cax=cax_2)\n",
    "\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1,0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1,1])\n",
    "#ax[.axis([7.7, 8.7, 49.55, 50.18])\n",
    "\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0,0].set_title('Soil Moisture Clustering by Feature Extraction from Time Series (5 Groups)')\n",
    "ax[0,1].set_title('Soil Moisture Clustering by Feature Extraction from Time Series on Agriculture Surfaces (5 Groups)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('kmeans_aoi_all_compare_results_5'))#K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbar_kwargs = {'label': 'soil moisture cm³/cm³', 'pad' : 0.1, 'shrink' : 0.5} #, 'drawedges': True\n",
    "legend_kwds={'label': \"Kmeans cluster group\"}\n",
    "\n",
    "#create gpd multipoint with shaply geometry from lon & lat equal to [Point(x, y) for x, y in zip(df.Longitude, df.Latitude)]\n",
    "gpd_kmeans_ried = gpd.GeoDataFrame(gdf_groups_ried, geometry=gpd.points_from_xy(gdf_groups_ried.longitude, gdf_groups_ried.latitude))\n",
    "gpd_kmeans_f_ried = gpd.GeoDataFrame(gdf_groups_f_ried, geometry=gpd.points_from_xy(gdf_groups_f_ried.longitude, gdf_groups_f_ried.latitude))\n",
    "\n",
    "gpd_kmeans_aoi = gpd.GeoDataFrame(gdf_groups_aoi, geometry=gpd.points_from_xy(gdf_groups_aoi.longitude, gdf_groups_aoi.latitude))\n",
    "gpd_kmeans_f_aoi = gpd.GeoDataFrame(gdf_groups_f_aoi, geometry=gpd.points_from_xy(gdf_groups_f_aoi.longitude, gdf_groups_f_aoi.latitude))\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 2, nrows=2, dpi=200)\n",
    "ax[0,0].scatter(X[:, 0], X[:, 1], c=km7.labels_)\n",
    "ax[0,1].scatter(X_f[:, 0], X_f[:, 1], c=km7_f.labels_)#.astype(np.float))\n",
    "\n",
    "divider = make_axes_locatable(ax[1,0])\n",
    "divider_2 = make_axes_locatable(ax[1,1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "gpd_kmeans_aoi.plot('group',  markersize=25, legend=True, ax=ax[1,0], alpha=1, cax=cax)\n",
    "\n",
    "gpd_kmeans_f_aoi.plot('group', markersize=25, legend=True, ax=ax[1,1], alpha=1, cax=cax_2)\n",
    "\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1,0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1,1])\n",
    "#ax[.axis([7.7, 8.7, 49.55, 50.18])\n",
    "\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0,0].set_title('Soil Moisture Clustering by Feature Extraction from Time Series (7 Groups)')\n",
    "ax[0,1].set_title('Soil Moisture Clustering by Feature Extraction from Time Series on Agriculture Surfaces (7 Groups)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('kmeans_aoi_all_compare_results_7'))#K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplleaflet\n",
    "ax = gpd_kmeans_f_aoi.plot(column='group')\n",
    "mplleaflet.show(fig=ax.figure, path='figures//gpd_kmeans_aoi_f_map_7.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/playing-with-time-series-data-in-python-959e2485bff8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single band data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tif file in xarray and squeeze single value dimensions & rename dims\n",
    "clc_100 = xr.open_rasterio(paths[0] + 'corine_land_cover//corine_land_cover_2018_100.tif').squeeze(drop = True).rename({'x' : 'longitude', 'y' : 'latitude'})\n",
    "clc_1000 = xr.open_rasterio(paths[0] + 'corine_land_cover//corine_land_cover_2018_1000.tif').squeeze(drop = True).rename({'x' : 'longitude', 'y' : 'latitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clc_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json with attributes file into variable\n",
    "with open(paths[0] + 'corine_land_cover//clc_2018_attributes.txt') as json_file:\n",
    "    clc_2018_attributes = json.load(json_file)\n",
    "    \n",
    "#add class names and values from dict\n",
    "df_attributes = pd.DataFrame({'landcover_class_names' : clc_2018_attributes['properties']['landcover_class_names'], 'landcover_class_values' : clc_2018_attributes['properties']['landcover_class_values']})\n",
    "#reduce class column to lvl0 class\n",
    "df_attributes['landcover_class'] = df_attributes['landcover_class_names'].apply(lambda x: x.partition(\";\")[0])\n",
    "df_attributes['landcover_lvl0_values'] = df_attributes['landcover_class_values'].apply(lambda x: int(str(x)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corine landcover to geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load shapefile via geopandas into a GeoSeries \n",
    "#vec_aoi = gpd.GeoSeries(gpd.read_file(paths[0] + 'aoi2020//aoi_2020.shp')['geometry'])\n",
    "#vec_ried = gpd.GeoSeries(gpd.read_file(paths[2] + 'Ried_225_222//hessisches_ried.shp')['geometry'])\n",
    "vec_clc_aoi = gpd.GeoDataFrame(gpd.read_file(paths[0] + 'corine_land_cover//vec_clc_aoi.shp'))\n",
    "\n",
    "#get Intersection of ried and aoi to create ried clc vector\n",
    "vec_clc_ried = gpd.overlay(gpd.GeoDataFrame(vec_ried), gpd.GeoDataFrame(vec_clc_aoi), how='intersection')\n",
    "\n",
    "#Set projection to GeoSeries \n",
    "#vec_aoi.crs = {'init' :'epsg:4326'}\n",
    "#vec_ried.crs = {'init' :'epsg:4326'}\n",
    "vec_clc_aoi.crs = {'init' :'epsg:4326'}\n",
    "vec_clc_ried.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove invalid polygon from lancover vector\n",
    "vec_clc_aoi = vec_clc_aoi[vec_clc_aoi['raster_val'] != 0.0]\n",
    "vec_clc_ried = vec_clc_ried[vec_clc_ried['raster_val'] != 0.0]\n",
    "#Add column for lvl 0 Group of Land classes\n",
    "vec_clc_aoi['raster_grp'] = vec_clc_aoi['raster_val'].apply(lambda x: int(str(x)[0]))\n",
    "vec_clc_ried['raster_grp'] = vec_clc_ried['raster_val'].apply(lambda x: int(str(x)[0]))\n",
    "#Add column for string description of landclasses\n",
    "vec_clc_aoi['landcover_class'] = vec_clc_aoi['raster_val'].apply(lambda x: df_attributes['landcover_class'][df_attributes['landcover_class_values'] == x].values[0])\n",
    "vec_clc_ried['landcover_class'] = vec_clc_ried['raster_val'].apply(lambda x: df_attributes['landcover_class'][df_attributes['landcover_class_values'] == x].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate coordinates (long & lat) in order to get same dimension size and spatial extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original resolution ~1km\n",
    "clc_1000_aoi = clc_100.interp(latitude=xr_model_aoi['latitude'], longitude=xr_model_aoi['longitude'], method='nearest')\n",
    "clc_1000_ried = clc_1000.interp(latitude=xr_model_ried['latitude'], longitude=xr_model_ried['longitude'], method='nearest')\n",
    "clc_100_ried = clc_100.interp(latitude=xr_model_ried['latitude'], longitude=xr_model_ried['longitude'], method='nearest')\n",
    "\n",
    "clc_1000_aoi['clc_grp'] = (['latitude', 'longitude'], np.array([int(str(x)[0]) if str(x) != 'nan' else np.nan for x in np.ravel(clc_1000_aoi.values.tolist())]).reshape(clc_1000_aoi.shape))\n",
    "clc_1000_grp = clc_1000_aoi.to_dataset(name='clc_class').reset_coords(['clc_grp'])\n",
    "\n",
    "\n",
    "#lat & long for 500m resolution \n",
    "lon_500 = np.linspace(xr_model_aoi.longitude[0], xr_model_aoi.longitude[-1], xr_model_aoi.dims['longitude'] * 2)\n",
    "lat_500 = np.linspace(xr_model_aoi.latitude[0], xr_model_aoi.latitude[-1], xr_model_aoi.dims['latitude'] * 2)\n",
    "\n",
    "#lat & long for 250m resolution \n",
    "lon_250 = np.linspace(xr_model_aoi.longitude[0], xr_model_aoi.longitude[-1], xr_model_aoi.dims['longitude'] * 4)\n",
    "lat_250 = np.linspace(xr_model_aoi.latitude[0], xr_model_aoi.latitude[-1], xr_model_aoi.dims['latitude'] * 4)\n",
    "\n",
    "#interpolating resolution ~500m & ~250m\n",
    "clc_500_aoi = clc_100.interp(latitude=lat_500, longitude=lon_500, method='nearest').astype(int)\n",
    "clc_250_aoi = clc_100.interp(latitude=lat_250, longitude=lon_250, method='nearest').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrame from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_db = pd.read_csv('csv_new\\\\aoi_1000_v1.csv')\n",
    "ried_db = pd.read_csv('csv_new\\\\ried_1000_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show measurement positions on Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_location_ried = gpd.GeoDataFrame(ried_db, geometry=gpd.points_from_xy(ried_db.lon, ried_db.lat)).drop_duplicates('lonlat')\n",
    "gpd_location_ried = gpd_location_ried[gpd_location_ried.within(polygon)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate (latitude, longitude) pairs\n",
    "location_1 = np.unique(gpd_location_ried[['lat','lon']], axis=0)\n",
    "weights_1 = [abs(x) * 100 for x in gpd_location_ried.groupby('lonlat').agg({'slope_lineregress': \"mean\"}).slope_lineregress.values.tolist()]\n",
    "location_2 = np.unique(gpd_location_ried[gpd_location_ried['clc_category'] == 2][['lat','lon']], axis=0)\n",
    "weights_2 = [abs(x)* 100 for x in gpd_location_ried[gpd_location_ried['clc_category'] == 2].groupby('lonlat').agg({'slope_lineregress': \"mean\"}).slope_lineregress.values.tolist()]\n",
    "location_3 = np.unique(aoi_db[aoi_db['clc_category'] == 2][['lat','lon']], axis=0)[:-2]\n",
    "weights_3 = [abs(x) * 100 for x in aoi_db[aoi_db['clc_category'] == 2].groupby('lonlat').agg({'slope_lineregress': \"mean\"}).slope_lineregress.values.tolist()]\n",
    "location_4 = np.unique(gpd_location_ried[(gpd_location_ried['clc_category'] == 2)&(gpd_location_ried['ndvi'].between(0,0.2))& (gpd_location_ried['slope_lineregress'] < 0)][['lat','lon']], axis=0)\n",
    "\n",
    "figures = list()\n",
    "for loc,w in zip([location_1,location_2, location_3],[weights_1,weights_2,weights_3]):\n",
    "    heatmap_layer = gmaps.heatmap_layer(loc, w, max_intensity=13, dissipating=False, point_radius=0.01) #, weights=sm slope mean , gradient=['white', 'red', 'blue']\n",
    "    heatmap_layer.gradient = [\n",
    "          'rgba(0, 255, 255, 0)',\n",
    "          'rgba(0, 255, 255, 1)',\n",
    "          'rgba(0, 191, 255, 1)',\n",
    "          'rgba(0, 127, 255, 1)',\n",
    "          'rgba(0, 63, 255, 1)',\n",
    "          'rgba(0, 0, 255, 1)',\n",
    "          'rgba(0, 0, 223, 1)',\n",
    "          'rgba(0, 0, 191, 1)',\n",
    "          'rgba(0, 0, 159, 1)',\n",
    "          'rgba(0, 0, 127, 1)',\n",
    "          'rgba(63, 0, 91, 1)',\n",
    "          'rgba(127, 0, 63, 1)',\n",
    "          'rgba(191, 0, 31, 1)',\n",
    "          'rgba(255, 0, 0, 1)']\n",
    "    gmap = gmaps.figure(map_type='SATELLITE', layout={'width': '400px', 'height': '600px', 'padding': '3px','border': '1px solid black'}, zoom_level=8, center=location_1[0])\n",
    "    gmap.add_layer(heatmap_layer)\n",
    "    figures.append(gmap)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = widgets.HTML('<h3>Soil Moisture Measurements with mean of slope for single locations!</h3>')\n",
    "widgets.VBox([title, widgets.HBox(figures, layout={'width': '100%'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_minimal_html('figures//slope_ried.html', views=[figures[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center coordinates to set view \n",
    "center = (49.77611923217773, 8.490663528442383)   \n",
    "#create figure for gmaps \n",
    "gmap = gmaps.figure(center=center, map_type='SATELLITE', zoom_level=10, layout={'width': '800px', 'height': '600px'})\n",
    "#add locations with marker\n",
    "location = location_2\n",
    "marker = gmaps.marker_layer(locations=[(x[0],y[1]) for x,y in zip(location[:], location[:])], info_box_content=[str((x[0],y[1])) for x,y in zip(location[:], location[:])])\n",
    "symbol = gmaps.symbol_layer(locations=[(x[0],y[1]) for x,y in zip(location[:], location[:])], info_box_content=[str((x[0],y[1])) for x,y in zip(location[:], location[:])])\n",
    "#add markers to figure\n",
    "gmap.add_layer(symbol)\n",
    "\n",
    "#display map\n",
    "print('unique locations : ', len(location_1))\n",
    "print('unique locations RIED (with agriculture surface derived from corine land cover): ', len(location_2))\n",
    "print('unique locations AOI (with agriculture surface derived from corine land cover): ', len(location_3))\n",
    "display(gmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "embed_minimal_html('figures//ried_map_circle.html', views=[gmap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = [(49.64369583129883, 8.407676696777344), (49.64369583129883, 8.407676696777344),(49.71588134765625, 8.50103759765625),(49.72792434692383, 8.50103759765625),(49.93307876586914, 8.573651313781737) ,(49.92098617553711, 8.594398498535156)]\n",
    "r1 = [(49.8484992980957, 8.490663528442383),(49.8484992980957, 8.480290412902832),(49.8484992980957, 8.469917297363281),(49.8484992980957, 8.459543228149414),(49.8484992980957, 8.449170112609862),(49.8484992980957, 8.438796997070312),(49.8484992980957, 8.428422927856445),(49.8484992980957, 8.418049812316895), (49.8484992980957, 8.407676696777344), (49.8484992980957, 8.397302627563478)]\n",
    "r3 = [(49.64369583129883, 8.428422927856445), (49.64369583129883, 8.418049812316895), (49.64369583129883, 8.407676696777344),(49.67977905273438, 8.511410713195799),(49.67977905273438, 8.521783828735353),(49.667747497558594, 8.521783828735353)]\n",
    "r4_good_collection = [((49.78817367553711, 8.438796997070312)), (49.78817367553711, 8.490663528442383), (49.67977905273438, 8.50103759765625), (49.691810607910156, 8.573651313781737), (49.59563446044922, 8.573651313781737), (49.75201416015625, 8.584024429321289), (49.88473129272461, 8.418049812316895), (49.860572814941406, 8.407676696777344), (49.8484992980957, 8.407676696777344), (49.83642959594727, 8.407676696777344), (49.908897399902344, 8.469917297363281)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center coordinates to set view \n",
    "center = (49.77611923217773, 8.490663528442383)   \n",
    "#create figure for gmaps \n",
    "gmap = gmaps.figure(center=center, map_type='SATELLITE', zoom_level=10, layout={'width': '800px', 'height': '600px'})\n",
    "#add locations with marker\n",
    "location = r3\n",
    "marker = gmaps.marker_layer(locations=[(x[0],y[1]) for x,y in zip(location[:], location[:])], info_box_content=[str((x[0],y[1])) for x,y in zip(location[:], location[:])])\n",
    "#add markers to figure\n",
    "gmap.add_layer(marker)\n",
    "display(gmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrl_ried_r1 = [xr_model_ried.sel(longitude=x[1], latitude=x[0], method='nearest', tolerance=0.0001) for x in r1]\n",
    "xrl_ried_r2 = [xr_model_ried.sel(longitude=x[1], latitude=x[0], method='nearest', tolerance=0.0001) for x in r2]\n",
    "xrl_ried_r3 = [xr_model_ried.sel(longitude=x[1], latitude=x[0], method='nearest', tolerance=0.0001) for x in r3]\n",
    "\n",
    "pdl_ried_r1 = [x.to_dataframe() for x in xrl_ried_r1]\n",
    "pdl_ried_r2 = [x.to_dataframe() for x in xrl_ried_r2]\n",
    "pdl_ried_r3 = [x.to_dataframe() for x in xrl_ried_r3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.suptitle('Time Series for Soil Moisture, Precipitation & Real Evapotranspiration from 10 points on same latitude with rolling mean of 7 days', fontsize=16, y=1.08)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, nrows=5, figsize=(45,25), gridspec_kw={'width_ratios': [3, 1,1]})\n",
    "for line in pdl_ried_r3[::1]:\n",
    "    line = line.resample('7d').mean()\n",
    "    line_m = np.isfinite(line.soil_moisture_1km)\n",
    "    line_sm = line.soil_moisture_1km[line_m]\n",
    "    for i,years in enumerate([2015,2016,2017,2018,2019]):\n",
    "        line_sm[line_sm.index.year == years].plot(ax=ax[i,0], linewidth=1.3, legend=True, label='%s' %(str(line.longitude[0])[:4]), marker='o', linestyle='-')\n",
    "        line[line.index.year == years].real_evapotranspiration.plot(ax=ax[i,1], linewidth=0.2)\n",
    "        line[line.index.year == years].precipitation_1km.plot(ax=ax[i,2], linewidth=0.2)\n",
    "        ax[i,0].legend(loc='right')\n",
    "        ax[i,0].tick_params('both', labelsize=11)\n",
    "        #plt.xlabel('longitude', fontsize=13,  labelpad=10)\n",
    "        ax[i,0].set_ylabel('Soil Moisture cm³/cm³', fontsize=13,  labelpad=10)\n",
    "        ax[i,1].set_ylabel('Precipitation mm', fontsize=13,  labelpad=10)\n",
    "        ax[i,2].set_ylabel('Real Evapotranspiration', fontsize=13,  labelpad=10)\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures//%s.png' %('ts_r3_7d'))#K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "default_cycler = (cycler(color=['blue', 'red', 'black', 'yellow', 'orange', 'purple']))\n",
    "\n",
    "plt.rc('axes', prop_cycle=default_cycler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=5, figsize=(45,20), gridspec_kw={'width_ratios': [3, 1,1]}, dpi=150)\n",
    "colors=['blue', 'red', 'black', 'yellow', 'orange']\n",
    "for line in pdl_ried_r3[::1]:\n",
    "    line = line.resample('14d').mean()\n",
    "    line_m = np.isfinite(line.soil_moisture_1km)\n",
    "    line_sm = line.soil_moisture_1km[line_m]\n",
    "    for i,years in enumerate([2015,2016,2017,2018,2019]):\n",
    "        line_sm[line_sm.index.year == years].plot(ax=ax[i,0], linewidth=2, legend=True, label='%s' %(str(line.longitude[0])[:4]), marker='o', linestyle='-')\n",
    "        line[line.index.year == years].real_evapotranspiration.plot(ax=ax[i,1], linewidth=0.2)\n",
    "        line[line.index.year == years].precipitation_1km.plot(ax=ax[i,2], linewidth=0.2)\n",
    "        ax[i,0].legend(loc='right',fontsize=16)\n",
    "        ax[i,0].tick_params('both', labelsize=15) \n",
    "        #plt.xlabel('longitude', fontsize=13,  labelpad=10)\n",
    "        ax[i,0].set_ylabel('Soil Moisture cm³/cm³', fontsize=16,  labelpad=10)\n",
    "        ax[i,1].set_ylabel('Precipitation mm', fontsize=16,  labelpad=10)\n",
    "        ax[i,2].set_ylabel('Real Evapotranspiration', fontsize=16,  labelpad=10)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('ts_r3_14d'))#K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from east to west\n",
    "collection_se = [(49.57161712646485, 8.656639099121094),(49.57161712646485, 8.646265983581543), (49.57161712646485, 8.635891914367676), (49.57161712646485, 8.625518798828125),(49.57161712646485, 8.615145683288574),(49.57161712646485, 8.604771614074707),(49.57161712646485, 8.594398498535156)]\n",
    "r_1 = (49.667747497558594, 8.397302627563478)\n",
    "r_2 = (49.63167572021485, 8.407676696777344)\n",
    "r_3 = (49.59563446044922, 8.428422927856445)\n",
    "r_4 = (49.908897399902344, 8.532157897949219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://geopandas.org/gallery/plotting_basemap_background.html#sphx-glr-gallery-plotting-basemap-background-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find locations for soil samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority list: (high  -> low) \n",
    "**1. cells with 0 & 0.5 & 1 mm rain**    \n",
    "**2. cells with NDVI between (0.036 - 0.2) ~bare soil**  \n",
    "**3. cells with specifi clc_2018 category (211-216??)**  \n",
    "**4. cells with highest count on different meassurements on same coordinate pair**     \n",
    "**5. cells with highest count on sm values within a period**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raiting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add period score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores\n",
    "sm_measurements_score = 0.2\n",
    "ndvi_score = 0.2\n",
    "ndvi_bare = 1\n",
    "lonlat_count_score = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([-0.5,-0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add score column to periods\n",
    "aoi_db['score'] = aoi_db.sm_measurements * sm_measurements_score  + aoi_db.lonlat_count * lonlat_count_score + aoi_db.ndvi.mul(ndvi_score, fill_value=0) + aoi_db.ndvi.between(0,0.2).mul(ndvi_bare) \n",
    "ried_db['score'] = ried_db.sm_measurements * sm_measurements_score  + ried_db.lonlat_count * lonlat_count_score + ried_db.ndvi.mul(ndvi_score, fill_value=0) + ried_db.ndvi.between(0,0.2).mul(ndvi_bare) \n",
    "\n",
    "#add score to latlon group\n",
    "aoi_db['location_count'] = aoi_db.groupby(by='lonlat').score.transform('sum')\n",
    "ried_db['location_count'] = ried_db.groupby(by='lonlat').score.transform('sum')\n",
    "#ried_db['slope_score'] = abs(ried_db.slope_lineregress)\n",
    "ried_db['slope_score_sum'] = ried_db.groupby(by='lonlat').slope_lineregress.transform('sum')\n",
    "\n",
    "ried_db.head()\n",
    "\n",
    "#groupby location score\n",
    "#dfg_location_score = df_lonlat.groupby(by='location_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_ried_db_complete = gpd.GeoDataFrame(ried_db, geometry=gpd.points_from_xy(ried_db.lon, ried_db.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(gpd_location_ried))\n",
    "print(len(gpd_location_ried[gpd_location_ried['clc_category'] == 2]))\n",
    "print(len(gpd_location_ried[(gpd_location_ried['clc_category'] == 2)&(gpd_location_ried['ndvi'].between(0,0.2))]))\n",
    "print(len(gpd_location_ried[(gpd_location_ried['clc_category'] == 2)&(gpd_location_ried['ndvi'].between(0,0.2))]))\n",
    "print(len(gpd_location_ried[(gpd_location_ried['clc_category'] == 2)&(gpd_location_ried['ndvi'].between(0,0.2))& (gpd_location_ried['slope_lineregress'] < 0)]))\n",
    "goal = gpd_location_ried[(gpd_location_ried['clc_category'] == 2)&(gpd_location_ried['ndvi'].between(0,0.2)) & (gpd_location_ried['slope_lineregress'] < 0)]\n",
    "goal\n",
    "#len(ried_db[ried_db['ndvi'].between(0,0.2)].groupby(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create boolean mask for clc category 2 \n",
    "mask_clc_ried = (ried_db['clc_category'] == 2).tolist()\n",
    "mask_clc_aoi = (aoi_db['clc_category'] == 2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove entries with clc unequal to category agricutlutre (2)\n",
    "aoi_db = aoi_db[mask_clc_aoi]\n",
    "ried_db = ried_db[mask_clc_ried]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create geopandas database \n",
    "aoi_db['sum_measurements_location'] = aoi_db.drop('Unnamed: 0', axis=1).groupby('lonlat').sm_measurements.transform('sum')\n",
    "aoi_db_llw = aoi_db.drop_duplicates('lonlat')\n",
    "gpd_aoi_db = gpd.GeoDataFrame(aoi_db_llw[['lonlat_count', 'sum_measurements_location']], geometry=gpd.points_from_xy(aoi_db_llw.lon, aoi_db_llw.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gpd_aoi_db.plot(markersize=[math.sqrt(x)*4 for x in gpd_aoi_db.sum_measurements_location], legend=True)\n",
    "mplleaflet.show(fig=ax.figure, path='figures//gpd_aoi_db.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Timeseries for tslearn and other machine learning modules  \n",
    "subset the timeseries to the dry periods otherwise noise is to strength and even better with equal evp values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aoi update_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file into variable\n",
    "with open(paths[0] + 'json//update_3_v1_aoi_clc1000m.txt') as json_file:\n",
    "    update_3_aoi = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ried update_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file into variable\n",
    "with open(paths[0] + 'json//update_3_v1_ried_clcl1000m.txt') as json_file:\n",
    "    update_3_ried = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lists == in pandas = objects to be avoided if possible\n",
    "days_number_list_aoi = [x[4][2][0] for x in update_3_aoi]\n",
    "days_list_aoi = [pd.date_range(x[2], periods=(x[4][0] + 1))[1:] for x in update_3_aoi]\n",
    "sm_list_aoi = [x[4][2][1] for x in update_3_aoi]\n",
    "pp_list_aoi = [x[4][2][2] for x in update_3_aoi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lists == in pandas = objects to be avoided if possible\n",
    "days_number_list_ried = [x[4][2][0] for x in update_3_ried]\n",
    "days_list_ried = [pd.date_range(x[2], periods=(x[4][0] + 1))[1:] for x in update_3_ried]\n",
    "sm_list_ried = [x[4][2][1] for x in update_3_ried]\n",
    "pp_list_ried = [x[4][2][2] for x in update_3_ried]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ried_db), len(days_list_ried), len(days_number_list_ried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_number_list_aoi = np.array(days_number_list_aoi)\n",
    "days_number_list_ried = np.array(days_number_list_ried)\n",
    "\n",
    "days_list_aoi = np.array(days_list_aoi)\n",
    "days_list_ried = np.array(days_list_ried)\n",
    "\n",
    "sm_list_aoi = np.array(sm_list_aoi)\n",
    "sm_list_ried = np.array(sm_list_ried)\n",
    "\n",
    "pp_list_aoi = np.array(pp_list_aoi)\n",
    "pp_list_ried = np.array(pp_list_ried)\n",
    "\n",
    "# lists\n",
    "lon_aoi = [x[0] for x in update_3_aoi]\n",
    "lon_ried = [x[0] for x in update_3_ried]\n",
    "\n",
    "lat_aoi = [x[1] for x in update_3_aoi]\n",
    "lat_ried = [x[1] for x in update_3_ried]\n",
    "\n",
    "event_date_aoi = [pd.Timestamp(x[2]) for x in update_3_aoi]\n",
    "event_date_ried = [pd.Timestamp(x[2]) for x in update_3_ried]\n",
    "\n",
    "event_pp_aoi = [x[3] for x in update_3_aoi]\n",
    "event_pp_ried = [x[3] for x in update_3_ried]\n",
    "\n",
    "event_et_mean_aoi = [x[4][1] for x in update_3_aoi]\n",
    "event_et_mean_ried = [x[4][1] for x in update_3_ried]\n",
    "\n",
    "periode_duration_aoi = [x[4][0] for x in update_3_aoi]\n",
    "periode_duration_ried = [x[4][0] for x in update_3_ried]\n",
    "\n",
    "sm_measurements_aoi = [np.count_nonzero(np.isfinite(x[4][2][1])) for x in update_3_aoi]\n",
    "sm_measurements_ried = [np.count_nonzero(np.isfinite(x[4][2][1])) for x in update_3_ried]\n",
    "\n",
    "slope_polyfit1d_aoi =  [x[5][0] for x in update_3_aoi]\n",
    "slope_polyfit1d_ried =  [x[5][0] for x in update_3_ried]\n",
    "\n",
    "intercept_polyfit1d_aoi =  [x[5][1] for x in update_3_aoi]\n",
    "intercept_polyfit1d_ried =  [x[5][1] for x in update_3_ried]\n",
    "\n",
    "slope_lineregress_aoi = [x[6][0] for x in update_3_aoi]\n",
    "slope_lineregress_ried = [x[6][0] for x in update_3_ried]\n",
    "\n",
    "intercept_lineregress_aoi = [x[6][1] for x in update_3_aoi]\n",
    "intercept_lineregress_ried = [x[6][1] for x in update_3_ried]\n",
    "\n",
    "ndvi_aoi = [np.nanmean(x[9]) for x in update_3_aoi]\n",
    "ndvi_ried = [np.nanmean(x[9]) for x in update_3_ried]\n",
    "\n",
    "clc_2018_aoi = [x[7][0] for x in update_3_aoi]\n",
    "clc_2018_ried = [x[7][0] for x in update_3_ried]\n",
    "\n",
    "soil_map_aoi = [x[7][1] for x in update_3_aoi]\n",
    "soil_map_ried = [x[7][1] for x in update_3_ried]\n",
    "\n",
    "lonlat_aoi = [lon+lat for lon,lat in zip(lon_aoi,lat_aoi)]\n",
    "lonlat_ried = [lon+lat for lon,lat in zip(lon_ried,lat_ried)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dry_periods on agriculture land: ', len(aoi_db), len(ried_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lonlat_aoi) == len(soil_map_aoi) == len(clc_2018_aoi) == len(ndvi_aoi) == len(intercept_lineregress_aoi) == len(slope_lineregress_aoi) == len(intercept_polyfit1d_aoi) == len(slope_polyfit1d_aoi) == len(sm_measurements_aoi) == len(periode_duration_aoi) == len(event_et_mean_aoi) == len(event_pp_aoi) == len(event_date_aoi) == len(lat_aoi) == len(lon_aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts(days, sm):\n",
    "    masks = [np.isfinite(x) for x in sm]\n",
    "    days = [np.array(x)[mask] for x, mask in zip(days,masks)]\n",
    "    sm = [np.array(x)[mask] for x, mask in zip(sm,masks)]\n",
    "    evp = event_et_mean_aoi\n",
    "    slope = slope_lineregress_aoi\n",
    "    count_sm = sm_measurements_aoi\n",
    "    lonlat = lonlat_aoi\n",
    "    clc = clc_2018_aoi\n",
    "    ndvi = ndvi_aoi\n",
    "    slope = slope_lineregress_aoi\n",
    "    lon = lon_aoi\n",
    "    lat = lat_aoi\n",
    "    return sm, days, evp, slope, count_sm, lonlat, clc, ndvi, slope, lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm, days, evp, slope, count_sm, lonlat, clc, ndvi,slope, lon, lat = get_ts(days_number_list_aoi,sm_list_aoi)\n",
    "len(sm), len(days), len(evp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with lat lon and mean of slopes\n",
    "slopes = pd.DataFrame({'lonlat' : lonlat, 'slope' : slope, 'lon' : lon, 'lat': lat,'clc' : clc})\n",
    "slopes_clc = slopes[slopes['clc'] < 300]\n",
    "slopes_mean = slopes.groupby('lonlat').mean().reset_index(drop=True)\n",
    "slopes_mean_clc = slopes_clc.groupby('lonlat').mean().reset_index(drop=True)\n",
    "print(len(slopes_clc), len(slopes))\n",
    "\n",
    "#find minimum sm value within dry periods:\n",
    "sm_minimum = pd.DataFrame({'lonlat' : lonlat, 'lon' : lon, 'lat': lat,'clc' : clc, 'sm' : [min(x) for x in sm]})\n",
    "sm_minimum_mean = sm_minimum.groupby('lonlat').mean().reset_index(drop=True)\n",
    "\n",
    "#create gpd multipoint with shaply geometry from lon & lat equal to [Point(x, y) for x, y in zip(df.Longitude, df.Latitude)]\n",
    "gpd_slopes = gpd.GeoDataFrame(slopes_mean, geometry=gpd.points_from_xy(slopes_mean.lon, slopes_mean.lat))\n",
    "gpd_slopes_clc = gpd.GeoDataFrame(slopes_mean, geometry=gpd.points_from_xy(slopes_mean.lon, slopes_mean.lat))\n",
    "gpd_sm_min_mean = gpd.GeoDataFrame(sm_minimum_mean, geometry=gpd.points_from_xy(sm_minimum_mean.lon, sm_minimum_mean.lat))\n",
    "\n",
    "#intersection\n",
    "polygon = vec_ried.geometry[0]\n",
    "gpd_slopes_ried = gpd_slopes[gpd_slopes.within(polygon)]\n",
    "gpd_slopes_ried_clc = gpd_slopes[gpd_slopes.within(polygon)]\n",
    "gpd_sm_min_mean_ried = gpd_sm_min_mean[gpd_sm_min_mean.within(polygon)]\n",
    "\n",
    "polygon = vec_ried.geometry[0]\n",
    "gdf_groups_ried = gdf_groups_aoi[gdf_groups_aoi.within(polygon)]\n",
    "gdf_groups_f_ried = gdf_groups_f_aoi[gdf_groups_f_aoi.within(polygon)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,dpi=300)\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "divider_2 = make_axes_locatable(ax[1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "#soil_map_data.plot('HAUPTGRUPP', figsize=(24,12), markersize=120, cmap='Accent', legend=True, ax=ax)\n",
    "gpd_sm_mean.plot(column='sm', markersize=20, legend=True, ax=ax[0], cax=cax, alpha=1, cmap='plasma_r')\n",
    "gpd_sm_mean_ried.plot(column='sm', markersize=60, legend=True, ax=ax[1],cax=cax_2, alpha=1, cmap='plasma_r')\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1])\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0].set_title('Min Soil Moisture  within dry periods')\n",
    "ax[1].set_title('Min Soil Moisture  within dry periods')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('sm_min_dry_periods'))#K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,dpi=300)\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "divider_2 = make_axes_locatable(ax[1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "#soil_map_data.plot('HAUPTGRUPP', figsize=(24,12), markersize=120, cmap='Accent', legend=True, ax=ax)\n",
    "gpd_sm_min_mean.plot(column='sm', markersize=20, legend=True, ax=ax[0], cax=cax, alpha=1, cmap='plasma_r', vmax=0.3)\n",
    "gpd_sm_min_mean_ried.plot(column='sm', markersize=85, legend=True, ax=ax[1],cax=cax_2, alpha=1, cmap='plasma_r', vmax=0.3)\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1])\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0].set_title('Min Soil Moisture  within dry periods')\n",
    "ax[1].set_title('Min Soil Moisture  within dry periods')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('sm_min_dry_periods'))#K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplleaflet\n",
    "ax = gpd_kmeans_f_aoi.plot(column='group')\n",
    "mplleaflet.show(fig=ax.figure, path='figures//gpd_kmeans_aoi_f_map_7.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplleaflet\n",
    "ax = gpd_sm_min_mean.plot(column='sm', markersize=20, legend=True, alpha=1, cmap='plasma_r', vmax=0.3)\n",
    "mplleaflet.show(fig=ax.figure, path='figures//mean_min_sm.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,dpi=300)\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "divider_2 = make_axes_locatable(ax[1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "#soil_map_data.plot('HAUPTGRUPP', figsize=(24,12), markersize=120, cmap='Accent', legend=True, ax=ax)\n",
    "gpd_slopes.plot(column='slope', markersize=20, legend=True, ax=ax[0], cax=cax, alpha=1, cmap='gist_rainbow')\n",
    "gpd_slopes_ried.plot(column='slope', markersize=60, legend=True, ax=ax[1],cax=cax_2, alpha=1, cmap='gist_rainbow')\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1])\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0].set_title('Mean slope from regression line within dry periods ')\n",
    "ax[1].set_title('Mean slope from regression line within dry periods ')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('Mean_slope_dry_periods'))#K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,dpi=300)\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "divider_2 = make_axes_locatable(ax[1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "#soil_map_data.plot('HAUPTGRUPP', figsize=(24,12), markersize=120, cmap='Accent', legend=True, ax=ax)\n",
    "gpd_slopes_clc.plot(column='slope', markersize=20, legend=True, ax=ax[0], cax=cax, alpha=1, cmap='gist_rainbow')\n",
    "gpd_slopes_ried_clc.plot(column='slope', markersize=60, legend=True, ax=ax[1],cax=cax_2, alpha=1, cmap='gist_rainbow')\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1])\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0].set_title('Mean slope from regression line within dry periods over agriculture surface ')\n",
    "ax[1].set_title('Mean slope from regression line within dry periods over agriculture surface')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('Mean_slope_dry_periods_clc'))#K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,dpi=300)\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "divider_2 = make_axes_locatable(ax[1])\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cax_2 = divider_2.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "#soil_map_data.plot('HAUPTGRUPP', figsize=(24,12), markersize=120, cmap='Accent', legend=True, ax=ax)\n",
    "gpd_slopes.plot(column='slope', markersize=20, legend=True, ax=ax[0], cax=cax, alpha=1, cmap='gist_rainbow', vmax=0)\n",
    "gpd_slopes_ried.plot(column='slope', markersize=60, legend=True, ax=ax[1],cax=cax_2, alpha=1, cmap='gist_rainbow', vmax=0)\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[0])\n",
    "vec_ried.geometry.boundary.plot(color=None,edgecolor='k', linewidth = 3, ax=ax[1])\n",
    "\n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*3, Size[1]*3, forward=True) # Set forward to True to resize window along with plot in figure\n",
    "ax[0].set_title('Mean slope from regression line within dry periods - only negative slopes')\n",
    "ax[1].set_title('Mean slope from regression line within dry periods - only negative slopes')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//%s.png' %('Mean_slope_dry_periods_negative_slopes'))#K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means clustering\n",
    "X = np.array(extracted_features_sm_s.iloc[:,:20])\n",
    "y_iris = np.array(extracted_features_sm_s['Hauptgruppe'])\n",
    "km2 = cluster.KMeans(n_clusters=4).fit(X)\n",
    "km3 = cluster.KMeans(n_clusters=3).fit(X)\n",
    "km4 = cluster.KMeans(n_clusters=7).fit(X)\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km2.labels_)\n",
    "plt.title(\"K=2, J=%.2f\" % km2.inertia_)\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km3.labels_)\n",
    "plt.title(\"K=3, J=%.2f\" % km3.inertia_)\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km4.labels_)#.astype(np.float))\n",
    "plt.title(\"K=7, J=%.2f\" % km4.inertia_)\n",
    "np.unique(y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = pd.DataFrame({'day' : days, 'sm' : sm, 'lonlat' : lonlat, 'clc' : clc, 'ndvi' : ndvi})\n",
    "ts_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.generators import random_walks\n",
    "X = random_walks(n_ts=50, sz=32, d=1)\n",
    "km = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\", max_iter=5,\n",
    "                      random_state=0).fit(X)\n",
    "km.cluster_centers_.shape\n",
    "#3,32, 1)\n",
    "km_dba = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\", max_iter=5,\n",
    "                          max_iter_barycenter=5,\n",
    "                          random_state=0).fit(X)\n",
    "km_dba.cluster_centers_.shape\n",
    "#3,32, 1)\n",
    "km_sdtw = TimeSeriesKMeans(n_clusters=3, metric=\"softdtw\", max_iter=5,\n",
    "                           max_iter_barycenter=5,\n",
    "                           metric_params={\"gamma\": .5},\n",
    "                           random_state=0).fit(X)\n",
    "km_sdtw.cluster_centers_.shape\n",
    "#3,32, 1)\n",
    "X_bis = to_time_series_dataset([[1, 2, 3, 4],\n",
    "                                [1, 2, 3],\n",
    "                                [2, 5, 6, 7, 8, 9]])\n",
    "km = TimeSeriesKMeans(n_clusters=2, max_iter=5,\n",
    "                      metric=\"dtw\", random_state=0).fit(X_bis)\n",
    "km.cluster_centers_.shape\n",
    "#2,3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display location with figures from periods within the location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lonlat = ried_db\n",
    "days_number_list = days_number_list_ried\n",
    "days_list = days_list_ried\n",
    "sm_list = sm_list_ried\n",
    "pp_list = pp_list_ried\n",
    "ndvi = ndvi_ried\n",
    "event_et_mean = event_et_mean_ried\n",
    "slope_lineregress = slope_lineregress_ried\n",
    "intercept_lineregress = intercept_lineregress_ried\n",
    "\n",
    "for location in r3:\n",
    "    #filter periods for location\n",
    "    df_location = df_lonlat[(df_lonlat['lon'].isin([location[1]])) & (df_lonlat['lat'].isin([location[0]]))]\n",
    "    #sort \n",
    "    df_location_sort = df_location.sort_values('sm_measurements', ascending=False) #sm_measurements\n",
    "    #get index of df for locations to acces the sm, pp, days lists\n",
    "    df_index = df_location_sort.index.tolist()\n",
    "    #get period counts and sqrt it to get ncols * nrows for plotting\n",
    "    periods_len = int(math.sqrt(len(df_index)))\n",
    "    #gmaps image with marker on lat/lon \n",
    "    center=(df_location_sort.lat.tolist()[0],df_location_sort.lon.tolist()[0])    \n",
    "    buf = Point(center).buffer(0.01, cap_style=3) #0.0156109,-0.00915375\n",
    "    buf_dict = [x for x in gpd.GeoSeries([buf]).__geo_interface__['features'][0]['geometry']['coordinates'][0]]\n",
    "    lat = df_location_sort.lat.tolist()[0]\n",
    "    lon = df_location_sort.lon.tolist()[0]\n",
    "    lon_transform = 0.0156109 / 2\n",
    "    lat_transform = 0.00915375 / 2\n",
    "    aoi = [(lat + lat_transform, lon + lon_transform), (lat + lat_transform, lon - lon_transform), (lat - lat_transform, lon - lon_transform), (lat - lat_transform, lon + lon_transform), (lat + lat_transform, lon + lon_transform)]\n",
    "    gmap = gmaps.figure(center=center, map_type='SATELLITE', zoom_level=14, layout={'width': '300', 'height': '300px'})\n",
    "    pol = gmaps.Polygon(aoi, stroke_color='red', stroke_weight=4)\n",
    "    drawing = gmaps.drawing_layer(features=[pol])\n",
    "    gmap.add_layer(drawing)\n",
    "    #create widget for displaying gmaps\n",
    "    display(gmap)\n",
    "    #display(df_location_sort)\n",
    "    #create figure & axis\n",
    "    fig, axs = plt.subplots(ncols=periods_len, nrows=periods_len, figsize=(35,25))\n",
    "\n",
    "    #loop through all periods within same location \n",
    "    for index, ax in zip(df_index, fig.axes):\n",
    "        #get single period \n",
    "        df_period = pd.DataFrame({'time' : days_list[index], 'days' : days_number_list[index], 'soil_moisture' : sm_list[index], \n",
    "                                  'precipitation' : pp_list[index], 'ndvi' : ndvi[index], 'et_r' :  event_et_mean[index], \n",
    "                                  'slope_lineregress' : slope_lineregress[index], 'intercept_lineregress' : intercept_lineregress[index]})\n",
    "    \n",
    "        #Mask where nans to get a not braking line between them\n",
    "        mask = [np.isfinite(x) for x in sm_list[index]]\n",
    "\n",
    "        #plot lineregression\n",
    "        lgx = np.linspace(0, len(df_period.days.values.tolist()), len(df_period.days.values.tolist()))\n",
    "        slope = df_period.slope_lineregress*lgx+df_period.intercept_lineregress\n",
    "        df_slope = pd.DataFrame({'lgx' : lgx, 'slope' : slope})\n",
    "        ax.plot('lgx','slope', data = df_slope, color='black', alpha=0.5)\n",
    "        \n",
    "        #plot time (x) vs. soil moisture (y)\n",
    "        ax.plot('days', 'soil_moisture', data=df_period[mask], marker='o',markerfacecolor='blue' ,linestyle='dotted', color='red', linewidth=6, markersize=15)\n",
    "\n",
    "        #create label for axes x & y & #change tick color\n",
    "        #ax.set_xlabel('time', color='blue')\n",
    "        ax.set_ylabel('soil moisture m³/m³', color='black', fontsize=18)\n",
    "        ax.set_xlabel('time', color='black', fontsize=15)\n",
    "        ax.set_ylim(0,0.8)\n",
    "        ax.tick_params(axis='y', labelcolor='red', labelsize=18)\n",
    "        ax.tick_params(axis='x', labelsize=15)\n",
    "\n",
    "        #create text annotation\n",
    "        table = ax.table(cellText=[['p_start', 'p.length', 'sm.count', 'ndvi', 'evapoT_r' ],\n",
    "                           [str(df_period.time.min()).replace('','')[2:10], str(len(df_period.days.values.tolist())), str(len(df_period[mask].soil_moisture.values.tolist())), str(df_period[mask].ndvi.mean())[0:4], str(df_period.et_r.mean())[0:4] ]],\n",
    "                             cellLoc='center', loc=16)#.auto_set_font_size(False)#.set_fontsize(20)\n",
    "        table.set_fontsize(20)\n",
    "        ax.legend(loc='upper left', fontsize='large')\n",
    "        \n",
    "        # create the xaxis label\n",
    "        plt.setp(ax.xaxis.get_label(), visible=True, text='time (days)')\n",
    "\n",
    "        # instantiate a second axes that shares the same x-axis and plot precipitation data\n",
    "        ax2 = ax.twinx() \n",
    "        ax2.bar('days', 'precipitation', data=df_period, alpha=0.45, color='blue', label='precipitation ' )\n",
    "\n",
    "        #create label for axis (y2)\n",
    "        ax2.set_ylabel('precipitation radolan 1km² (mm/24h)', color='black', fontsize=18)  \n",
    "        ax2.set_ylim(0,1)\n",
    "        \n",
    "        #change tick color\n",
    "        ax2.tick_params(axis='y', labelcolor='blue', labelsize=18)\n",
    "\n",
    "        ax2.legend(loc=0, fontsize='large')\n",
    "\n",
    "    fig.tight_layout(h_pad=4.5, w_pad = 3.0)\n",
    "    \n",
    "    plt.savefig('figures//locs//r3//%s.png' %(str(location).replace('(','').replace(')', '').replace(', ', '_').replace('.','_'))) #K means clustering\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lonlat = aoi_db\n",
    "days_number_list = days_number_list_aoi\n",
    "days_list = days_list_aoi\n",
    "sm_list = sm_list_aoi\n",
    "pp_list = pp_list_aoi\n",
    "ndvi = ndvi_aoi\n",
    "event_et_mean = event_et_mean_aoi\n",
    "slope_lineregress = slope_lineregress_aoi\n",
    "intercept_lineregress = intercept_lineregress_aoi\n",
    "\n",
    "for location in df_lonlat.sort_values('location_count', ascending=False)['lonlat'].unique()[0:5]:\n",
    "    #filter periods for location\n",
    "    df_location = df_lonlat[df_lonlat['lonlat'].isin([location])]\n",
    "    #sort \n",
    "    df_location_sort = df_location.sort_values('sm_measurements', ascending=False)\n",
    "    display(df_location_sort)\n",
    "    #get index of df for locations to acces the sm, pp, days lists\n",
    "    df_index = df_location_sort.index.tolist()\n",
    "    #get period counts and sqrt it to get ncols * nrows for plotting\n",
    "    periods_len = int(math.sqrt(len(df_index)))\n",
    "    #gmaps image with marker on lat/lon \n",
    "    center=(df_location_sort.lat.tolist()[0],df_location_sort.lon.tolist()[0])    \n",
    "    buf = Point(center).buffer(0.01, cap_style=3) #0.0156109,-0.00915375\n",
    "    buf_dict = [x for x in gpd.GeoSeries([buf]).__geo_interface__['features'][0]['geometry']['coordinates'][0]]\n",
    "    lat = df_location_sort.lat.tolist()[0]\n",
    "    lon = df_location_sort.lon.tolist()[0]\n",
    "    lon_transform = 0.0156109 / 2\n",
    "    lat_transform = 0.00915375 / 2\n",
    "    aoi = [(lat + lat_transform, lon + lon_transform), (lat + lat_transform, lon - lon_transform), (lat - lat_transform, lon - lon_transform), (lat - lat_transform, lon + lon_transform), (lat + lat_transform, lon + lon_transform)]\n",
    "    gmap = gmaps.figure(center=center, map_type='SATELLITE', zoom_level=13, layout={'width': '400', 'height': '300px'})\n",
    "    pol = gmaps.Polygon(aoi, stroke_color='red')\n",
    "    drawing = gmaps.drawing_layer(features=[pol])\n",
    "    gmap.add_layer(drawing)\n",
    "    #create widget for displaying gmaps\n",
    "    display(gmap)\n",
    "    #create figure & axis\n",
    "    fig, axs = plt.subplots(ncols=periods_len, nrows=periods_len, figsize=(25,15))\n",
    "\n",
    "    #loop through all periods within same location \n",
    "    for index, ax in zip(df_index, fig.axes):\n",
    "        #get single period \n",
    "        df_period = pd.DataFrame({'time' : days_list[index], 'days' : days_number_list[index], 'soil_moisture' : sm_list[index], \n",
    "                                  'precipitation' : pp_list[index], 'ndvi' : ndvi[index], 'et_r' :  event_et_mean[index], \n",
    "                                  'slope_lineregress' : slope_lineregress[index], 'intercept_lineregress' : intercept_lineregress[index]})\n",
    "    \n",
    "        #Mask where nans to get a not braking line between them\n",
    "        mask = [np.isfinite(x) for x in sm_list[index]]\n",
    "\n",
    "        #plot lineregression\n",
    "        lgx = np.linspace(0, len(df_period.days.values.tolist()), len(df_period.days.values.tolist()))\n",
    "        slope = df_period.slope_lineregress*lgx+df_period.intercept_lineregress\n",
    "        df_slope = pd.DataFrame({'lgx' : lgx, 'slope' : slope})\n",
    "        ax.plot('lgx','slope', data = df_slope, color='black', alpha=0.5)\n",
    "        \n",
    "        #plot time (x) vs. soil moisture (y)\n",
    "        ax.plot('days', 'soil_moisture', data=df_period[mask], marker='o', linestyle='dotted', color='red')\n",
    "\n",
    "        #create label for axes x & y & #change tick color\n",
    "        #ax.set_xlabel('time', color='blue')\n",
    "        ax.set_ylabel('soil moisture m³/m³', color='black')\n",
    "        ax.set_ylim(0,0.8)\n",
    "        ax.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        #create text annotation\n",
    "        ax.table(cellText=[['p_start', 'p.length', 'sm.count', 'ndvi', 'evapoT_r' ],\n",
    "                           [str(df_period.time.min()).replace('','')[2:10], str(len(df_period.days.values.tolist())), str(len(df_period[mask].soil_moisture.values.tolist())), str(df_period[mask].ndvi.mean())[0:4], str(df_period.et_r.mean())[0:4] ]],\n",
    "                             cellLoc='center', loc=16).auto_set_font_size(False)#.set_fontsize(20)\n",
    "        \n",
    "        ax.legend(loc='upper left')\n",
    "        \n",
    "        # create the xaxis label\n",
    "        plt.setp(ax.xaxis.get_label(), visible=True, text='time (days)')\n",
    "\n",
    "        # instantiate a second axes that shares the same x-axis and plot precipitation data\n",
    "        ax2 = ax.twinx() \n",
    "        ax2.bar('days', 'precipitation', data=df_period, alpha=0.45, color='blue', label='precipitation ' )\n",
    "\n",
    "        #create label for axis (y2)\n",
    "        ax2.set_ylabel('precipitation radolan 1km² (mm/24h)', color='black')  \n",
    "        ax2.set_ylim(0,1)\n",
    "        \n",
    "        #change tick color\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        ax2.legend(loc=0)\n",
    "\n",
    "    fig.tight_layout(h_pad=4.5, w_pad = 3.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sample location with NDVI and corine land cover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_collection = [(49.848499, 8.428423),(49.62977731115676, 8.428339501487915),(49.63273697221979, 8.434362993266106)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pd dataframe & reset index and drop all nan values\n",
    "df_ndvi = ndvi_datasets[0].NDVI.to_dataframe().reset_index().dropna()\n",
    "#create unique identifier for location \n",
    "df_ndvi['latlon'] = df_ndvi['latitude'] + df_ndvi['longitude']\n",
    "#group by this identifier\n",
    "locations_low_ndvi = df_ndvi.groupby('latlon').mean().sort_values('NDVI').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center coordinates to set view \n",
    "center = (locations_low_ndvi.latitude.tolist()[0],locations_low_ndvi.longitude.tolist()[0])    \n",
    "#create figure for gmaps \n",
    "gmap = gmaps.figure(center=center, map_type='SATELLITE', zoom_level=11, layout={'width': '800px', 'height': '600px'})\n",
    "#add locations with marker\n",
    "marker = gmaps.marker_layer(locations=[(x,y) for x,y in zip(locations_low_ndvi[0:25].latitude.tolist(), locations_low_ndvi[0:25].longitude.tolist())], info_box_content=[str((x,y)) for x,y in zip(locations_low_ndvi[0:25].latitude.tolist(), locations_low_ndvi[0:25].longitude.tolist())])\n",
    "#add markers to figure\n",
    "gmap.add_layer(marker)\n",
    "#display map\n",
    "display(gmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_year = np.arange(1,13,1)\n",
    "time_doy = np.arange(1,367,1)\n",
    "time_week = np.arange('2018-01', '2018-12', dtype='datetime64[M]') #\n",
    "#time_week = [x for x in range(1,13,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_na(array):\n",
    "    return np.isfinite(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create figure and subplots\n",
    "fig, ax = plt.subplots(ncols=int(math.sqrt(len(coord_collection))) + 1,nrows=int(math.sqrt(len(coord_collection))), figsize=(len(coord_collection)*8,len(coord_collection)*3))\n",
    "#correct x axes time view\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "for loc, ax in zip(coord_collection, fig.axes):\n",
    "    #ndvi data\n",
    "    ndvi = ndvi_datasets[0].NDVI.sel(latitude = loc[0], longitude = loc[1], method='nearest').values\n",
    "    mask_ndvi = np.isfinite(ndvi)\n",
    "    time_ndvi = ndvi_datasets[0].sel(latitude = loc[0], longitude = loc[1], method='nearest').time.values\n",
    "    #soil_moisture data \n",
    "    sm = xr_model_aoi.soil_moisture_1km.sel(latitude = loc[0], longitude = loc[1], method='nearest').groupby('time.week').apply(lambda x: x.mean()).values\n",
    "    mask_sm = np.isfinite(sm)\n",
    "    time_sm = [x for x in xr_model_aoi.soil_moisture_1km.sel(latitude = loc[0], longitude = loc[1], method='nearest').groupby('time.week').groups.keys()]\n",
    "    time_sm = np.arange(np.datetime64(xr_model_aoi.time.min().values, 'M'), 53, dtype='datetime64[M]')\n",
    "    #plot data on axes\n",
    "    ax.plot(time_ndvi[mask_ndvi], ndvi[mask_ndvi], 'gx', label='ndvi')\n",
    "    ax.plot(time_sm, sm[mask_sm], 'ro--', label = 'soil_moisture monthly mean ')\n",
    "    ax.legend()\n",
    "    #fig.title('NDVI & monthl soil moisture time series')\n",
    "    ax.grid(axis='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection1d paired¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Geoviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_to_hv(da):\n",
    "    try: \n",
    "        xr_img = da.clone(datatype=['xarray'])\n",
    "        print(type(xr_img.data))    \n",
    "    except:\n",
    "        print('xarray interface could not be imported.')\n",
    "    return \n",
    "\n",
    "def img_to_hv(img, **kwag):\n",
    "    return(hv.Image(img, **kwag))\n",
    "\n",
    "def ds_to_gv(ds):\n",
    "    gv_dataset = gv.Dataset(ds)\n",
    "    return gv_dataset\n",
    "\n",
    "def ds_to_hv(ds, **kwag):\n",
    "    return hv.Dataset(ds, **kwag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridded Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare dimensions and variables\n",
    "kdims = ['longitude', 'latitude', 'time']\n",
    "vdims = ['soil_moisture_1km', 'precipitation_1km', 'real_evapotranspiration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load gv objects from xarray\n",
    "hvD_aoi_sm = ds_to_hv(xr_model_aoi.soil_moisture_1km.dropna(dim='time', how='all'))\n",
    "hvD_aoi_pp = ds_to_hv(xr_model_aoi.precipitation_1km)\n",
    "hvD_aoi_evp = ds_to_hv(xr_model_aoi.real_evapotranspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load gv objects from xarray\n",
    "hvD_ried_sm = ds_to_hv(xr_model_ried.soil_moisture_1km.dropna(dim='time', how='all'))\n",
    "hvD_ried_pp = ds_to_hv(xr_model_ried.precipitation_1km)\n",
    "hvD_ried_evp = ds_to_hv(xr_model_ried.real_evapotranspiration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvI_sub_clc_100 = img_to_hv(clc_100, **{'name' : 'clc_100', 'label' : 'Corine Land Cover 2018 100m'})\n",
    "hvI_sub_clc_1000 = img_to_hv(clc_1000, **{'name' : 'clc_1000', 'label' : 'Corine Land Cover 2018 1000m'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask out non Agriculture Surfaces\n",
    "vec_agri_ried = vec_clc_ried[vec_clc_ried['raster_grp'] == 2]\n",
    "vec_agri_aoi = vec_clc_aoi[vec_clc_aoi['raster_grp'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvP_ried = hv.Polygons(vec_ried)\n",
    "hvP_ried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvP_aoi = hv.Polygons(vec_aoi)\n",
    "hvP_clc_aoi = hv.Polygons(vec_clc_aoi)\n",
    "hvP_clc_ried = hv.Polygons(vec_clc_ried)\n",
    "hvP_clc_agri_ried = hv.Polygons(vec_agri_ried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now copy your GeoDataFrame and change the projection to a Cartesian system (EPSG:3857, unit= m as in the answer of ResMar)\n",
    "vec_clc_aoi[\"area\"] = vec_clc_aoi.to_crs({'init': 'epsg:3857'})['geometry'].area/ 10**6\n",
    "#But the surfaces in the Mercator projection are not correct, so with other projection in meters.\n",
    "vec_clc_aoi = vec_clc_aoi.to_crs({'init': 'epsg:32633'})\n",
    "vec_clc_aoi[\"area\"] = vec_clc_aoi['geometry'].area/ 10**6\n",
    "#go back to original crs\n",
    "vec_clc_aoi = vec_clc_aoi.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "#Now copy your GeoDataFrame and change the projection to a Cartesian system (EPSG:3857, unit= m as in the answer of ResMar)\n",
    "vec_clc_ried[\"area\"] = vec_clc_ried.to_crs({'init': 'epsg:3857'})['geometry'].area/ 10**6\n",
    "#But the surfaces in the Mercator projection are not correct, so with other projection in meters.\n",
    "vec_clc_ried = vec_clc_ried.to_crs({'init': 'epsg:32633'})\n",
    "vec_clc_ried[\"area\"] = vec_clc_ried['geometry'].area/ 10**6\n",
    "#go back to original crs\n",
    "vec_clc_ried = vec_clc_ried.to_crs({'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38337918/plot-pie-chart-and-table-of-pandas-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_simple = xr_model_data.sel(time=slice('2018-01-03', '2018-01-18')).soil_moisture_1km.plot(x='longitude', y='latitude', col='time', col_wrap=4)\n",
    "plt.savefig('figures//xr_before_merge.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agriculture Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clc_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clc_grp(value):\n",
    "    return int(str(value)[0])\n",
    "\n",
    "shape = clc_100.values.shape\n",
    "arr = np.ravel(clc_100.values.tolist())\n",
    "arr_grp = list(map(lambda x: clc_grp(x), arr))\n",
    "arr_ = np.array(arr_grp).reshape(shape)\n",
    "\n",
    "clc_100['landcover_grp'] = (['latitude', 'longitude'],arr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_area_aoi = vec_clc_aoi.groupby('landcover_class').sum().area.values\n",
    "landcover_area_ried = vec_clc_ried.groupby('landcover_class').sum().area.values\n",
    "landcover_class = vec_clc_aoi.groupby('landcover_class').sum().index.values\n",
    "explode = (0.1,0,0,0)\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}%\\n({:d} m³)\".format(pct, absolute)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "fig.suptitle('Landcover Classes from Corine Land Cover 2018 in Nördl. Oberrheingraben / Ried', fontsize=17)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "gs.update()\n",
    "ax1 = plt.subplot(gs[0, :2], )\n",
    "ax2 = plt.subplot(gs[0, 2:])\n",
    "ax3 = plt.subplot(gs[1, 1:3])\n",
    "ax1.pie(landcover_area_aoi[:-1], explode=explode, labels=landcover_class[:-1], autopct=lambda pct: func(pct, landcover_area_aoi),\n",
    "        shadow=True, startangle=90, colors=['grey','red', 'green', 'blue'], textprops={'fontsize':15})\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "#ax1.set_title('Landcover Classes from Corine Land Cover 2018 in AOI', fontsize=17)\n",
    "\n",
    "ax2.pie(landcover_area_ried[:-1], explode=explode, labels=landcover_class[:-1], autopct=lambda pct: func(pct, landcover_area_ried),\n",
    "        shadow=True, startangle=90, colors=['grey','red', 'green', 'blue'], textprops={'fontsize':15})\n",
    "ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "#ax2.set_title('Landcover Classes from Corine Land Cover 2018 in Ried', fontsize=17)\n",
    "\n",
    "clc_100.landcover_grp.plot.imshow(ax=ax3, levels=5, colors=['red','grey', 'green', 'blue', 'marple'], add_colorbar=True, cbar_kwargs={'shrink' : 0.8} )\n",
    "\n",
    "vec_ried.boundary.geometry.plot(ax=ax3, edgecolor='black', linewidth=3)\n",
    "\n",
    "plt.savefig('figures//landcover_pie_chart.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(24,8))\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(24,12))\n",
    "vec_agri_aoi.plot(ax=ax[0], color='lightgreen')\n",
    "vec_agri_ried.plot(ax=ax[0], color='red')\n",
    "vec_agri_ried.plot(ax=ax[1], color='lightgreen')\n",
    "ax[0].set_title('Agriculture Surfaces AOI')\n",
    "ax[1].set_title('Agriculture Surfaces Ried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create HoloMap Slider for time Dimension\n",
    "hmS_aoi_sm = hvD_aoi_sm.ndloc[:, :, :10].to(hv.Image, kdims=kdims[0:2], dynamic=False)#.opts(colorbar=True, fig_size=200)\n",
    "hmS_aoi_pp = hvD_aoi_pp.ndloc[:, :, :10].to(hv.Image, kdims=kdims[0:2], dynamic=False).opts(colorbar=True, fig_size=200)\n",
    "hmS_aoi_evp = hvD_aoi_evp.ndloc[:, :, :10].to(hv.Image, kdims=kdims[0:2], dynamic=False).opts(colorbar=True, fig_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create HoloMap Slider for time Dimension\n",
    "hmS_ried_sm = hvD_ried_sm.ndloc[:, :, :10].to(hv.Image, kdims=kdims[0:2], dynamic=False).opts(fig_size=200)\n",
    "hmS_ried_pp = hvD_ried_pp.ndloc[:, :, :10].to(hv.Image, kdims=kdims[0:2], dynamic=False).opts(colorbar=True, fig_size=200)\n",
    "hmS_ried_evp = hvD_ried_evp.ndloc[:, :, :10].to(hv.Image, kdims=kdims[0:2], dynamic=False).opts(colorbar=True, fig_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmS_ried_sm * hvP_clc_agri_ried.opts(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_html(hmS_aoi_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvP_ried.opts(alpha=0.5) * hvP_clc_aoi.opts(alpha=0.5) * hmS_aoi_sm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hvP_ried.opts(alpha=0.5) * hvP_clc_aoi.opts(alpha=0.5) * hmS_ried_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calendar information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm = {'noleap': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       '365_day': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       'standard': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       'gregorian': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       'proleptic_gregorian': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       'all_leap': [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       '366_day': [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "       '360_day': [0, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calendar functions to determine the number of days in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leap_year(year, calendar='standard'):\n",
    "    \"\"\"Determine if year is a leap year\"\"\"\n",
    "    leap = False\n",
    "    if ((calendar in ['standard', 'gregorian',\n",
    "        'proleptic_gregorian', 'julian']) and\n",
    "        (year % 4 == 0)):\n",
    "        leap = True\n",
    "        if ((calendar == 'proleptic_gregorian') and\n",
    "            (year % 100 == 0) and\n",
    "            (year % 400 != 0)):\n",
    "            leap = False\n",
    "        elif ((calendar in ['standard', 'gregorian']) and\n",
    "                 (year % 100 == 0) and (year % 400 != 0) and\n",
    "                 (year < 1583)):\n",
    "            leap = False\n",
    "    return leap\n",
    "\n",
    "def get_dpm(time, calendar='standard'):\n",
    "    \"\"\"\n",
    "    return a array of days per month corresponding to the months provided in `months`\n",
    "    \"\"\"\n",
    "    month_length = np.zeros(len(time), dtype=np.int)\n",
    "\n",
    "    cal_days = dpm[calendar]\n",
    "\n",
    "    for i, (month, year) in enumerate(zip(time.month, time.year)):\n",
    "        month_length[i] = cal_days[month]\n",
    "        if leap_year(year, calendar=calendar) and month == 2:\n",
    "            month_length[i] += 1\n",
    "    return month_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap it into a simple function\n",
    "def season_mean(ds, calendar='standard'):\n",
    "    # Make a DataArray of season/year groups\n",
    "    year_season = xr.DataArray(ds.time.to_index().to_period(freq='Q-NOV').to_timestamp(how='E'),\n",
    "                               coords=[ds.time], name='year_season')\n",
    "\n",
    "    # Make a DataArray with the number of days in each month, size = len(time)\n",
    "    month_length = xr.DataArray(get_dpm(ds.time.to_index(), calendar=calendar),\n",
    "                                coords=[ds.time], name='month_length')\n",
    "    # Calculate the weights by grouping by 'time.season'\n",
    "    weights = month_length.groupby('time.season') / month_length.groupby('time.season').sum()\n",
    "\n",
    "    # Test that the sum of the weights for each season is 1.0\n",
    "    np.testing.assert_allclose(weights.groupby('time.season').sum().values, np.ones(4))\n",
    "\n",
    "    # Calculate the weighted average\n",
    "    return (ds * weights).groupby('time.season').sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_mean_sm = season_mean(xr_model_data.soil_moisture_1km, calendar='standard')\n",
    "season_mean_pp = season_mean(xr_model_data.precipitation_1km, calendar='standard')\n",
    "season_mean_evp = season_mean(xr_model_data.real_evapotranspiration, calendar='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "for i, ax  in enumerate(fig.axes):\n",
    "    season_mean_sm.sel(season=seasons[i]).plot.pcolormesh(ax=ax, cbar_kwargs={\"label\": 'cm³/cm³'})\n",
    "    ax.tick_params('both', labelsize=15)\n",
    "    ax.set_title(label = seasons[i], fontsize=16)\n",
    "    ax.set_ylabel(fontsize=16, ylabel='latitude')\n",
    "    ax.set_xlabel(fontsize=16, xlabel='longitude')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//mean_seasonal_ca.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "for i, ax  in enumerate(fig.axes):\n",
    "    season_mean_pp.sel(season=seasons[i]).plot.pcolormesh(ax=ax, cbar_kwargs={\"label\": 'cm³/cm³'})\n",
    "    ax.tick_params('both', labelsize=15)\n",
    "    ax.set_title(label = seasons[i], fontsize=16)\n",
    "    ax.set_ylabel(fontsize=16, ylabel='latitude')\n",
    "    ax.set_xlabel(fontsize=16, xlabel='longitude')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "for i, ax  in enumerate(fig.axes):\n",
    "    season_mean_evp.where(season_mean_evp > 0).sel(season=seasons[i]).plot.pcolormesh(ax=ax, cbar_kwargs={\"label\": 'cm³/cm³'})\n",
    "    ax.tick_params('both', labelsize=15)\n",
    "    ax.set_title(label = seasons[i], fontsize=16)\n",
    "    ax.set_ylabel(fontsize=16, ylabel='latitude')\n",
    "    ax.set_xlabel(fontsize=16, xlabel='longitude')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_mean_sm_aoi = season_mean(xr_model_aoi.soil_moisture_1km, calendar='standard')\n",
    "season_mean_pp_aoi = season_mean(xr_model_aoi.precipitation_1km, calendar='standard')\n",
    "season_mean_evp_aoi = season_mean(xr_model_aoi.real_evapotranspiration, calendar='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "for i, ax  in enumerate(fig.axes):\n",
    "    season_mean_sm_aoi.sel(season=seasons[i]).plot.pcolormesh(ax=ax, cbar_kwargs={\"label\": 'cm³/cm³'})\n",
    "    ax.tick_params('both', labelsize=15)\n",
    "    ax.set_title(label = seasons[i], fontsize=16)\n",
    "    ax.set_ylabel(fontsize=16, ylabel='latitude')\n",
    "    ax.set_xlabel(fontsize=16, xlabel='longitude')\n",
    "    vec_ried.boundary.geometry.plot(ax=ax, edgecolor='black', linewidth=3.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures//mean_seasonal_aoi.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "for i, ax  in enumerate(fig.axes):\n",
    "    season_mean_pp_aoi.sel(season=seasons[i]).plot.pcolormesh(ax=ax, cbar_kwargs={\"label\": 'cm³/cm³'})\n",
    "    ax.tick_params('both', labelsize=15)\n",
    "    ax.set_title(label = seasons[i], fontsize=16)\n",
    "    ax.set_ylabel(fontsize=16, ylabel='latitude')\n",
    "    ax.set_xlabel(fontsize=16, xlabel='longitude')\n",
    "    vec_ried.plot(ax=ax, alpha=0.4, edgecolor='black', linewidth=3.5)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,nrows=2, figsize=(24,16))\n",
    "seasons = ['DJF', 'MAM', 'JJA', 'SON']\n",
    "for i, ax  in enumerate(fig.axes):\n",
    "    season_mean_evp_aoi.sel(season=seasons[i]).plot.pcolormesh(ax=ax, cbar_kwargs={\"label\": 'cm³/cm³'})\n",
    "    ax.tick_params('both', labelsize=15)\n",
    "    ax.set_title(label = seasons[i], fontsize=16)\n",
    "    ax.set_ylabel(fontsize=16, ylabel='latitude')\n",
    "    ax.set_xlabel(fontsize=16, xlabel='longitude')\n",
    "    vec_ried.plot(ax=ax, alpha=0.2, edgecolor='black', linewidth=3.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standart deviation \n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html#numpy.std  \n",
    "use ddoff=1 against normally underestimating the not knowing population distribution "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "groupby dt accesor: time. + “year”, “month”, “day”, “hour”, “minute”, “second”, “dayofyear”, “week”, “dayofweek”, “weekday” and “quarter”, \"season\"\n",
    "\n",
    "rcParams[\"axes.prop_cycle\"] = cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standart deviation \n",
    "fig,ax = plt.subplots(figsize=(25,18))\n",
    "sm_std = xr_model_aoi.soil_moisture_1km.groupby('latitude','longitude').std('time')\n",
    "sm_std.plot(ax=ax)\n",
    "plt.savefig('figures//std_aoi.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "low std could be an indicator for small changes over time so this could be nadelwald, cities, water; high standart deviation could be an indicator for changes in sm / check with corine land cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramm plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get soil moisture from dataset but why do i do this? why dont use the xarray from beginning? I don't remember \n",
    "dataframe_sm_aoi = xr_model_aoi.soil_moisture_1km.groupby('longitude', 'latitude').std('time').to_dataframe().reset_index()\n",
    "dataframe_sm_ried = xr_model_ried.soil_moisture_1km.groupby('longitude', 'latitude').std('time').to_dataframe().reset_index()\n",
    "\n",
    "#Get std from sm time series\n",
    "xr_sm_ried = xr_model_ried.soil_moisture_1km.groupby('longitude', 'latitude').std('time').reset_index(dims_or_levels=['longitude', 'latitude']).rename({'longitude_': 'longitude', 'latitude_': 'latitude'})\n",
    "xr_sm_aoi = xr_model_aoi.soil_moisture_1km.groupby('longitude', 'latitude').std('time').reset_index(dims_or_levels=['longitude', 'latitude']).rename({'longitude_': 'longitude', 'latitude_': 'latitude'})\n",
    "\n",
    "xr_ndvi_ried = s2_1c.NDVI.groupby('longitude', 'latitude').std('time').reset_index(dims_or_levels=['longitude', 'latitude']).rename({'longitude_': 'longitude', 'latitude_': 'latitude'})\n",
    "\n",
    "#align coordinates for clc map to match sm map coordinates\n",
    "#clc_2018_1km_interp = clc_2018_1km.interp(longitude=xr_sm_ried['longitude'], latitude=xr_sm_ried['latitude'])\n",
    "xr_sm_ried_interp = xr_sm_ried.interp(longitude=clc_1000['longitude'], latitude=clc_1000['latitude'], method='nearest')\n",
    "\n",
    "xr_ndvi_ried_interp = xr_ndvi_ried.interp(longitude=clc_1000['longitude'], latitude=clc_1000['latitude'], method='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy and flatten array\n",
    "df_sm_ried_interp = xr_sm_ried_interp.values.reshape(-1)\n",
    "df_clc_1000_interp = clc_1000.values.reshape(-1)\n",
    "df_ndvi_ried_interp = xr_ndvi_ried_interp.values.reshape(-1)\n",
    "#combine arrays in dataframe\n",
    "df_clc_sm = pd.DataFrame({'sm' : df_sm_ried_interp, 'clc' : df_clc_1000_interp, 'ndvi' : df_ndvi_ried_interp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clc_sm.dropna(how='any').plot.scatter(x='clc', y='sm', c='clc', cmap='viridis', title='coreine land cover class vs. sm std')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is nothing really to see... all kind of land cover classes are more or less equal distributed through the std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clc_sm.dropna(how='any').plot.scatter(x='sm', y='ndvi', c='ndvi', cmap='viridis', title='ndvi vs. soil moisture')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "as hihger the std from sm as higher is the std from ndvi but what means this??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clc_2018 values: ', len(clc_1000.values.reshape(-1)))\n",
    "print('clc_2018_1km values: ', len(clc_1000.values.reshape(-1)))\n",
    "print('xr_sm_ried_interp: ', len(xr_sm_ried_interp.values.reshape(-1)))\n",
    "print('dataframe_sm_ried: ', len(dataframe_sm_ried))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With np.isnan(X) you get a boolean mask back with True for positions containing NaNs.\n",
    "With np.where(np.isnan(X)) you get back a tuple with i, j coordinates of NaNs.\n",
    "Finally, with np.nan_to_num(X) you \"replace nan with zero and inf with finite numbers\".\n",
    "Alternatively, you can use:\n",
    "    sklearn.impute.SimpleImputer for mean / median imputation of missing values, or\n",
    "    pandas' pd.DataFrame(X).fillna(), if you need something other than filling it with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_model_aoi_clc2 = xr_model_aoi.where(clc_1000_grp['clc_grp'] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(cords, dataset):\n",
    "    x_list = []\n",
    "    x1_list = []\n",
    "    x2_list = []\n",
    "    y_list = []\n",
    "    y1_list = []\n",
    "    y2_list = []\n",
    "    y_nan_list = []\n",
    "    y1_nan_list = []\n",
    "    y2_nan_list = []\n",
    "    x_nan_list = []\n",
    "    \n",
    "    for loc in cords:\n",
    "        x = dataset.sel(longitude=loc[1], latitude=loc[0], method='nearest').time.values\n",
    "        x_nan_list.append(x)\n",
    "        #print(x)\n",
    "        #mask_x = np.isfinite(x)\n",
    "\n",
    "        y = dataset.sel(longitude=loc[1], latitude=loc[0], method='nearest').soil_moisture_1km.values\n",
    "        y_nan_list.append(y)\n",
    "        mask_y = np.isfinite(y)\n",
    "        \n",
    "        y1 = dataset.sel(longitude=loc[1], latitude=loc[0], method='nearest').precipitation_1km.values\n",
    "        y1_nan_list.append(y1)\n",
    "        mask_y1 = np.isfinite(y1)\n",
    "        \n",
    "        y2 = dataset.sel(longitude=loc[1], latitude=loc[0], method='nearest').real_evapotranspiration.values\n",
    "        y2_nan_list.append(y2)\n",
    "        mask_y2 = np.isfinite(y2)\n",
    "        \n",
    "        y_v = y[mask_y].astype('double')\n",
    "        y_list.append(y_v)\n",
    "        \n",
    "        x_v = x[mask_y]\n",
    "        x_list.append(x_v)\n",
    "        \n",
    "        x1_v = x[mask_y1]\n",
    "        y1_v = y1[mask_y1].astype('double')\n",
    "        x1_list.append(x1_v)\n",
    "        y1_list.append(y1_v)\n",
    "        \n",
    "        x2_v = x[mask_y2]\n",
    "        y2_v = y2[mask_y2].astype('double')\n",
    "        x2_list.append(x2_v)\n",
    "        y2_list.append(y2_v)\n",
    "        \n",
    "        \n",
    "    return x_list, x1_list, x2_list, y_list, y1_list, y2_list, y_nan_list, y1_nan_list, y2_nan_list, x_nan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data for coordinates\n",
    "x, x1, x2, y, y1, y2, y_nan, y1_nan, y2_nan,x_nan = get_x_y(r3, xr_model_aoi_clc2)\n",
    "\n",
    "#create Dataframe for statistics\n",
    "df_stat = pd.DataFrame({'sm_mean': [np.nan], 'sm_std': [np.nan], 'sm_var': [np.nan], 'pp_mean': [np.nan], 'pp_std': [np.nan], 'pp_var': [np.nan], 'ept_mean': [np.nan], 'ept_std': [np.nan], 'ept_var': [np.nan]})\n",
    "#clear mask & reset index\n",
    "df_stat = df_stat.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=6, nrows=len(r3), figsize=(40,len(r3)*4.5)) #nrows=len(coord_collection)\n",
    "\n",
    "#loop through coordinates collection and plot timeseries, hists for values and add statistics to pandas df\n",
    "for i, (time, time_pp, time_ept, sm, pp, ept) in enumerate(zip(x, x1, x2, y, y1, y2)):\n",
    "    df_stat = df_stat.append({'sm_mean': sm.mean(), 'sm_std': sm.std(ddof=1), 'sm_var': sm.var(ddof=1), 'pp_mean': pp.mean(), 'pp_std': pp.std(ddof=1), 'pp_var': pp.var(ddof=1), 'ept_mean': ept.mean(), 'ept_std': ept.std(ddof=1), 'ept_var': ept.var(ddof=1)}, ignore_index=True)\n",
    "    ax[i][0].plot(time, sm, label='soil moisture')\n",
    "    ax[i][0].legend()\n",
    "    ax[i][1].hist(sm)\n",
    "    ax_t = ax[i][1].twinx()\n",
    "    x_nd = np.linspace(sm.mean() - 3*sm.std(ddof=1), sm.mean() + 3*sm.std(ddof=1), 10)\n",
    "    ax_t.plot(x_nd, scipy.stats.norm.pdf(x_nd, sm.mean(), sm.std(ddof=1)))\n",
    "    ax[i][2].plot(time_pp, pp, label='precipitation')\n",
    "    ax[i][2].legend()\n",
    "    ax[i][3].hist(pp)\n",
    "    ax_t = ax[i][3].twinx()\n",
    "    x_nd = np.linspace(pp.mean() - 3*pp.std(ddof=1), pp.mean() + 3*pp.std(ddof=1), 10)\n",
    "    ax_t.plot(x_nd, scipy.stats.norm.pdf(x_nd, pp.mean(), pp.std(ddof=1)))\n",
    "    ax[i][4].plot(time_ept, ept, label='real Evapotranspiration ')\n",
    "    ax[i][4].legend()\n",
    "    ax[i][5].hist(ept)\n",
    "    ax_t = ax[i][5].twinx()\n",
    "    x_nd = np.linspace(ept.mean() - 3*ept.std(ddof=1), ept.mean() + 3*ept.std(ddof=1), 10)\n",
    "    ax_t.plot(x_nd, scipy.stats.norm.pdf(x_nd, ept.mean(), ept.std(ddof=1)))\n",
    "display(df_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance & Spearman & pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols = len(x_nan), figsize=(len(x_nan)*8,4))\n",
    "\n",
    "for i, (time, sm, pp, ept, ax) in enumerate(zip(x_nan, y_nan, y1_nan, y2_nan, fig.axes)):\n",
    "    df = pd.DataFrame({'time' : time, 'sm' : sm, 'pp' : pp, 'ept' : ept})\n",
    "    overall_pearson_r = df.corr(method='pearson')\n",
    "    print(f\"Pandas computed overall Pearson r: \\n{overall_pearson_r}\")\n",
    "    # out: Pandas computed Pearson r: 0.2058774513561943\n",
    "\n",
    "    r, p = scipy.stats.pearsonr(df.dropna().sm, df.dropna().pp)\n",
    "    print('')\n",
    "    print(f\"Scipy computed overall Pearson r: \\n{r} and p-value: \\n{p}\")\n",
    "    # out: Scipy computed Pearson r: 0.20587745135619354 and p-value: 3.7902989479463397e-51\n",
    "    # Compute rolling window synchrony\n",
    "    abc = df.set_index('time').rolling(window=12,center=True).median()\n",
    "    print(abc.iloc[500:550])\n",
    "    bcd = np.isfinite(abc.sm)\n",
    "    print(abc.sm[bcd])\n",
    "    bnm = abc.sm[bcd].plot(ax=ax)\n",
    "    ax.set(xlabel='Time',ylabel='monthly  mean ')\n",
    "    ax.set(title=f\"Overall Pearson r \\n{np.round(overall_pearson_r,2)}\")\n",
    "    \n",
    "    ## Set window size to compute moving window synchrony.\n",
    "    #r_window_size = 7\n",
    "    ## Interpolate missing data.\n",
    "    #df_interpolated = df.interpolate()\n",
    "    ## Compute rolling window synchrony\n",
    "    #rolling_r = abs(df_interpolated.sm.rolling(window=7, center=True).corr(df_interpolated.pp))\n",
    "    #rolling_r2 = abs(df_interpolated.sm.rolling(window=15, center=True).corr(df_interpolated.pp))\n",
    "    #rolling_r3 = abs(df_interpolated.sm.rolling(window=30, center=True).corr(df_interpolated.pp))\n",
    "    #rolling_r4 = abs(df_interpolated.sm.rolling(window=60, center=True).corr(df_interpolated.pp))\n",
    "    #\n",
    "    #f,ax=plt.subplots(2,1,figsize=(14,6),sharex=True)\n",
    "    #df.set_index('time').rolling(window=15,center=True).median().plot(ax=ax[0])\n",
    "    #ax[0].set(xlabel='Frame',ylabel='values ')\n",
    "    #print(rolling_r)\n",
    "    #rolling_r.plot(ax=ax[1], linewidth=0.3, grid=True)\n",
    "    #rolling_r2.plot(ax=ax[1], linewidth=0.4)\n",
    "    #rolling_r3.plot(ax=ax[1], linewidth=1.3, grid=True)\n",
    "    #rolling_r4.plot(ax=ax[1], linewidth=2, grid=True)\n",
    "    #ax[1].set(xlabel='Frame',ylabel='Pearson r sm & pp')\n",
    "    #plt.suptitle(\"sm & pp data and rolling window (30days) correlation\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "letzten 3 Grafiken sind interpolated also achtung da sm wahrscheinlich nicht so einfach interpolierbar ist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global synchrony that reduces the relationship between two signals to a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Correlation\n",
    "find leader and follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"1mWsGTGVdAsy6KoF3n3MyLA.gif\">')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assesses signal dynamics at a global level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(datax, datay, lag=0, wrap=False):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Shifted data filled with NaNs \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    if wrap:\n",
    "        shiftedy = datay.shift(lag)\n",
    "        shiftedy.iloc[:lag] = datay.iloc[-lag:].values\n",
    "        return datax.corr(shiftedy)\n",
    "    else: \n",
    "        return datax.corr(datay.shift(lag))\n",
    "\n",
    "d1 = df.pp\n",
    "d2 = df.sm\n",
    "rs = [crosscorr(d1,d2, lag) for lag in range(60)]\n",
    "offset = np.ceil(len(rs)/2)-np.argmax(rs)\n",
    "f,ax=plt.subplots(figsize=(14,3))\n",
    "ax.plot(rs)\n",
    "ax.axvline(np.ceil(len(rs)/2),color='k',linestyle='--',label='Center', linewidth=3)\n",
    "ax.axvline(np.argmax(rs),color='red',linestyle='solid',label='Peak synchrony', linewidth=3)\n",
    "#ax.set(title=f'Offset = {offset} frames\\nS1 leads <> S2 leads',ylim=[.1,.5],xlim=[0,1901], xlabel='Offset',ylabel='Pearson r')\n",
    "#ax.set_xticks([0, 50, 100, 151, 201, 251, 301,351,401,451,501,551,601,651,701,751,801,851,901,951,1001,1051,1101,1151,1201,1251,1301,1351,1401,1451,1501,1551,1601,1651,1701,1751,1801,1801])\n",
    "#ax.set_xticklabels([-900,-850,-800,-750,-700,-650,-600,-550,-500,-450,-400,-350,-300,-250,-200,-150, -100, -50, 0, 50, 100, 150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900]);\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pp leads sm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowed time lagged cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed time lagged cross correlation #sample data has 5400 rows 54\n",
    "days_w = 14\n",
    "no_splits = int(len(df)/days_w)\n",
    "samples_per_split = df.shape[0]/no_splits\n",
    "rss=[]\n",
    "for t in range(0, no_splits):\n",
    "    d1 = df.pp.loc[(t)*samples_per_split:(t+1)*samples_per_split]\n",
    "    d2 = df.sm.loc[(t)*samples_per_split:(t+1)*samples_per_split]\n",
    "    rs = [crosscorr(d1,d2, lag) for lag in range(-int(days_w),int(days_w+1))]\n",
    "    rss.append(rs)\n",
    "rss = pd.DataFrame(rss)\n",
    "f,ax = plt.subplots(figsize=(10,8))\n",
    "sns.heatmap(rss,cmap='RdBu_r',ax=ax)\n",
    "ax.set(title=f'Windowed Time Lagged Cross Correlation',xlim=[0,days_w*2], xlabel='Offset',ylabel='Window epochs')\n",
    "ax.set_xticks(range(0,days_w*2))\n",
    "ax.set_xticklabels(range(-days_w,days_w));\n",
    "#ax.set_xticks([0, 5, 10, 15, 20, 25, 30])\n",
    "#ax.set_xticklabels([-15, -10, -5, 0, 5, 10, 15]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window time lag cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window time lagged cross correlation\n",
    "days_w = 7\n",
    "window_size = 400 #samples\n",
    "t_start = 0\n",
    "t_end = t_start + window_size\n",
    "step_size = 14\n",
    "rss=[]\n",
    "while t_end < len(df):\n",
    "    d1 = df.pp.iloc[t_start:t_end]\n",
    "    d2 = df.sm.iloc[t_start:t_end]\n",
    "    rs = [crosscorr(d1,d2, lag, wrap=False) for lag in range(-int(days_w),int(days_w+1))]\n",
    "    rss.append(rs)\n",
    "    t_start = t_start + step_size\n",
    "    t_end = t_end + step_size\n",
    "rss = pd.DataFrame(rss)\n",
    "\n",
    "f,ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(rss,cmap='RdBu_r',ax=ax)\n",
    "ax.set(title=f'Rolling Windowed Time Lagged Cross Correlation',xlim=[0,15], xlabel='Offset',ylabel='Epochs')\n",
    "ax.set_xticks(range(0,15,1))\n",
    "ax.set_xticklabels(range(-7,8,1));\n",
    "plt.savefig('figures//Rolling_Windowed_Time_Lagged_Cross_Correlation.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "these signals have been computed with the assumption that events are happening simultaneously and also in similar lengths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = df.pp.interpolate().values\n",
    "d2 = df.sm.interpolate().values\n",
    "mask_d2 = d2 > 0\n",
    "d1 = d1[mask_d2]\n",
    "d2 = d2[mask_d2]\n",
    "\n",
    "d, cost_matrix, acc_cost_matrix, path = accelerated_dtw(d1,d2, dist='euclidean')\n",
    "\n",
    "plt.imshow(acc_cost_matrix.T, origin='lower', cmap='gray', interpolation='gaussian')\n",
    "plt.plot(path[0], path[1], 'w')\n",
    "plt.xlabel('precipitation')\n",
    "plt.ylabel('soil moisture')\n",
    "plt.title(f'DTW Minimum Path with minimum distance: {np.round(d,2)}')\n",
    "plt.savefig('figures//DTW_Minimum_Path_with_minimum_distance.png') #K means clustering\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "lowcut  = .01\n",
    "highcut = .5\n",
    "fs = 30.\n",
    "order = 1\n",
    "d1 = df.pp.interpolate().values\n",
    "d2 = df.sm.interpolate().values\n",
    "d3 = df.ept.interpolate().values\n",
    "mask_d2 = d2 > 0\n",
    "d1 = d1[mask_d2]\n",
    "d2 = d2[mask_d2]\n",
    "d3 = d3[mask_d2]\n",
    "\n",
    "y1 = butter_bandpass_filter(d1,lowcut=lowcut,highcut=highcut,fs=fs,order=order)\n",
    "y2 = butter_bandpass_filter(d2,lowcut=lowcut,highcut=highcut,fs=fs,order=order)\n",
    "y3 = butter_bandpass_filter(d3,lowcut=lowcut,highcut=highcut,fs=fs,order=order)\n",
    "\n",
    "al1 = np.angle(hilbert(y1),deg=False)\n",
    "al2 = np.angle(hilbert(y2),deg=False)\n",
    "al3 = np.angle(hilbert(y3),deg=False)\n",
    "\n",
    "phase_synchrony = 1-np.sin(np.abs(al1-al2)/2)\n",
    "N = len(al1)\n",
    "\n",
    "# Plot results\n",
    "f,ax = plt.subplots(3,1,figsize=(14,7),sharex=True)\n",
    "ax[0].plot(y1,color='r',label='sm')\n",
    "ax[0].plot(y2,color='b',label='pp')\n",
    "ax[0].plot(y3,color='g',label='evp')\n",
    "\n",
    "ax[0].legend(bbox_to_anchor=(0., 1.02, 1., .102),ncol=2)\n",
    "ax[0].set(xlim=[0,N], title='Filtered Timeseries Data')\n",
    "ax[1].plot(al1,color='r')\n",
    "ax[1].plot(al2,color='b')\n",
    "ax[1].plot(al3,color='g')\n",
    "ax[1].set(ylabel='Angle',title='Angle at each Timepoint',xlim=[0,N])\n",
    "phase_synchrony = 1-np.sin(np.abs(al1-al2)/2)\n",
    "ax[2].plot(phase_synchrony)\n",
    "ax[2].set(ylim=[0,1.1],xlim=[0,N],title='Instantaneous Phase Synchrony',xlabel='Time',ylabel='Phase Synchrony')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('figures//Instantaneous_Phase_Synchrony.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The instantaneous phase synchrony measure is a great way to compute moment-to-moment synchrony between two signals without arbitrarily deciding the window size as done in rolling window correlations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This allows us to assess if two signals are in phase (moving up and down together) or out of phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Pandas == no problem with nan, np & scipy == problem must drop nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://soilgrids.org/#!/?layer=ORCDRC_M_sl2_250m&vector=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## http://jinhyuncheong.com/jekyll/update/2017/12/10/Timeseries_synchrony_tutorial_and_simulations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.target)\n",
    "X = df_cluster.loc[:,:2]\n",
    "y_iris = X.target\n",
    "print(y_iris)\n",
    "km2 = cluster.KMeans(n_clusters=2).fit(X)\n",
    "km3 = cluster.KMeans(n_clusters=3).fit(X)\n",
    "km4 = cluster.KMeans(n_clusters=4).fit(X)\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km2.labels_)\n",
    "plt.title(\"K=2, J=%.2f\" % km2.inertia_)\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km3.labels_)\n",
    "plt.title(\"K=3, J=%.2f\" % km3.inertia_)\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km4.labels_)#.astype(np.float))\n",
    "plt.title(\"K=4, J=%.2f\" % km4.inertia_)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_coords = pd.DataFrame([xr_model_aoi.sel(longitude=x[1], latitude=x[0], method='nearest').soil_moisture_1km.to_series() for x in r3]).transpose()\n",
    "column_names_new = [str(x) for x in r3]\n",
    "column_names_old = sm_coords.columns.tolist()\n",
    "sm_coords.rename(columns={key:value for (key,value) in zip(column_names_old,column_names_new)}, inplace=True)\n",
    "sm_coords_i = sm_coords.resample('D').interpolate()[::120]\n",
    "fig, ax = plt.subplots(figsize=(26,8))\n",
    "plot = sm_coords_i.plot(style='o-', ax=ax) # draw the lines so they appears in the legend\n",
    "colors = [line.get_color() for line in plot.lines] # get the colors of the markers\n",
    "#df = sm_coords.interpolate(limit_area='inside') # interpolate\n",
    "#lines = plot.plot(df.index, df.values) # add more lines (with a new set of colors)\n",
    "for color, line in zip(colors, lines):\n",
    "  line.set_color(color) # overwrite the new lines colors with the same colors as the old lines\n",
    "\n",
    "ax.set(ylabel='soil moisture cm³/cm³')\n",
    "plt.savefig('figures//ts_r3_lineplot.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet = sm_coords.iloc[:,1].dropna()\n",
    "diet_resamp_yr = diet.resample('A').mean()\n",
    "diet_roll_yr = diet.rolling(15).mean()\n",
    "fig, ax = plt.subplots(figsize=(26,8))\n",
    "\n",
    "diet.plot(alpha=0.5, style='-',ax = ax) # store axis (ax) for latter plots\n",
    "diet_resamp_yr.plot(style=':', label='Resample at year frequency', ax=ax)\n",
    "diet_roll_yr.plot(style='--', label='Rolling average (smooth), window size=12', ax=ax)\n",
    "ax.set(ylabel='soil moisture cm³/cm³')\n",
    "ax.legend()\n",
    "plt.savefig('figures//rolling_average-example.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(diet)\n",
    "win = 12\n",
    "win_half = int(win / 2)\n",
    "# print([((idx-win_half), (idx+win_half)) for idx in np.arange(win_half, len(x))])\n",
    "diet_smooth = np.array([x[(idx-win_half):(idx+win_half)].mean() for idx in np.arange(win_half, len(x))])\n",
    "fig, ax = plt.subplots(figsize=(26,8))\n",
    "plt.plot(diet_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym = sm_coords.iloc[:].dropna()\n",
    "df_avg = pd.concat([diet.rolling(12).mean(), gym.rolling(12).mean()], axis=1)\n",
    "fig, ax = plt.subplots(figsize=(26,8))\n",
    "df_avg.plot(ax=ax)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.savefig('figures//ts_r3_rolling_mean.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtrend = sm_coords.iloc[:,0:2].dropna() - df_avg\n",
    "fig, ax = plt.subplots(figsize=(26,8))\n",
    "\n",
    "df_dtrend.plot(ax=ax)\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-order differencing: Seasonal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = original - shiftted data\n",
    "# (exclude first term for some implementation details)\n",
    "assert np.all((diet.diff() == diet - diet.shift())[1:])\n",
    "fig, ax = plt.subplots(figsize=(26,8))\n",
    "\n",
    "df.iloc[:,2:4].diff().plot(ax=ax)\n",
    "plt.xlabel('timesteps')\n",
    "plt.savefig('figures//First-order_differencing.png') #K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time, time_pp, time_ept, sm, pp, ept, sm_nan in zip(x, x1, x2, y, y1, y2, y_nan):\n",
    "    pp = [x for x in pp]\n",
    "    sm = [x for x in sm_nan]\n",
    "    ept = [x for x in ept]\n",
    "    \n",
    "    #weekly average\n",
    "    sm_week_mean = [np.nanmean(x) for x in np.array_split(sm, len(pp)/7)]\n",
    "    sm_mask = np.isfinite(sm_week_mean)\n",
    "    ept_week_mean = np.array([np.mean(x) for x in np.array_split(ept, len(ept)/7)])\n",
    "    mask_ept = np.isfinite(ept_week_mean)\n",
    "    pp_week_mean = np.array([np.mean(x) for x in np.array_split(pp, len(pp)/7)])\n",
    "    \n",
    "    ept_week_mean_mask = ept_week_mean[mask_ept]\n",
    "    pp_week_mean_mask = pp_week_mean[mask_ept]\n",
    "    sm_week_mean_mask = np.array(sm_week_mean)[sm_mask]\n",
    "    \n",
    "    \n",
    "    #assume normal distribution\n",
    "    stat, p = stats.pearsonr(ept_week_mean_mask, pp_week_mean_mask )\n",
    "    print('pearsonr')\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('Probably independent')\n",
    "    else:\n",
    "        print('Probably dependent')\n",
    "  \n",
    "    # Example of the Spearman's Rank Correlation Test\n",
    "    from scipy.stats import spearmanr\n",
    "    stat, p = spearmanr(ept_week_mean_mask, pp_week_mean_mask )\n",
    "    print('spearmanr')\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('Probably independent')\n",
    "    else:\n",
    "        print('Probably dependent')    #abc = np.array([pp_week_mean , sm_week_mean])\n",
    "        #cov = np.cov(m = abc,rowvar=False, ddof=1)\n",
    "        #corc = np.corrcoef(x = abc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 600 # number of smaples\n",
    "T = 1.0 / 800.0 # sample spacing\n",
    "x = np.linspace(0.0, N*T, N)\n",
    "window_sizes = [10,20,30,40,50]\n",
    "window_sizes = np.arange(10,51,10).astype(int)\n",
    "phase_y1_1,phase_y1_2, phase_y2 = 80., 50., 60\n",
    "amp_y1, amp_y2 = 1., 1.\n",
    "y1 = amp_y1*np.sin(phase_y1_1 * 2.0*np.pi*x) + amp_y1*np.sin(phase_y1_2 * 2.0*np.pi*x)\n",
    "y2 = amp_y2*np.sin(phase_y2 * 2.0*np.pi*x)\n",
    "al1 = np.angle(hilbert(y1),deg=False)\n",
    "al2 = np.angle(hilbert(y2),deg=False)\n",
    "f = plt.figure(figsize=(20,8))\n",
    "gs = mpl.gridspec.GridSpec(4,8)\n",
    "ax=[f.add_subplot(gs[0,:5]), f.add_subplot(gs[1,:5]),\n",
    "    f.add_subplot(gs[2,:5]),f.add_subplot(gs[3,:5]),\n",
    "    f.add_subplot(gs[:2,5:]), f.add_subplot(gs[2:,5:])]\n",
    "ax[0].plot(y1,color='r',label='y1')\n",
    "ax[0].plot(y2,color='b',label='y2')\n",
    "ax[0].legend(bbox_to_anchor=(0., 1.02, 1., .102),ncol=2)\n",
    "ax[0].set(xlim=[0,N], title='Timeseries Data')\n",
    "ax[1].plot(al1,color='r')\n",
    "ax[1].plot(al2,color='b')\n",
    "ax[1].set(ylabel='Angle', xlim=[0,N],title='Angle at each Timepoint')\n",
    "phase_synchrony = 1-np.sin(np.abs(al1-al2)/2)\n",
    "ax[2].plot(phase_synchrony)\n",
    "ax[2].set(ylim=[0,1],xlim=[0,N],title='Instantaneous Phase Synchrony',ylabel='Phase\\nSynchrony')\n",
    "window_corr_synchrony = pd.DataFrame(columns=window_sizes,index=np.arange(0,N))\n",
    "for window_size in window_sizes:\n",
    "    window_corr_synchrony[window_size]=rolling_correlation(data=pd.DataFrame({'y1':y1,'y2':y2}),wrap=True,window=window_size,center=True)\n",
    "window_corr_synchrony.plot(ax=ax[3])\n",
    "ax[3].legend(bbox_to_anchor=(0., 1.02, 1., .102),ncol=3)\n",
    "ax[3].set(ylim=[-1.1,1.1],xlim=[0,N],title='Windowed Correlation Synchrony',xlabel='Time',ylabel='Correlation\\nSynchrony')\n",
    "\n",
    "ticksteps = 30\n",
    "yf1,yf2 = fft(y1),fft(y2) # perform FFT\n",
    "amp1,amp2 = 2.0/N * np.abs(yf1),2.0/N * np.abs(yf2)\n",
    "ax[4].plot(amp1[:N//2],color='r',label='y1')\n",
    "ax[4].plot(amp2[:N//2],color='b',label='y2')\n",
    "ax[4].set(xticks=(np.arange(0,N//2,ticksteps)), ylabel='Amplitude',xlabel='Frequency (Hz)',title='FFT result',xlim=[0,N//2])\n",
    "ax[4].set_xticklabels([int(_tick) for _tick in np.round(fftfreq(N,T)[:N//2],2)[::ticksteps]],rotation=0)\n",
    "ax[4].legend(bbox_to_anchor=(0., 1.02, 1., .102),ncol=3)\n",
    "\n",
    "rs_per_window = pd.DataFrame(columns=['rs'],index=np.arange(min(window_sizes),max(window_sizes)+1,1))\n",
    "rs_per_window['rs']=np.nan\n",
    "for window_size in window_sizes:\n",
    "    rs_per_window.loc[window_size,'rs'] = (np.round(stats.pearsonr(phase_synchrony,window_corr_synchrony[window_size].values.ravel())[0],2))\n",
    "rs_per_window.interpolate(method='index').plot(ax=ax[5],legend=False)\n",
    "ax[5].set(xticks=window_sizes,xticklabels=[int(w) for w in window_sizes],ylim=[-1.1,1.1],xlabel='Window Size',title='Association between Phase and Window correlation Synchrony')\n",
    "ax[5].set_ylabel('Correlation between\\nPhase and Window\\nSynchrony',rotation=0,labelpad=70)\n",
    "rs_bool = rs_per_window==rs_per_window.max()\n",
    "rs_bool.loc[rs_per_window.idxmax().values[0]-1:rs_per_window.idxmax().values[0]+1]=True\n",
    "ax[5].fill_between(np.arange(min(window_sizes),max(window_sizes)+1,1),-1.1,1.1,where=rs_bool.values.ravel(),facecolor='red',alpha=.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance https://scikit-learn.org/stable/modules/covariance.html#covariance  \n",
    "https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk Normality Test\n",
    "from scipy.stats import shapiro\n",
    "data = [x,y]\n",
    "for n, variable in enumerate(data):\n",
    "    stat, p = shapiro(variable)\n",
    "    print('variable: ', variable[0:2],'...',variable[-2:])\n",
    "    print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > 0.05:\n",
    "        print('Probably Gaussian')\n",
    "    else:\n",
    "        print('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n",
    "#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "# License: BSD\n",
    "\n",
    "def isotonic_reg(cords):\n",
    "    x,y = get_x_y(cords)\n",
    "    \n",
    "    #x = x[mask_y]\n",
    "    x = np.arange(0, len(x))\n",
    "\n",
    "    #y = y[mask_y].astype('double')\n",
    "    n = len(x)\n",
    "    \n",
    "    # #############################################################################\n",
    "    # Fit IsotonicRegression and LinearRegression models\n",
    "\n",
    "    ir = IsotonicRegression()\n",
    "\n",
    "    y_ = ir.fit_transform(x, y)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n",
    "\n",
    "    # #############################################################################\n",
    "    # Plot result\n",
    "\n",
    "    segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\n",
    "    lc = LineCollection(segments, zorder=0)\n",
    "    lc.set_array(np.ones(len(y)))\n",
    "    lc.set_linewidths(np.full(n, 0.5))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, y, 'r.', markersize=12)\n",
    "    plt.plot(x, y_, 'b.-', markersize=12)\n",
    "    plt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\n",
    "    plt.gca().add_collection(lc)\n",
    "    plt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\n",
    "    plt.title('Isotonic regression')\n",
    "    return y_#, plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = isotonic_reg(coord_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**does not assume any form for the target function such as linearity. For comparison a linear regression is also presented**  \n",
    "https://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html#sphx-glr-auto-examples-plot-isotonic-regression-py  \n",
    "\n",
    "In statistics, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations under the following constraints: the fitted free-form line has to be non-decreasing (or non-increasing) everywhere, and it has to lie as close to the observations as possible."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Die gesamte Zeitreihe zu betreachten scheint keinen Sinn zu machen für die isotonic regression, stattdessen müsste man sich die einzelnen Perioden mit dieser form mal näher anschauen und vergleichen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get locations where ndvi indicates bare_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = xr.ufuncs.isnan(test_ar_mean)\n",
    "xr.apply_ufunc(function=invert(*mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ar_mean = s2_1c_ndvi_100m.NDVI.mean( dim='time')\n",
    "test_ar_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precipitation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_pairs(ds):\n",
    "    #convert long & lat to pandas series then to python list\n",
    "    longitude = ds.longitude.to_series().tolist()\n",
    "    latitude = ds.latitude.to_series().tolist()\n",
    "    return longitude, latitude\n",
    "\n",
    "def get_dates(ds):\n",
    "    #convert dates to pandas series then to python list\n",
    "    time = ds.time.to_series().tolist()\n",
    "    return time \n",
    "\n",
    "def get_combinations(lon, lat, time, ds):\n",
    "    #make list of all possible fields\n",
    "    comb = list()\n",
    "    for i in time:\n",
    "        for x in lon:\n",
    "            for y in lat:\n",
    "                pp = float(ds.sel(longitude=[x], latitude=[y], time=[i]).radolan_precipitation_1km.values)\n",
    "                if pp > 10:\n",
    "                    comb.append([x,y,i,pp])\n",
    "                else:\n",
    "                    continue\n",
    "                #comb.append([x,y,i])\n",
    "    return comb\n",
    "\n",
    "def clean_combinations(comb, ds, threshold):\n",
    "    \n",
    "    #next day\n",
    "    t1 = datetime.timedelta(days=1)\n",
    "    \n",
    "    #check if day after pp event has pp < 1, True = append to list\n",
    "    comb_clean = [x for x in comb if ds.sel(longitude=x[0], latitude=x[1], time=x[2] + t1).radolan_precipitation_1km.values < threshold]\n",
    "    \n",
    "    return comb_clean\n",
    "\n",
    "def get_con_days(comb, ds, threshold):\n",
    "    \n",
    "    #cast list for dry_days\n",
    "    dry_days_count = list()\n",
    "    dry_days = list()\n",
    "    da_list = list()\n",
    "    \n",
    "    #make timedeltas to create timespan within expected maximum dry days\n",
    "    t1 = datetime.timedelta(days=1)\n",
    "    t2 = datetime.timedelta(days=40)\n",
    "    \n",
    "    #loop through possible start combinations look to next day... if >1 elimiate from list \n",
    "    for combi in comb:\n",
    "        \n",
    "        t_start = combi[2] + t1\n",
    "        t_end = combi[2] + t2\n",
    "        \n",
    "        #select np array first day after event + 30 days in future \n",
    "        pp = ds.sel(longitude=combi[0], latitude=combi[1], time=slice(t_start, t_end)).radolan_precipitation_1km.values\n",
    "        \n",
    "        #make boolean mask where pp > 1mm \n",
    "        pp = np.where(pp > threshold , False, True)\n",
    "        \n",
    "        #count occurence of the beginning True periode == dry periode\n",
    "        try:\n",
    "            count = int(np.where(pp == False)[0][0])\n",
    "        except:\n",
    "            print(pp)\n",
    "        \n",
    "        #update dry_days list\n",
    "        #dry_days_count.append(count)\n",
    "        \n",
    "        #select dry periode excluding first occurence of rain > 1mm and first occurence of rain > 10mm\n",
    "        t_end = t_start + datetime.timedelta(days=count - 1)\n",
    "        \n",
    "        #select soil_moisute_1km within this interval\n",
    "        ds_s = ds.sel(longitude=combi[0], latitude=combi[1], time=slice(t_start, t_end))\n",
    "        \n",
    "        #get mean evapo_r for dry periode\n",
    "        evapo_r = np.mean(ds_s.evapo_r.values.tolist())\n",
    "        \n",
    "        #create list for sm &pp  and time & period length\n",
    "        x = list(range(0,len(ds_s.time.values.tolist())))\n",
    "        y = ds_s.soil_moisture_1km.values.tolist()\n",
    "        z = ds_s.radolan_precipitation_1km.values.tolist()\n",
    "        dry_days.append([count,evapo_r,[x,y,z]])\n",
    "\n",
    "    #New list with updatet dry_days count\n",
    "    comb_ext = [x + [y] for x,y in zip(comb,dry_days)]\n",
    "    \n",
    "    return comb_ext\n",
    "\n",
    "def clean_con_days(comb_ext):\n",
    "    \n",
    "    #new list for every item with 2 or more sm values (neccesary to calculate slope)\n",
    "    clean_list = [x for x in comb_ext if np.count_nonzero(~np.isnan(x[4][2][1])) >= 2]\n",
    "    \n",
    "    return clean_list\n",
    " \n",
    "def polyfit_xr(values):\n",
    "    sm_indizes = np.where(np.invert(np.isnan(values[1])))[0].tolist()\n",
    "    time = values[0][sm_indizes]\n",
    "    sm = values[1][sm_indizes]\n",
    "    fit = np.polyfit(time, sm, 1)\n",
    "    fit_fn = np.poly1d(fit)\n",
    "    s, i = fit\n",
    "    \n",
    "    return [s,i]\n",
    "\n",
    "\n",
    "def slope_xr(values):\n",
    "    sm_indizes = np.where(np.invert(np.isnan(values[1])))[0].tolist()\n",
    "    time = values[0][sm_indizes]\n",
    "    sm = values[1][sm_indizes]\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(time, sm)\n",
    "    return [slope, intercept, r_value, p_value, std_err]\n",
    "\n",
    "\n",
    "def get_clc_soil(values):\n",
    "    \n",
    "    #Insert your lat/lon/band below to extract corresponding pixel value\n",
    "    raster_value_clc = clc_2018.sel(band=1, y=values[1], x=values[0], method='nearest', tolerance=1000).values.tolist()\n",
    "    raster_value_soil = soil_map_small_scale.sel(band=1, y=values[1], x=values[0], method='nearest', tolerance=1000).values.tolist()    \n",
    "    return raster_value_clc, raster_value_soil\n",
    "\n",
    "def get_ndvi(values):\n",
    "    #create valid timespan to search for ndvi\n",
    "    t_delta_f = datetime.timedelta(days=20)\n",
    "    t_delta_b = datetime.timedelta(days=15) # more time from this point in futute\n",
    "    time_start = pd.Timestamp(values[2]) - t_delta_b\n",
    "    time_end = pd.Timestamp(values[2]) + t_delta_f\n",
    "    \n",
    "    ds_s2_1c_100m = s2_1c_ndvi_100m.sel(time=slice(time_start, time_end))\n",
    "    ds_s2_1c = s2_1c.sel(time=slice(time_start, time_end))\n",
    "    ds_l7_sr = l7_sr_ndvi_100m.sel(time=slice(time_start, time_end))\n",
    "    ds_l8_sr = l8_sr_ndvi_100m.sel(time=slice(time_start, time_end))\n",
    "\n",
    "    ndvi = []\n",
    "    ndvi.extend([np.nanmean(ds_s2_1c_100m.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "    ndvi.extend([np.nanmean(ds_s2_1c.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "    ndvi.extend([np.nanmean(ds_l7_sr.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "    ndvi.extend([np.nanmean(ds_l8_sr.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "\n",
    "    index = None\n",
    "    index = np.argwhere(np.isfinite(np.array(ndvi).flatten()))\n",
    "    \n",
    "    if len(index) > 0:\n",
    "        ndvi = ndvi[0]\n",
    "    else:\n",
    "        ndvi = [np.nan]\n",
    "    return [ndvi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wäre es zulässig alle sm Einträge zur zeit von d0 auf 0.75 zu setzten weil ja der boden zu diesem zeitpunkt aus niederschlagssicht volle kanne erwischt wurde**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lon & lat values #x:  37 y:  33\n",
    "longitude, latitude = get_coord_pairs(subset)\n",
    "print('x: ', len(longitude), 'y: ', len(latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lon & lat values #x:  37 y:  33\n",
    "longitude, latitude = get_coord_pairs(subset)\n",
    "print('x: ', len(longitude), 'y: ', len(latitude))\n",
    "\n",
    "#combine both to get all possible combinations == acces to every single field #87471\n",
    "#comb = get_combinations(longitude, latitude, time, subset)\n",
    "#print('Overall cell count: ', len(comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dates  #x:  37 y:  33 time:  1812\n",
    "time = get_dates(subset)\n",
    "print('x: ', len(longitude), 'y: ', len(latitude), 'time: ', len(time))\n",
    "print(subset.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both to get all possible combinations == acces to every single field #87471\n",
    "comb = get_combinations(longitude, latitude, time, subset)\n",
    "print('Overall cell count: ', len(comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all entrys where rain on first day is > 1mm #43850\n",
    "comb_clean = clean_combinations(comb, subset, 1)\n",
    "print('possible dry periods d>0 cell count: ', len(comb_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get number of days with less than 1mm rain  #43850\n",
    "comb_ext = get_con_days(comb_clean, subset, 1)\n",
    "print('possible dry periods d>0 cell count: ', len(comb_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comb_ext_clean = clean_con_days(comb_ext) # 6206\n",
    "print('possible dry periods d>0 & sm values not nan >= 2 cell count: ', len(comb_ext_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_calc = [x + [polyfit_xr(np.array(x[4][2]))] for x in comb_ext_clean]\n",
    "print('calculated polyfit1d slop & intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comb_calc_1 = [x + [slope_xr(np.array(x[4][2]))] for x in comb_calc]\n",
    "print('calculated lineregression slop & intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_1 = [x + list(get_clc_soil(x[:2])) for x in comb_calc_1]\n",
    "print('read corresponding clc_2018 & soil map value for coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_2 = [x + [np.count_nonzero(~np.isnan(x[4][-1][-1]))] for x in update_1]\n",
    "print('add the amount of sm values not nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_3 = [x + list(get_ndvi(x)) for x in update_2] #642 not-null\n",
    "print('add ndvi to list, periods without nan/overall periods: ', np.sum(np.isfinite([np.sum(x[-1]) for x in update_3])) , '/' , len(update_3))\n",
    "print('means not nan / overall amount of means: ', np.sum(np.isfinite([np.sum(x[-1]) for x in update_3])) , '/' , len(update_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ried_model_v1 = update_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from final json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists\n",
    "lon = [x[0] for x in update_3]\n",
    "lat = [x[1] for x in update_3]\n",
    "event_date = [pd.Timestamp(x[2]) for x in update_3]\n",
    "event_pp = [x[3] for x in update_3]\n",
    "event_et_mean = [x[4][1] for x in update_3]\n",
    "periode_duration = [x[4][0] for x in update_3]\n",
    "sm_measurements = [np.count_nonzero(~np.isnan(x[4][2][1])) for x in update_3]\n",
    "slope_polyfit1d =  [x[5][0] for x in update_3]\n",
    "intercept_polyfit1d =  [x[5][1] for x in update_3]\n",
    "slope_lineregress = [x[6][0] for x in update_3]\n",
    "intercept_lineregress = [x[6][1] for x in update_3]\n",
    "ndvi = [np.nanmean(x[-1]) for x in update_3]\n",
    "clc_2018 = [x[7] for x in update_3]\n",
    "soil_map = [x[8] for x in update_3]\n",
    "\n",
    "#list of lists == in pandas = objects to be avoided if possible\n",
    "days_number_list = [x[4][2][0] for x in update_3]\n",
    "days_list = [pd.date_range(x[2], periods=(x[4][0] + 1))[1:] for x in update_3]\n",
    "sm_list = [x[4][2][1] for x in update_3]\n",
    "pp_list = [x[4][2][2] for x in update_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Pandas DataFrame\n",
    "ried_db = pd.DataFrame({'lon' : lon,\n",
    "                        'lat' : lat,\n",
    "                        'event_date' : event_date,\n",
    "                        'event_pp' : event_pp,\n",
    "                        'event_et_mean' : event_et_mean,\n",
    "                        'periode_duration' : periode_duration,\n",
    "                        'sm_measurements' : sm_measurements,\n",
    "                        'slope_polyfit1d' : slope_polyfit1d,\n",
    "                        'intercept_polyfit1d' : intercept_polyfit1d,\n",
    "                        'slope_lineregress' : slope_lineregress,\n",
    "                        'intercept_lineregress' : intercept_lineregress,\n",
    "                        'clc_2018' : clc_2018,\n",
    "                        'soil_map' : soil_map,\n",
    "                        'ndvi' : ndvi\n",
    "                       })\n",
    "print(ried_db.info())\n",
    "ried_db[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ried_db.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Devis: Erst grob anfangen, also ndvi (0.0 - 0.3), wenn ergebnisse unscharf dann verfeinern der Gruppen ndvi (0.1 -0.2, 0.2-0.3 ...)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priority (high -> low) : 1) equal NDVI ranges, 2) clc same category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grouped layer for data with equal ndvi intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_v1 = ried_db.groupby(pd.cut(ried_db['ndvi'],6))\n",
    "grp_v1.sm_measurements.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show measurement positions on gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_v1 = (ried_db['ndvi'] < 0.2) & (ried_db['ndvi'] > 0.036) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_measurements = ried_db[mask_v1]\n",
    "print('length of db with mask v1:', len(db_measurements), '/', len(ried_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_sort = db_measurements.sort_values(by=['sm_measurements'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ried_db[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_thumbnail(db, location, zoom, label, index, col_names):\n",
    "    \n",
    "    #gmaps image with marker on lat/lon \n",
    "    gmap = gmaps.figure(center=location, map_type='SATELLITE', zoom_level=zoom, layout={'width': '400px', 'height': '300px'})\n",
    "    marker_layer = gmaps.marker_layer([location], label=str(label)) #,info_box_content=infobox.to_list(), display_info_box=True\n",
    "    gmap.add_layer(marker_layer)\n",
    "    \n",
    "    #plot graph of sm measurements\n",
    "    fig, ax = plt.subplots() #, figsize=(30,4)\n",
    "    #x = days_list[index]\n",
    "    #y1 = np.array(sm_list[index])\n",
    "    #y1_mask = np.isfinite(sm_list[index])\n",
    "    #y2 = pp_list[index]\n",
    "    #p1 = ax[1].plot(x[y1_mask], y1[y1_mask], 'ro' , linestyle='dashed')\n",
    "    #p2 = ax[1].plot(x, y2, 'ro', color='blue')\n",
    "    ax.axis('off')\n",
    "    #ax.axis('tight')\n",
    "    table = ax.table(cellText=db[0:1].values, colLabels=db.columns,cellLoc ='center')#.set_fontsize(10).scale(1,1.6) #, fontsize=50\n",
    "    for cell in table._cells:\n",
    "        if cell[0] == 0:\n",
    "            table._cells[cell].get_text().set_rotation(90)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "\n",
    "    #ax[0].axis('tight')\n",
    "    #ax[0].axis('off')\n",
    "    #for row in db:\n",
    "        \n",
    "    #the_table = ax[0].table(cellText=str(db.lon), colLabels=['lon'], loc='center')\n",
    "    #plt.legend((p1[0], p2[0]), ('soil moisture', 'radolan precipitation'))\n",
    "    #plt.table()\n",
    "    #fig.autofmt_xdate()\n",
    "    \n",
    "    return gmap, plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [x for x in ried_db[0:1].values.tolist()][0]\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dcsummary = pd.DataFrame([ried_db[0:1].values.tolist()],index=ried_db.columns.tolist()[0:2])\n",
    "\n",
    "plt.table(cellText=dcsummary.values,colWidths = [0.25]*len(ried_db.columns),\n",
    "          rowLabels=dcsummary.index,\n",
    "          colLabels=dcsummary.columns,\n",
    "          cellLoc = 'center', rowLoc = 'center',\n",
    "          loc='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail(ried_db, (db_sort.iloc[0][1],db_sort.iloc[0][0]), 15, db_sort.iloc[0][13], db_sort.iloc[0].name, col_names)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A small amount of rain <1mm is enough to increase soil moisture up to +0.2**\n",
    "**Better to set minimal amount of rain to 0.15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((db_sort.iloc[1][1],db_sort.iloc[1][0]), 15, db_sort.iloc[1][-2], db_sort.iloc[1].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((db_sort.iloc[2][1],db_sort.iloc[2][0]), 18, db_sort.iloc[2][-2], db_sort.iloc[2].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((db_sort.iloc[3][1],db_sort.iloc[3][0]), 18, db_sort.iloc[3][-2], db_sort.iloc[3].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((points.iloc[3][1],points.iloc[3][0]), 18, points.iloc[3][-2], points.iloc[3].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((points.iloc[78][1],points.iloc[78][0]), 18, points.iloc[78][-2], points.iloc[78].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((points.iloc[88][1],points.iloc[88][0]), 18, points.iloc[88][-2], points.iloc[88].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((points.iloc[102][1],points.iloc[102][0]), 18, points.iloc[102][-2], points.iloc[102].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((points.iloc[11][1],points.iloc[11][0]), 18, points.iloc[11][-2], points.iloc[11].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap, fig = show_thumbnail((points.iloc[250][1],points.iloc[250][0]), 18, points.iloc[250][-2], points.iloc[250].name)\n",
    "gmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate (latitude, longitude) pairs\n",
    "locations = [(y,x) for y,x in zip(lat,lon)]\n",
    "\n",
    "heatmap_layer = gmaps.heatmap_layer(locations) #, weights=soil_map\n",
    "#marker_layer = gmaps.marker_layer(locations)\n",
    "fig = gmaps.figure(map_type='SATELLITE')\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where does the two patches come from?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram of sm_measurements within a single periode\n",
    "plt.axis([2, 13, 0, 1500])\n",
    "arr = plt.hist(sm_measurements)\n",
    "\n",
    "n=0\n",
    "for x,y in zip(arr[0], arr[1]):\n",
    "    print('sm value amount: ', int(y), 'measurements: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Worse Distribution ca. 80% der slopes aus nur aus Messwerten**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ich muss mir die einzelnen Perioden anschauen und innerhalb derer nach unterschieden im slope suchen so sind alle anderen parameter gleichzusetzten (Temperatur, Wind, Jahreszeit)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_0 = [x for x in update_2 if x[-1] == 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beispiel für Wald boundary_0[1]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beispiel für Acker vielleicht bewässer boundary_0[20]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funktion schreiben die für die einzelne messreihe das google maps bild bekommt mit graph drunter von sm \n",
    "https://stackoverflow.com/questions/7490491/capture-embedded-google-map-image-with-python-without-using-a-browser/50536888#50536888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**211 is agriculture land without irrigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture = [x for x in update_1 if x[-1][0] == 211]\n",
    "print('corresponding clc to 211: ', len(agriculture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture_1 = [x for x in update_1 if (x[-1][0] == 211) and (x[-4][1] < 5) and (x[-4][1] > 0)]\n",
    "print('corresponding clc to 211 & evapo_r between 0-5: ', len(agriculture_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture_1[0][4][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slopes = [x[-3][0] for x in agriculture]\n",
    "evapo_r = [x[-4][1] for x in agriculture]\n",
    "color = [x[-1][1] for x in agriculture]\n",
    "soil = [x[-1][1] for x in agriculture]\n",
    "\n",
    "slopes_1 = [x[-3][0] for x in agriculture_1]\n",
    "evapo_r_1 = [x[-4][1] for x in agriculture_1]\n",
    "color_1 = [x[-1][1] for x in agriculture_1]\n",
    "soil_1 = [x[-1][1] for x in agriculture_1]\n",
    "\n",
    "print(len(slopes) == len(evapo_r) == len(color))\n",
    "print(len(set(color)), set(color))\n",
    "print(len(set(color_1)), set(color_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==1 ])\n",
    "s2 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==3 ])\n",
    "s3 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==4 ])\n",
    "s4 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==5 ])\n",
    "s5 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==7 ])\n",
    "s6 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==9 ])\n",
    "s7 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==10 ])\n",
    "s8 = np.mean([x[-3][0] for x in agriculture if x[-1][1] ==13 ])\n",
    "\n",
    "slope_mean = [s1, s2, s3, s4, s5, s6, s7, s8]\n",
    "soil_class = [1,3,4,5,7,9,10,13]\n",
    "soil_types_legend = {1:'Niedermoore, Hochmoore', 2:'Vega, Auengleye, örtl. Anmoorgleye', 3:'Tschernoseme', 4:'Parabraunerden', 5:'Braunerden mit Bändern, Bänder-Parabraunerden, örtl. Podsol-Braunerden', 7:'Pararendzinen, Braunerden mit Bändern, örtl. Bänder-Parabraunerden', 8:'Braunerden mit Bändern, Bänder-Parabraunerden, örtl. Podsol-Braunerden', 9:'Pararendzinen', 13: 'Parabraunerden, örtl. Pseudogley-Parabraunerden',25: 'Braunerden, Ranker-Braunerden, Regosol-Braunerden' , 34 : 'Braunerden, Braunerde-Pseudogleye, örtl. Podsol-Braunerden'} #Legende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(soil_class, slope_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**slope test m=(y2-y1)/(x2-x1) example: 0.3/8=0,0375 realistic example when you have 8 day difference between 2 sm values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Werte für die Tage von d=1 auf d=0.1 reduzieren** Darstellung ändert sich ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(soil, slopes, 'ro', markersize=0.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis([-0.1, 0.1, 2, 5])\n",
    "plt.scatter(slopes_1, evapo_r_1, c=color_1, norm = plt.Normalize(vmin=0, vmax=7), cmap = \"nipy_spectral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slopes, evapo_r, c=color, norm = plt.Normalize(vmin=0, vmax=7), cmap = \"nipy_spectral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**horizontal lines base on the fact that when slopes available then the possible amount of similiar evapo_r rates is hight becuase it was a long dry event and the evapo_r value is a mean value for each period**  \n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographical describtion & subsetting   \n",
    "https://georepository.com/crs_6933/WGS-84-NSIDC-EASE-Grid-2-0-Global.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D Array of Coordinates\n",
    "coords_ds = dataset.latitude + dataset.longitude\n",
    "subset_ds = subset.latitude + subset.longitude\n",
    "coords_subset_min = subset_overlapping_sm.latitude + subset_overlapping_sm.longitude\n",
    "\n",
    "#EASE Grid\n",
    "EASE_crs = ccrs.epsg(6933)\n",
    "\n",
    "#Coordinates to Gejson Multipoint\n",
    "ds_lon = dataset.longitude.to_series().tolist()\n",
    "ds_lat = dataset.latitude.to_series().tolist()\n",
    "ds_multipoint = list(zip(ds_lon, ds_lat))\n",
    "ds_multipoint = MultiPoint(ds_multipoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create figure\n",
    "#fig = plt.figure( )\n",
    "#Create the geoaxes \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10), subplot_kw=dict(projection=ccrs.PlateCarree()), gridspec_kw=dict(wspace=0., hspace =0.))\n",
    "# Create a Stamen terrain background instance.\n",
    "stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "# Add the Stamen data at zoom level 8.\n",
    "tuple(map(lambda x: x.add_image(stamen_terrain, 8), (ax1, ax2, ax3)))\n",
    "# Add gridlines\n",
    "tuple(map(lambda x: x.gridlines(), (ax1, ax2, ax3)))\n",
    "#Set Extent\n",
    "#ax1.set_extent([4, 13, 47, 53])\n",
    "#ax2.set_extent([4, 13, 47, 53])\n",
    "#Add datalayer\n",
    "xr.plot.pcolormesh(darray = coords_ds,  ax=ax1, alpha=0.1)\n",
    "xr.plot.pcolormesh(darray = subset_ds,  ax=ax3, alpha=0.1)\n",
    "xr.plot.pcolormesh(darray = coords_subset_min,  ax=ax2, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables, Dimensions & Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = 'radolan_precipitation_1km'\n",
    "var_1 = subset[var1]\n",
    "var2 = 'soil_moisture_1km'\n",
    "var_2 = subset[var2]\n",
    "\n",
    "\n",
    "dim1 = 'longitude'\n",
    "dim2 = 'latitude'\n",
    "dim3 = 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all values in single charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "Xarray suffers when plotting large data arrays switch to other plotting module will be better...  \n",
    "remove all the xaxes labels and show only the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(ds):\n",
    "    fig, axes = plt.subplots(nrows=len(np.unique(ds['time.year'].data)) * len(ds), figsize=(20,24))\n",
    "    count = np.arange(0,len(np.unique(ds['time.year'].data)) * len(ds))\n",
    "    count_x = count[0]\n",
    "    for year in np.unique(ds['time.year'].data):\n",
    "        for var in ds:\n",
    "            if var == var1:\n",
    "                ds[var].sel(time=str(year)).mean(['latitude', 'longitude']).plot(ax=axes[count_x]) #2d line https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D\n",
    "            else:\n",
    "                axes[count_x].set_autoscaley_on(False)\n",
    "                axes[count_x].set_ylim(0.0,0.6)\n",
    "                ds[var].sel(time=str(year)).mean(['latitude', 'longitude']).plot(ax=axes[count_x] , marker='x', linestyle='') #2d line https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D\n",
    "            try:\n",
    "                count_x = count[count_x + 1]\n",
    "            except:\n",
    "                count_x = 0\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_values(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show random date for each variable\n",
    "def show_random_date(ds):\n",
    "    fig, axes = plt.subplots(ncols=len(ds), figsize=(12,4))\n",
    "    plots = list()\n",
    "    for x, var in enumerate(ds):\n",
    "        plots.append(ds[var].isel(time=np.random.choice(np.arange(5, ds.time.shape[0]))).plot(ax=axes[x]))\n",
    "    return plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_date(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(ds):\n",
    "    \n",
    "    #location mean over time\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(25,15))\n",
    "    for var,axs in zip(ds,fig.axes):\n",
    "        ds.mean(dim='time')[var].plot(ax = axs)\n",
    "        axs.title.set_text(var + 'location mean')\n",
    "    \n",
    "    #location standart deviation\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(25,15))\n",
    "    for var,axs in zip(ds,fig.axes):\n",
    "        ds.std(dim='time')[var].plot(ax = axs)\n",
    "        axs.title.set_text(var + 'standart deviation')    \n",
    "        #ds[var].groupby('longitude', 'latitude').map(lambda x: x.mean()).plot(ax=ax)\n",
    "        #ds[var].groupby('longitude', 'latitude').map(lambda x: x.mean()).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean(dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Was verursacht bei soil moisture dies peaks von bis zu +-0.15 unterschied im mean()  \n",
    "Warum verläuft die evapo_r mean kurve gegenteil zu soil moisture? h.d je niedriger evapo_r ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tables and Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram \n",
    "def show_hist(ds):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(13,10))\n",
    "    plots = list()\n",
    "    for x, var in enumerate(ds):\n",
    "        plots.append(ds[var].plot.hist(ax=axes[x,0], histtype ='bar', align='mid'))\n",
    "        plots.append(ds[var].plot.hist(ax=axes[x,1], histtype ='bar', align='mid', cumulative =True))\n",
    "    return plots\n",
    "\n",
    "def grouped_bins_rad(ds,var):\n",
    "    ds = ds[var].groupby_bins(group=ds[var], bins=[0. , 0.1, 1,5,10,20,50,100,1000])\n",
    "    return ds\n",
    "def grouped_bins_smap(ds,var):\n",
    "    ds = ds[var].groupby_bins(group=ds[var], bins=[0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "    return ds\n",
    "    \n",
    "def get_freq_table(ds,var):\n",
    "    freq_dict = dict()\n",
    "    for key, value in sorted(grouped_bins(ds,var)):\n",
    "        freq_dict['%s - %s' %(key.left,key.right)] = value.to_series().sum()\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist(subset)[3][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var1, 'interval value count:')\n",
    "get_freq_table(subset,var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var2, 'interval value count:')\n",
    "get_freq_table(subset,var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a scatterplot (2 variables against each other)\n",
    "subset.plot.scatter(x='radolan_precipitation_1km.month', y='soil_moisture_1km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_reg(x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/covariance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting (Rectangle) & ROI (Polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read shapefile into salem with geopandas and adds bounding box\n",
    "sl_aoi = salem.read_shapefile(paths[0] + 'aoi2020//aoi_2020.shp') #use cached=True for load pickle into temp\n",
    "sl_ried = salem.read_shapefile(paths[2] + 'Ried_225_222//hessisches_ried.shp')\n",
    "sl_clc_aoi = salem.read_shapefile(paths[0] + 'corine_land_cover//vec_clc_aoi.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset an xarray dataset with salem accesor \n",
    "xr_aoi_sm = ls_grid[0].salem.subset(shape=sl_aoi).dropna(dim='time', how='all')\n",
    "xr_aoi_pp = ls_grid[1].salem.subset(shape=sl_aoi).dropna(dim='time', how='all')\n",
    "xr_aoi_evp = ls_grid[2].salem.subset(shape=sl_aoi).dropna(dim='time', how='all')\n",
    "\n",
    "xr_aoi_clc = ls_grid[3].salem.subset(shape=sl_aoi)\n",
    "xr_aoi_clc_km = ls_grid[4].salem.subset(shape=sl_aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,24))\n",
    "xr.align(*[xr_aoi_evp, xr_aoi_pp, xr_aoi_sm], join='right', exclude=['time'])[2].soil_moisture_1km.isel(time=150).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROI xarray datasets with salem accesor\n",
    "xr_ried_sm = xr_aoi_sm.salem.subset(shape=sl_ried).salem.roi(shape=sl_ried).dropna(dim='time', how='all')\n",
    "xr_ried_pp = xr_aoi_pp.salem.subset(shape=sl_ried).salem.roi(shape=sl_ried).dropna(dim='time', how='all')\n",
    "xr_ried_evp = xr_aoi_evp.salem.subset(shape=sl_ried).salem.roi(shape=sl_ried).dropna(dim='time', how='all')\n",
    "\n",
    "xr_ried_clc = xr_aoi_clc.salem.subset(shape=sl_ried).salem.roi(shape=sl_ried)\n",
    "xr_ried_clc_km = xr_aoi_clc_km.salem.subset(shape=sl_ried).salem.roi(shape=sl_ried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~250m Trick: erst np.linspace mit xr sub sm dann mit complete mode l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolating nan values to minimize the gaps \n",
    "xr_model_complete_i = xr_model_complete.interpolate_na(dim='longitude', method='linear', limit=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolating nan values to minimize the gaps \n",
    "xr_model_complete_i_ried = xr_model_complete_ried.interpolate_na(dim='longitude', method='linear', limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolating nan values to minimize the gaps \n",
    "xr_model_data = xr_model_ried.interpolate_na(dim='longitude', method='nearest', limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5,nrows=2, figsize=(32,8))\n",
    "xr_model_aoi.isel(time=1).soil_moisture_1km.plot(ax=ax[0,0])\n",
    "xr_model_aoi.isel(time=1).soil_moisture_1km.plot(ax=ax[1,0])\n",
    "xr_model_aoi.isel(time=1).soil_moisture_1km.plot(ax=ax[0,1])\n",
    "xr_model_ried.isel(time=1).soil_moisture_1km.plot(ax=ax[1,1])\n",
    "xr_i_aoi_sm.isel(time=1).soil_moisture_1km.plot(ax=ax[0,2])\n",
    "xr_i_ried_sm.isel(time=1).soil_moisture_1km.plot(ax=ax[1,2])\n",
    "xr_model_complete.isel(time=94).soil_moisture_1km.plot(ax=ax[0,3])\n",
    "xr_model_complete_ried.isel(time=94).soil_moisture_1km.plot(ax=ax[1,3])\n",
    "xr_model_complete_i.isel(time=94).soil_moisture_1km.plot(ax=ax[0,4])\n",
    "xr_model_complete_i_ried.isel(time=94).soil_moisture_1km.plot(ax=ax[1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, figsize=(24,8))\n",
    "precipitation_1km.isel(time=1).precipitation_1km.plot(ax=ax[0])\n",
    "precipitation_1km.isel(time=1).precipitation_1km.plot(ax=ax[0])\n",
    "xr_sub_pp.isel(time=1).precipitation_1km.plot(ax=ax[1])\n",
    "xr_sub_pp.isel(time=1).precipitation_1km.plot(ax=ax[1])\n",
    "xr_i_sub_pp.isel(time=1).precipitation_1km.plot(ax=ax[2])\n",
    "xr_model_complete.isel(time=1).precipitation_1km.plot(ax=ax[3])\n",
    "xr_model_complete_i.isel(time=1).precipitation_1km.plot(ax=ax[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, figsize=(24,8))\n",
    "real_evapotranspiration.isel(time=1).real_evapotranspiration.plot(ax=ax[0])\n",
    "xr_sub_evp.isel(time=1).real_evapotranspiration.plot(ax=ax[1])\n",
    "xr_i_sub_evp.isel(time=1).real_evapotranspiration.plot(ax=ax[2])\n",
    "xr_model_complete.isel(time=1).real_evapotranspiration.plot(ax=ax[3])\n",
    "xr_model_complete_i.isel(time=1).real_evapotranspiration.plot(ax=ax[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_i_ried_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(16,8))\n",
    "xr_ried_sm.soil_moisture_1km.isel(time=1).plot(ax=ax1)\n",
    "ax1.set_title('Raw data')\n",
    "xr_i_ried_sm.soil_moisture_1km.isel(time=1).plot(ax=ax2)\n",
    "ax2.set_title('Interpolated data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precipitation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_pairs(ds):\n",
    "    #convert long & lat to pandas series then to python list\n",
    "    longitude = ds.longitude.to_series().tolist()\n",
    "    latitude = ds.latitude.to_series().tolist()\n",
    "    return longitude, latitude\n",
    "\n",
    "def get_dates(ds):\n",
    "    #convert dates to pandas series then to python list\n",
    "    time = ds.time.to_series().tolist()\n",
    "    return time \n",
    "\n",
    "def get_combinations(lon, lat, time, ds):\n",
    "    #make list of all possible fields\n",
    "    pp = [[x, y, i, float(ds.sel(longitude=[x], latitude=[y], time=[i]).precipitation_1km.values)] for i in time for x in lon for y in lat]\n",
    "    comb = [x for x in pp if x[3] > 10]\n",
    "    return comb\n",
    "\n",
    "#def get_combinations(lon, lat, time, ds):\n",
    "#    #make list of all possible fields\n",
    "#    comb = list()\n",
    "#    for i in time:\n",
    "#        for x in lon:\n",
    "#            for y in lat:\n",
    "#                pp = float(ds.sel(longitude=[x], latitude=[y], time=[i]).precipitation_1km.values)\n",
    "#                if pp > 10:\n",
    "#                    comb.append([x,y,i,pp])\n",
    "#                else:\n",
    "#                    continue\n",
    "#                #comb.append([x,y,i])\n",
    "#    return comb\n",
    "\n",
    "def clean_combinations(comb, ds, threshold):\n",
    "    \n",
    "    #next day\n",
    "    t1 = datetime.timedelta(days=1)\n",
    "    \n",
    "    #check if day after pp event has pp < 1, True = append to list\n",
    "    comb_clean = [x for x in comb if ds.sel(longitude=x[0], latitude=x[1], time=x[2] + t1).precipitation_1km.values < threshold]\n",
    "    \n",
    "    return comb_clean\n",
    "\n",
    "def get_con_days(comb, ds, threshold):\n",
    "    \n",
    "    #cast list for dry_days\n",
    "    dry_days_count = list()\n",
    "    dry_days = list()\n",
    "    da_list = list()\n",
    "    \n",
    "    #make timedeltas to create timespan within expected maximum dry days\n",
    "    t1 = datetime.timedelta(days=1)\n",
    "    t2 = datetime.timedelta(days=40)\n",
    "    \n",
    "    #loop through possible start combinations look to next day... if >1 elimiate from list \n",
    "    for combi in comb:\n",
    "        \n",
    "        if isinstance(combi[2], str):\n",
    "            combi[2] = datetime.datetime.strptime(combi[2][0:10], '%Y-%m-%d')\n",
    "        \n",
    "        t_start = combi[2] + t1\n",
    "        t_end = combi[2] + t2\n",
    "        \n",
    "        #select np array first day after event + 30 days in future \n",
    "        pp = ds.sel(longitude=combi[0], latitude=combi[1], time=slice(t_start, t_end)).precipitation_1km.values\n",
    "        \n",
    "        #make boolean mask where pp > 1mm \n",
    "        pp = np.where(pp > threshold , False, True)\n",
    "        \n",
    "        #count occurence of the beginning True periode == dry periode\n",
    "        try:\n",
    "            count = int(np.where(pp == False)[0][0])\n",
    "        except:\n",
    "            print(pp)\n",
    "        \n",
    "        #update dry_days list\n",
    "        #dry_days_count.append(count)\n",
    "        \n",
    "        #select dry periode excluding first occurence of rain > 1mm and first occurence of rain > 10mm\n",
    "        t_end = t_start + datetime.timedelta(days=count - 1)\n",
    "        \n",
    "        #select soil_moisute_1km within this interval\n",
    "        ds_s = ds.sel(longitude=combi[0], latitude=combi[1], time=slice(t_start, t_end))\n",
    "        \n",
    "        #get mean evapo_r for dry periode\n",
    "        evapo_r = np.mean(ds_s.real_evapotranspiration.values.tolist())\n",
    "        \n",
    "        #create list for sm &pp  and time & period length\n",
    "        x = list(range(0,len(ds_s.time.values.tolist())))\n",
    "        y = ds_s.soil_moisture_1km.values.tolist()\n",
    "        z = ds_s.precipitation_1km.values.tolist()\n",
    "        dry_days.append([count,evapo_r,[x,y,z]])\n",
    "\n",
    "    #New list with updatet dry_days count\n",
    "    comb_ext = [x + [y] for x,y in zip(comb,dry_days)]\n",
    "    \n",
    "    return comb_ext\n",
    "\n",
    "def clean_con_days(comb_ext):\n",
    "    \n",
    "    #new list for every item with 2 or more sm values (neccesary to calculate slope)\n",
    "    clean_list = [x for x in comb_ext if np.count_nonzero(~np.isnan(x[4][2][1])) >= 2]\n",
    "    \n",
    "    return clean_list\n",
    " \n",
    "def polyfit_xr(values):\n",
    "    sm_indizes = np.where(np.invert(np.isnan(values[1])))[0].tolist()\n",
    "    time = values[0][sm_indizes]\n",
    "    sm = values[1][sm_indizes]\n",
    "    fit = np.polyfit(time, sm, 1)\n",
    "    fit_fn = np.poly1d(fit)\n",
    "    s, i = fit\n",
    "    \n",
    "    return [s,i]\n",
    "\n",
    "\n",
    "def slope_xr(values):\n",
    "    sm_indizes = np.where(np.invert(np.isnan(values[1])))[0].tolist()\n",
    "    time = values[0][sm_indizes]\n",
    "    sm = values[1][sm_indizes]\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(time, sm)\n",
    "    return [slope, intercept, r_value, p_value, std_err]\n",
    "\n",
    "def get_clc_soil(values):\n",
    "    #Insert your lat/lon/band below to extract corresponding pixel value\n",
    "    raster_value_clc = clc_1000.sel(latitude=values[1], longitude=values[0], method='nearest').values.tolist()\n",
    "    raster_value_soil = [xr_soil_map_100.HAUPTGRUPP.sel(y=values[1], x=values[0], method='nearest', tolerance=1000).values.tolist(), \n",
    "                         xr_soil_map_100.GRUPPE.sel(y=values[1], x=values[0], method='nearest', tolerance=1000).values.tolist(),\n",
    "                         xr_soil_map_100.UNTERGRUPP.sel(y=values[1], x=values[0], method='nearest', tolerance=1000).values.tolist(),\n",
    "                         xr_soil_map_100.BODENEINHE.sel(y=values[1], x=values[0], method='nearest', tolerance=1000).values.tolist()]\n",
    "    return raster_value_clc, raster_value_soil\n",
    "\n",
    "def get_ndvi(values):\n",
    "    #create valid timespan to search for ndvi\n",
    "    t_delta_f = datetime.timedelta(days=20)\n",
    "    t_delta_b = datetime.timedelta(days=15) # more time from this point in futute\n",
    "    time_start = pd.Timestamp(values[2]) - t_delta_b\n",
    "    time_end = pd.Timestamp(values[2]) + t_delta_f\n",
    "    \n",
    "    ds_s2_1c_100m = s2_1c_ndvi_100m.sel(time=slice(time_start, time_end))\n",
    "    ds_s2_1c = s2_1c.sel(time=slice(time_start, time_end))\n",
    "    ds_l7_sr = l7_sr_ndvi_100m.sel(time=slice(time_start, time_end))\n",
    "    ds_l8_sr = l8_sr_ndvi_100m.sel(time=slice(time_start, time_end))\n",
    "\n",
    "    ndvi = []\n",
    "    ndvi.extend([np.nanmean(ds_s2_1c_100m.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "    ndvi.extend([np.nanmean(ds_s2_1c.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "    ndvi.extend([np.nanmean(ds_l7_sr.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "    ndvi.extend([np.nanmean(ds_l8_sr.sel(longitude=values[0], latitude=values[1], method='nearest').NDVI.values.tolist())])\n",
    "\n",
    "    index = None\n",
    "    index = np.argwhere(np.isfinite(np.array(ndvi).flatten()))\n",
    "    \n",
    "    if len(index) > 0:\n",
    "        ndvi = ndvi[0]\n",
    "    else:\n",
    "        ndvi = [np.nan]\n",
    "    return [ndvi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xr_model_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lon & lat values #x:  37 y:  33\n",
    "longitude, latitude = get_coord_pairs(model)\n",
    "print('x: ', len(longitude), 'y: ', len(latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dates  #x:  37 y:  33 time:  1812\n",
    "time = get_dates(model)\n",
    "print('x: ', len(longitude), 'y: ', len(latitude), 'time: ', len(time))\n",
    "print(model.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine both to get all possible combinations == acces to every single field V1: #87471 -ried 1km # - aoi 365692 1km #87994 ried\n",
    "comb = get_combinations(longitude, latitude, time, model)\n",
    "print('Overall cell count: ', len(comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all entrys where rain on first day is > 1mm V1: #43850 ried   #42364 ried\n",
    "comb_clean = clean_combinations(comb, model, 1)\n",
    "print('possible dry periods d>0 cell count: ', len(comb_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get number of days with less than 1mm rain  #43850 #185893 aoi #42364 ried\n",
    "comb_ext = get_con_days(comb_clean, model, 1)\n",
    "print('possible dry periods d>0 cell count: ', len(comb_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comb_ext_clean = clean_con_days(comb_ext) #ried 6206 #49610aoi #ried 8275\n",
    "print('possible dry periods d>0 & sm values not nan >= 2 cell count: ', len(comb_ext_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_calc = [x + [polyfit_xr(np.array(x[4][2]))] for x in comb_ext_clean]\n",
    "print('calculated polyfit1d slop & intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comb_calc_1 = [x + [slope_xr(np.array(x[4][2]))] for x in comb_calc]\n",
    "print('calculated lineregression slop & intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_1 = [x + [get_clc_soil(x[:2])] for x in comb_calc_1]\n",
    "print('read corresponding clc_2018 & soil map value for coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_2 = [x + [np.count_nonzero(np.isfinite(x[4][2][1]))] for x in update_1]\n",
    "print('add the amount of sm values not nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_3 = [x + list(get_ndvi(x)) for x in update_2] #642 not-null #4405 ried\n",
    "print('add ndvi to list, periods without nan/overall periods: ', np.sum(np.isfinite([np.sum(x[-1]) for x in update_3])) , '/' , len(update_3))\n",
    "print('means not nan / overall amount of means: ', np.sum(np.isfinite([np.sum(x[-1]) for x in update_3])) , '/' , len(update_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from final json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists\n",
    "lon = [x[0] for x in update_3]\n",
    "lat = [x[1] for x in update_3]\n",
    "event_date = [pd.Timestamp(x[2]) for x in update_3]\n",
    "event_pp = [x[3] for x in update_3]\n",
    "event_et_mean = [x[4][1] for x in update_3]\n",
    "periode_duration = [x[4][0] for x in update_3]\n",
    "sm_measurements = [np.count_nonzero(np.isfinite(x[4][2][1])) for x in update_3]\n",
    "slope_polyfit1d =  [x[5][0] for x in update_3]\n",
    "intercept_polyfit1d =  [x[5][1] for x in update_3]\n",
    "slope_lineregress = [x[6][0] for x in update_3]\n",
    "intercept_lineregress = [x[6][1] for x in update_3]\n",
    "ndvi = [np.nanmean(x[9]) for x in update_3]\n",
    "clc_2018 = [x[7][0] for x in update_3]\n",
    "soil_map = [x[7][1] for x in update_3]\n",
    "lonlat = [lon+lat for lon,lat in zip(lon,lat)]\n",
    "\n",
    "#list of lists == in pandas = objects to be avoided if possible\n",
    "days_number_list = [x[4][2][0] for x in update_3]\n",
    "days_list = [pd.date_range(x[2], periods=(x[4][0] + 1))[1:] for x in update_3]\n",
    "sm_list = [x[4][2][1] for x in update_3]\n",
    "pp_list = [x[4][2][2] for x in update_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as json\n",
    "#convert time to string while dump via json (datetime is not json seriazeable)\n",
    "def myconverter(o):\n",
    "    if isinstance(o, datetime.datetime):\n",
    "        return o.__str__()\n",
    "\n",
    "with open('C:\\\\Users\\\\USER\\\\Desktop\\\\master-thesis-master\\\\json\\\\update_3_v1_aoi_clc1000m.txt', 'w') as filehandle:\n",
    "    json.dump(update_3, filehandle, default = myconverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file into variable\n",
    "with open(paths[0] + 'json//update_3_v1_ried_clcl1000m.txt') as json_file:\n",
    "    update_3 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file into variable\n",
    "with open(paths[0] + 'json//update_3_v1_aoi_clc1000m.txt') as json_file:\n",
    "    update_3 = json.load(json_file)\n",
    "    \n",
    "#list of lists == in pandas = objects to be avoided if possible\n",
    "days_number_list = [x[4][2][0] for x in update_3]\n",
    "days_list = [pd.date_range(x[2], periods=(x[4][0] + 1))[1:] for x in update_3]\n",
    "sm_list = [x[4][2][1] for x in update_3]\n",
    "pp_list = [x[4][2][2] for x in update_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Pandas DataFrame\n",
    "ried_db = pd.DataFrame({'lon' : lon,\n",
    "                        'lat' : lat,\n",
    "                        'event_date' : event_date,\n",
    "                        'event_pp' : event_pp,\n",
    "                        'event_et_mean' : event_et_mean,\n",
    "                        'periode_duration' : periode_duration,\n",
    "                        'sm_measurements' : sm_measurements,\n",
    "                        'slope_polyfit1d' : slope_polyfit1d,\n",
    "                        'intercept_polyfit1d' : intercept_polyfit1d,\n",
    "                        'slope_lineregress' : slope_lineregress,\n",
    "                        'intercept_lineregress' : intercept_lineregress,\n",
    "                        'clc_2018' : clc_2018,\n",
    "                        'soil_map' : soil_map,\n",
    "                        'ndvi' : ndvi,\n",
    "                        'lonlat' : lonlat\n",
    "                       })\n",
    "print(ried_db.info())\n",
    "ried_db[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add derived columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add clc category\n",
    "ried_db['clc_category'] = ried_db['clc_2018'].apply(lambda x: int(str(x)[:1]))\n",
    "\n",
    "#add count to latlon\n",
    "ried_db['lonlat_count'] = ried_db.groupby('lonlat')['lonlat'].transform('count')\n",
    "\n",
    "print('Unique cell locations:', len(ried_db['lonlat'].unique()), 'periods:', len(ried_db))\n",
    "ried_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ried_db.to_csv('csv_new\\\\ried_1000_v1.csv', date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Devis: Erst grob anfangen, also ndvi (0.0 - 0.3), wenn ergebnisse unscharf dann verfeinern der Gruppen ndvi (0.1 -0.2, 0.2-0.3 ...)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priority (high -> low) : 1) equal NDVI ranges, 2) clc same category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create grouped layer for data with equal ndvi intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_v1 = ried_db_agri.groupby(pd.cut(ried_db_agri['ndvi'],6))\n",
    "grp_v1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_sm_dates = list(xr_sub_sm.time.values)\n",
    "date_start = '2018-01-01'\n",
    "date_end = '2019-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_grid = [soil_moisture_1km, precipitation_1km, real_evapotranspiration, clc_2018_100, clc_2018_1000]\n",
    "ls_grid_sub = [xr_aoi_sm, xr_aoi_pp, xr_aoi_evp, xr_aoi_clc,  xr_aoi_clc_km]\n",
    "ls_grid_ried = [xr_ried_sm, xr_ried_pp, xr_ried_evp, xr_ried_clc, xr_ried_clc_km]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=15\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (20,8),\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75,\n",
    "          'axes.titlepad': 25}\n",
    "plt.rcParams.update(params)\n",
    "matplotlib.rcdefaults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
