{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read shapefile\n",
    "vec_loc20 = gpd.read_file(paths[0] + 'soil_sample//final_quarters.shp')\n",
    "\n",
    "#get center location for quarter polygons\n",
    "vec_center_loc20 = vec_loc20.centroid\n",
    "\n",
    "#turn shapely geometry into list of points\n",
    "list_loc20_center = [(x.y , x.x) for x in vec_center_loc20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ried Cell Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read shapefile\n",
    "vec_ried = gpd.read_file(paths[0] + 'soil_sample//gpd_point.shp')\n",
    "\n",
    "#turn shapely geometry into list of points\n",
    "list_ried_center = [(x.y , x.x) for x in vec_ried.geometry]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = xr_model_data.isel({'time' : 1}).to_dataframe()\n",
    "coords_AOI = [(x[0],x[1]) for x in df_sm.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Soil Moisture SMAP_SP_L2 m³/m³ product\n",
    "xr_model_data = xr.open_dataset(paths[1] + 'soil_moisture_1km.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soil Moisture time series data -resample & interpolate daily data -  Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_values = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().dropna('time').to_series().droplevel(level=0).sort_index() for p in range(0,len(list_loc20_center))]\n",
    "#resample to daily data points and interpolate to fill nan values\n",
    "sm_sample_values_resample_1 = [x.resample('1D').interpolate(method='linear') for x in sm_sample_values]\n",
    "sm_sample_values_resample_3 = [x.resample('3D').interpolate(method='linear') for x in sm_sample_values]\n",
    "sm_sample_values_resample_6 = [x.resample('6D').interpolate(method='linear') for x in sm_sample_values]\n",
    "sm_sample_values_resample_1_6 = [sm_sample_values_resample_1, sm_sample_values_resample_3, sm_sample_values_resample_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soil Moisture time series data -resample & interpolate daily data -  Ried Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_values_ried = [xr_model_data.sel(latitude=list_ried_center[p][0], longitude=list_ried_center[p][1]).to_array().dropna('time').to_series().droplevel(level=0).sort_index() for p in range(0,len(list_ried_center))]\n",
    "#resample to daily data points and interpolate to fill nan values\n",
    "sm_sample_values_resample_1_ried = [x.resample('1D').interpolate(method='linear') for x in sm_sample_values_ried]\n",
    "sm_sample_values_resample_3_ried = [x.resample('3D').interpolate(method='linear') for x in sm_sample_values_ried]\n",
    "sm_sample_values_resample_6_ried = [x.resample('6D').interpolate(method='linear') for x in sm_sample_values_ried]\n",
    "sm_sample_values_resample_15_ried = [x.resample('15D').interpolate(method='linear') for x in sm_sample_values_ried]\n",
    "sm_sample_values_resample_30_ried = [x.resample('30D').interpolate(method='linear') for x in sm_sample_values_ried]\n",
    "\n",
    "sm_sample_values_resample_1_6_ried = [sm_sample_values_resample_1_ried, sm_sample_values_resample_3_ried, sm_sample_values_resample_6_ried, sm_sample_values_resample_15_ried, sm_sample_values_resample_30_ried]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soil Moisture time series data -resample & interpolate daily data -  AOI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_values_AOI = [xr_model_data.sel(latitude=coords_AOI[p][0], longitude=coords_AOI[p][1]).to_array().dropna('time').to_series().droplevel(level=0).sort_index() for p in range(0,len(coords_AOI))]\n",
    "#resample to daily data points and interpolate to fill nan values\n",
    "sm_sample_values_res_int_7D_AOI = [x.resample('7D').interpolate(method='linear') for x in sm_sample_values_AOI]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil Moisture time series - rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_rolling_3 = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().rolling(time=3, center=True).mean().dropna('time').to_series().droplevel(level=0) for p in range(0,len(list_loc20_center))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Seasonal soil moisture values - mean, min, max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_season_mean = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().groupby('time.season').mean('time').to_series().droplevel(level=0).reindex(index=['DJF', 'MAM', 'JJA', 'SON']) for p in range(0,len(list_loc20_center))]\n",
    "sm_sample_season_min = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().groupby('time.season').min('time').to_series().droplevel(level=0).reindex(index=['DJF', 'MAM', 'JJA', 'SON']) for p in range(0,len(list_loc20_center))]\n",
    "sm_sample_season_max = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().groupby('time.season').max('time').to_series().droplevel(level=0).reindex(index=['DJF', 'MAM', 'JJA', 'SON']) for p in range(0,len(list_loc20_center))]\n",
    "sm_sample_season = [sm_sample_season_mean,sm_sample_season_min,sm_sample_season_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Monthly soil moisture values - mean, min, max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_monthly_mean = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().groupby('time.month').mean('time').to_series().droplevel(level=0) for p in range(0,len(list_loc20_center))]\n",
    "sm_sample_monthly_min = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().groupby('time.month').min('time').to_series().droplevel(level=0) for p in range(0,len(list_loc20_center))]\n",
    "sm_sample_monthly_max = [xr_model_data.sel(latitude=list_loc20_center[p][0], longitude=list_loc20_center[p][1]).to_array().groupby('time.month').max('time').to_series().droplevel(level=0) for p in range(0,len(list_loc20_center))]\n",
    "sm_sample_monthly = [sm_sample_monthly_mean,sm_sample_monthly_min,sm_sample_monthly_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cluster_sample = list(itertools.chain.from_iterable([[sm_sample_values],sm_sample_season, sm_sample_monthly]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Preprocessing for all time series within the ried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soil_moisutre_rsm = [xr_model_data.sel(latitude=list_ried_center[p][0], longitude=list_ried_center[p][1]).to_array().groupby('time.season').mean('time').to_series().droplevel(level=0) for p in range(0,len(list_ried_center))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Matrix Porfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stumpy.readthedocs.io/en/latest/Tutorial_The_Matrix_Profile.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize dataset\n",
    "#figures & axes objects\n",
    "fig, axs = plt.subplots(nrows=9, sharey=True, figsize=(20,30), dpi=200, sharex=True) #, gridspec_kw={'hspace': 0}\n",
    "\n",
    "#plotting data on several axes, linwidth & color\n",
    "plot_1 = [axs[x].plot(sm_sample_values_resample_1_6[y][0], linewidth=2, color='navy' ) for x,y  in zip(range(0,3), range(0,3))]\n",
    "plot_2 = [axs[x].plot(sm_sample_values_resample_1_6[y][5], linewidth=2, color='red' )for x,y in zip(range(3,6), range(0,3))]\n",
    "plot_3 = [axs[x].plot(sm_sample_values_resample_1_6[y][10], linewidth=2, color='blue' )for x,y in zip(range(6,9), range(0,3))]\n",
    "\n",
    "#set axes properties (Grid & ticks & fontsize )\n",
    "[x.grid(True) for x in axs];\n",
    "[x.yaxis.set(ticks=[.1,.2,.3,.4,.5,.6,],ticklabels=[0,0.1,0.2,0.3,0.4,]) for x in axs];\n",
    "[x.tick_params(axis='both', labelsize=30) for x in axs];\n",
    "\n",
    "#create secondary y axis & set properties(ticks, label, size)\n",
    "secaxy = [axs[x].secondary_yaxis('right') for x in range(0,9)]\n",
    "[(x.set_ticks([]), x.set_ylabel('Interp.: ' + str(y) + 'D', size=25)) for x,y in zip(secaxy, [1,3,6,1,3,6,1,3,6])];\n",
    "\n",
    "\n",
    "fig.subplots_adjust(hspace=0) \n",
    "fig.suptitle('Soil Moisture (m³/m³) for 1/3/6 days interpolated Time Series', fontsize='30');\n",
    "fig.tight_layout()\n",
    "plt.savefig(paths[5] + 'sm_time_series_interpolated_1_6.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum distance samples vs squence length - Random Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix profile = nearest neighbor distance  \n",
    "matrix profile index = the index to each nearest neighbor within the time series  \n",
    "the output of stump is an array that contains all of the matrix profile values (i.e., z-normalized Euclidean distance to your nearest neighbor) and matrix profile indices in the first and second columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [3,6,9,12,15,20,25,30,60,120,180]\n",
    "y_1 = [stumpy.stump(sm_sample_values_resample_1_6[0][0], m) for m in x]\n",
    "y_3 = [stumpy.stump(sm_sample_values_resample_1_6[1][0], m) for m in x]\n",
    "y_6 = [stumpy.stump(sm_sample_values_resample_1_6[2][0], m) for m in x]\n",
    "len_vector = pd.DataFrame({'1D Interp' : [len(y_1[x]) for x in range(0,11)], '3D Interp' : [len(y_3[x]) for x in range(0,11)], '6D Interp' : [len(y_6[x]) for x in range(0,11)]}, index=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Matrix-Profile Array with increasing seuquence length (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "plt.plot(len_vector,  marker='o', linestyle='dashed',linewidth=2, markersize=12)\n",
    "plt.title('Length of Matrix-Profile Array with increasing sequence width (m)', fontsize='20')\n",
    "plt.setp(ax.get_xticklabels(),  fontsize=20)\n",
    "plt.setp(ax.get_yticklabels(),  fontsize=20)\n",
    "ax.grid(True)\n",
    "ax.legend(['1D Interp','3D Interp.','6D Interp.'], fontsize=20)\n",
    "ax.set(ylabel='len(stump) array ', xlabel='sequence length (m)')\n",
    "ax.xaxis.label.set_size(20)\n",
    "ax.yaxis.label.set_size(20)\n",
    "table = pd.plotting.table(ax, len_vector.transpose(), rowColours=cycle[0:3], cellColours=n, colColours=n[0], bbox=[0.0, -1.3, 1.0, 1.0], cellLoc='center',) #, , colWidths=[.1]*11fontsize=20\n",
    "table.set_fontsize(30)\n",
    "table.auto_set_column_width(col=list(range(len(len_vector.columns)))) # Provide integer list of columns to adjust\n",
    "plt.savefig(paths[5] + 'Matrix_profile_array_length.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-normal euclidean distances for different m and 1d,3d,6d interp. \n",
    "y1_v1_min = [x[:,0].min() for x in y_1]\n",
    "y3_v1_min = [x[:,0].min() for x in y_3]\n",
    "y6_v1_min = [x[:,0].min() for x in y_6]\n",
    "\n",
    "y1_v1_mean = [x[:,0].min() for x in y_1]\n",
    "distance_min = pd.DataFrame({'y1_v1_min' : y1_v1_min, 'y3_v1_min' : y3_v1_min, 'y6_v1_min' : y6_v1_min}, index=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot \n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "distance_min.plot(grid=True, fontsize=30, ax=ax, linewidth=3)\n",
    "ax.legend(['1D Interp.', '3D Interp.', '6D Interp.'], fontsize=20)\n",
    "ax.set_xlabel('sequnce length - m', fontsize=20)\n",
    "ax.set_ylabel('z-normalized euclidean distance', fontsize=20)\n",
    "plt.title('minimum distance with increasing sequence length m', fontsize='30');\n",
    "plt.savefig(paths[5] + 'minimum_distance_with_increasing_sequence_length.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean minimum distance samples vs squence length - All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_ = pd.DataFrame({'v1' : [], 'm' : 0, 'p' :0})\n",
    "y3_ = pd.DataFrame({'v1' : [], 'm' : 0, 'p' :0})\n",
    "y6_ = pd.DataFrame({'v1' : [], 'm' : 0, 'p' :0})\n",
    "\n",
    "indexes = [list(range(x,y)) for x,y in zip(range(0,220,11),range(11,221, 11))]\n",
    "p = 0\n",
    "for sample, index in zip(sm_sample_values_resample_1_6[0],indexes):\n",
    "    p = p + 1\n",
    "    for m,i in zip(x,index):\n",
    "        y1_.loc[i] = [np.min([stumpy.stump(sample, m)[:,0]]), m, p]\n",
    "        \n",
    "p = 0\n",
    "for sample, index in zip(sm_sample_values_resample_1_6[1],indexes):\n",
    "    p = p + 1\n",
    "    for m,i in zip(x,index):\n",
    "        y3_.loc[i] = [np.min([stumpy.stump(sample, m)[:,0]]), m, p]\n",
    "\n",
    "p = 0\n",
    "for sample, index in zip(sm_sample_values_resample_1_6[2],indexes):\n",
    "    p = p + 1\n",
    "    for m,i in zip(x,index):\n",
    "        y6_.loc[i] = [np.min([stumpy.stump(sample, m)[:,0]]), m, p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby p\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20,8))\n",
    "y1_.groupby(['m']).mean()['v1'].plot(ax=ax[0])\n",
    "y3_.groupby(['m']).mean()['v1'].plot(ax=ax[0])\n",
    "y6_.groupby(['m']).mean()['v1'].plot(ax=ax[0])\n",
    "y1_.groupby(['m']).mean()['v1'].plot(ax=ax[1])\n",
    "y3_.groupby(['m']).mean()['v1'].plot(ax=ax[1])\n",
    "y6_.groupby(['m']).mean()['v1'].plot(ax=ax[1])\n",
    "fig.suptitle('mean minimum distance of all samples with increasing sequence length m', fontsize='20')\n",
    "ax[0].legend(['1D Interp.', '3D Interp.', '6D Interp.'], fontsize=15)\n",
    "ax[1].legend(['1D Interp.', '3D Interp.', '6D Interp.'], fontsize=15)\n",
    "ax[0].set_xlabel('sequnce length - m', fontsize=15)\n",
    "ax[0].set_ylabel('z-normalized euclidean distance', fontsize=15)\n",
    "ax[1].set_xlabel('sequnce length - m', fontsize=15)\n",
    "ax[1].set_ylabel('z-normalized euclidean distance', fontsize=15)\n",
    "ax[1].set_xlim(10, 30)\n",
    "ax[1].set_ylim(0, 2);\n",
    "plt.savefig(paths[5] + 'Mean_minimum_distance_samples_vs_squence_m.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stump - Matrix Profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a Motif Using STUMP\n",
    "What we learn is that the global minima (vertical dashed lines) from the matrix profile correspond to the locations of the two subsequences that make up the motif pair! And the exact z-normalized Euclidean distance between these two subsequences is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20\n",
    "mp = np.array(stumpy.stump(sm_sample_values_resample_1_6[0][0], m))\n",
    "mp_sort = mp[mp[:,0].argsort()]\n",
    "mp_sort[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "fig, axs = plt.subplots(2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(20,8))\n",
    "plt.suptitle('Motif (Pattern) Discovery Global Minimum 1D/P1', fontsize='30')\n",
    "\n",
    "axs[0].plot(sm_sample_values_resample_1_6[0][0].values)\n",
    "axs[0].set_ylabel('Soil Moisture m³/m³', fontsize='20')\n",
    "axs[0].tick_params( labelsize=20)\n",
    "[axs[0].add_patch(Rectangle((x, 0),m,1, facecolor='lightgrey')) for x in mp_sort[:,1][0:10]]\n",
    "\n",
    "axs[1].set_xlabel('Time', fontsize ='20')\n",
    "axs[1].set_ylabel('Matrix Profile', fontsize='20')\n",
    "axs[1].plot(mp[:, 0])\n",
    "#axs[1].set(ylim=[0,.03])\n",
    "axs[1].tick_params( labelsize=20)\n",
    "plt.savefig(paths[5] + 'Global_minimum_pattern_disc_1dp1.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Anomalies using STUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences lengths\n",
    "m_1 = 9\n",
    "m_3 = 6\n",
    "m_6 = 3\n",
    "\n",
    "#Matrix Profiles for random sample\n",
    "rsample = 0\n",
    "mp_1 = stumpy.stump(sm_sample_values_resample_1_6[0][rsample], m_1)\n",
    "mp_3 = stumpy.stump(sm_sample_values_resample_1_6[1][rsample], m_3)\n",
    "mp_6 = stumpy.stump(sm_sample_values_resample_1_6[2][rsample], m_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix profile index also tells us which subsequence within the time series does not have nearest neighbor that resembles itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort after first(0) column (values)\n",
    "mp_1_sort = mp_1[mp_1[:,0].argsort()]\n",
    "mp_3_sort = mp_3[mp_3[:,0].argsort()]\n",
    "mp_6_sort = mp_6[mp_6[:,0].argsort()]\n",
    "\n",
    "#second column is index of values\n",
    "mp_1_local_max_5 = mp_1_sort[:,1][-5:]\n",
    "mp_1_local_min_5 = mp_1_sort[:,1][:5]\n",
    "\n",
    "mp_3_local_max_5 = mp_3_sort[:,1][-5:]\n",
    "mp_3_local_min_5 = mp_3_sort[:,1][:5]\n",
    "\n",
    "mp_6_local_max_5 = mp_6_sort[:,1][-5:]\n",
    "mp_6_local_min_5 = mp_6_sort[:,1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(20,8))\n",
    "plt.suptitle('Discord (Anomaly/Novelty) Discovery 1D Interp. with local Maxima P1', fontsize='30')\n",
    "\n",
    "axs[0].plot(sm_sample_values_resample_1_6[0][rsample].values)\n",
    "axs[0].set_ylabel('Soil Moisture m³/m³', fontsize='20')\n",
    "axs[0].tick_params( labelsize=20)\n",
    "[axs[0].add_patch(Rectangle((x, 0),m_1,1, facecolor='lightgrey')) for x in mp_1_local_max_5]\n",
    "\n",
    "axs[1].set_xlabel('Time', fontsize ='20')\n",
    "axs[1].set_ylabel('Matrix Profile', fontsize='20')\n",
    "axs[1].plot(mp_1[:, 0])\n",
    "axs[1].tick_params(labelsize=20)\n",
    "plt.savefig(paths[5] + 'local_max_Discord_disc_1dp1.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(20,8))\n",
    "plt.suptitle('Discord (Anomaly/Novelty) Discovery 3D Interp. with local Maxima 1P', fontsize='30')\n",
    "\n",
    "axs[0].plot(sm_sample_values_resample_1_6[1][0].values)\n",
    "axs[0].set_ylabel('Soil Moisture m³/m³', fontsize='20')\n",
    "[axs[0].add_patch(Rectangle((x, 0),m_3,40, fill=True, facecolor='lightgrey')) for x in mp_3_local_max_5]\n",
    "axs[0].tick_params( labelsize=20)\n",
    "\n",
    "axs[1].set_xlabel('Time', fontsize ='20')\n",
    "axs[1].set_ylabel('Matrix Profile', fontsize='20')\n",
    "axs[1].plot(mp_3[:, 0])\n",
    "axs[1].tick_params( labelsize=20)\n",
    "plt.savefig(paths[5] + 'local_max_Discord_disc_3dp1.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, sharex=True, gridspec_kw={'hspace': 0}, figsize=(20,8))\n",
    "plt.suptitle('Discord (Anomaly/Novelty) Discovery 6D Interp. with local Maxima', fontsize='30')\n",
    "\n",
    "axs[0].plot(sm_sample_values_resample_1_6[2][0].values)\n",
    "axs[0].set_ylabel('Soil Moisture m³/m³', fontsize='20')\n",
    "[axs[0].add_patch(Rectangle((x, 0),m_6,1, fill=True, facecolor='lightgrey')) for x in mp_6_local_max_5]\n",
    "axs[0].tick_params( labelsize=20)\n",
    "\n",
    "axs[1].set_xlabel('Time', fontsize ='20')\n",
    "axs[1].set_ylabel('Matrix Profile', fontsize='20')\n",
    "axs[1].plot(mp_6[:, 0])\n",
    "axs[1].tick_params( labelsize=20)\n",
    "plt.savefig(paths[5] + 'local_max_Discord_disc_6dp1.jpg', bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_ = pd.DataFrame(sm_sample_values_resample_1_6[0][0], columns=['value'])\n",
    "p1_['timestamp'] = pd.to_datetime(p1_.index)\n",
    "p1_.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is going to be utilized to control the axis labeling of the plots\n",
    "DAY_MULTIPLIER = 7  # Specify for the amount of days you want between each labeled x-axis tick\n",
    "\n",
    "x_axis_labels = p1_[(p1_.timestamp.dt.hour==0)]['timestamp'].dt.strftime('%b %d').values[::DAY_MULTIPLIER]\n",
    "x_axis_labels[1::2] = \" \"\n",
    "x_axis_labels, DAY_MULTIPLIER\n",
    "\n",
    "plt.suptitle('Taxi Passenger Raw Data', fontsize='30')\n",
    "plt.xlabel('Window Start Date', fontsize ='20')\n",
    "plt.ylabel('Half-Hourly Average\\nNumber of Taxi Passengers', fontsize='20')\n",
    "plt.plot(p1_['value'])\n",
    "\n",
    "plt.xticks(np.arange(0, p1_['value'].shape[0], (48*DAY_MULTIPLIER)/2), x_axis_labels)\n",
    "plt.xticks(rotation=75)\n",
    "plt.minorticks_on()\n",
    "plt.margins(x=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rain = 48\n",
    "mp_rain = stumpy.stump(p1_['value'], m=m)\n",
    "\n",
    "\n",
    "plt.suptitle('1-Day STUMP', fontsize='30')\n",
    "plt.xlabel('Window Start', fontsize ='20')\n",
    "plt.ylabel('Matrix Profile', fontsize='20')\n",
    "plt.plot(mp_rain[:, 0])\n",
    "\n",
    "plt.plot(575, 1.7, marker=\"v\", markersize=15, color='b')\n",
    "plt.text(620, 1.6, 'Columbus Day', color=\"black\", fontsize=20)\n",
    "plt.plot(1535, 3.7, marker=\"v\", markersize=15, color='b')\n",
    "plt.text(1580, 3.6, 'Daylight Savings', color=\"black\", fontsize=20)\n",
    "plt.plot(30, .2, marker=\"^\", markersize=15, color='b', fillstyle='none')\n",
    "plt.plot(363, .2, marker=\"^\", markersize=15, color='b', fillstyle='none')\n",
    "plt.xticks(np.arange(0, 1800, (m*DAY_MULTIPLIER)/2), x_axis_labels)\n",
    "plt.xticks(rotation=75)\n",
    "plt.minorticks_on()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot dry periods as markers or rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_dict ={\n",
    "  \"3-Day\": 3,\n",
    "  \"5-Days\": 5,\n",
    "  \"7-Days\": 7,\n",
    "  \"9-Days\": 9,\n",
    "  \"10-Days\": 10,\n",
    "  \"11-Days\": 11,\n",
    "    \"12-Days\": 12,\n",
    "    \"13-Days\": 13,\n",
    "    \"14-Days\": 14,\n",
    "    \"15-Days\": 15,\n",
    "    \"16-Days\": 16,\n",
    "    \"17-Days\": 17\n",
    "}\n",
    "\n",
    "days_df = pd.DataFrame.from_dict(days_dict, orient='index', columns=['m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(12, sharex=True, gridspec_kw={'hspace': 0}, figsize=(20,20))\n",
    "fig.text(0.5, -0.1, 'Subsequence Start Date', ha='center', fontsize='20')\n",
    "fig.text(0.08, 0.5, 'Matrix Profile', va='center', rotation='vertical', fontsize='20')\n",
    "plt.text(1400, 3.6, '? Rainfall?', color=\"black\", fontsize=30)\n",
    "\n",
    "for i, varying_m in enumerate(days_df['m'].values):\n",
    "    mp = stumpy.stump(p1_['value'], varying_m)\n",
    "    axs[i].plot(mp[:, 0])\n",
    "    axs[i].set_ylim(0,4)\n",
    "    axs[i].set_xlim(0,1800)\n",
    "    title = f\"m = {varying_m}\"\n",
    "    axs[i].set_title(title, fontsize=20, y=.5)\n",
    "#plt.xticks(np.arange(0, p1_.shape[0], (48*DAY_MULTIPLIER)/2), x_axis_labels)\n",
    "plt.xticks(rotation=75)\n",
    "plt.suptitle('STUMP with Varying Window Sizes', fontsize='30')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/TDAmeritrade/stumpy/blob/master/docs/Tutorial_Semantic_Segmentation.ipynb\n",
    "segment it into homogeneous regions.\" In other words, wouldn't it be nice if you could take your long time series data and be able to segment or chop it up into k regions (where k is small) and with the ultimate goal of presenting only k short representative patterns to a human (or machine) annotator in order to produce labels for the entire dataset. These segmented regions are also known as \"regimes\". Additionally, as an exploratory tool, one might uncover new actionable insights in the data that was previously undiscovered. Fast low-cost unipotent semantic segmentation (FLUSS) is an algorithm that produces something called an \"arc curve\" which annotates the raw time series with information about the likelihood of a regime change. Fast low-cost online semantic segmentation (FLOSS) is a variation of FLUSS that, according to the original paper, is domain agnostic, offers streaming capabilities with potential for actionable real-time intervention, and is suitable for real world data (i.e., does not assume that every region of the data belongs to a well-defined semantic segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sample = 1\n",
    "p2_ = pd.DataFrame(sm_sample_values_resample_6[r_sample], columns=['value']).reset_index(drop=True).reset_index()\n",
    "p2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p2_['index'], p2_['value'])\n",
    "rect = Rectangle((900,0),200,200,facecolor='black')\n",
    "plt.gca().add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 900 - 50\n",
    "stop = 900 + 50\n",
    "abp = p2_.iloc[start:stop, 1]\n",
    "plt.plot(range(abp.shape[0]), abp)\n",
    "plt.ylim(0, 0.4)\n",
    "plt.axvline(x=100, linestyle=\"dashed\")\n",
    "\n",
    "style=\"Simple, tail_width=0.5, head_width=6, head_length=8\"\n",
    "kw = dict(arrowstyle=style, color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapelet Discovery\n",
    "https://github.com/TDAmeritrade/stumpy/blob/master/docs/Tutorial_Shapelet_Discovery.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.style.use('arviz-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create InferenceData\n",
    "inf_data = az.convert_to_inference_data(xr_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELM\n",
    "Ensemble Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS FRESH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tsfresh.readthedocs.io/en/latest/text/faq.html\n",
    "https://github.com/blue-yonder/tsfresh/blob/master/notebooks/feature_extraction_with_datetime_index.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction import extract_features\n",
    "# TimeBasedFCParameters contains all functions that use the Datetime index of the timeseries container\n",
    "from tsfresh.feature_extraction.settings import TimeBasedFCParameters\n",
    "from tsfresh.feature_selection import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters, settings\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop values before 2016.01\n",
    "ts_fresh_samples_6 = [x.loc['2016-01-01' : '2020-01-01'].to_frame() for x in sm_sample_values_resample_6]\n",
    "#create abc string list for column_id\n",
    "entities = [x for x in string.ascii_lowercase]\n",
    "for x,e in zip(ts_fresh_samples_6,entities):\n",
    "    x['id'] = e\n",
    "    x.rename(columns={0:'soil_moisture'}, inplace=True)\n",
    "    x.reset_index(inplace=True)\n",
    "tf6 = pd.concat(ts_fresh_samples_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_time = TimeBasedFCParameters()\n",
    "setting_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract from stacked dataframe\n",
    "features_tf6 = extract_features(tf6, column_id='id', column_sort=\"time\", column_kind=None, column_value=\"soil_moisture\", default_fc_parameters=MinimalFCParameters(),\n",
    "                           impute_function=impute)\n",
    "features_tf6_max = extract_features(tf6, column_id='id', column_sort=\"time\", column_kind=None, column_value=\"soil_moisture\", impute_function=impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering without filtering\n",
    "from sklearn import cluster\n",
    "X_tf6_kmeans = [KMeans(n_clusters=x).fit(features_tf6).labels_ for x in range(2,8)]\n",
    "X_tf6_kmeans_max = [KMeans(n_clusters=x).fit(features_tf6_max).labels_ for x in range(2,8)]\n",
    "X_kmeans = [KMeans(n_clusters=x).fit([x.set_index('time')['soil_moisture'] for x in ts_fresh_samples_6]).labels_ for x in range(2,8)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([X_tf6_kmeans,X_tf6_kmeans_max,X_kmeans])\n",
    "a.rename({0 : 'X_tf6_kmeans', 1 : 'X_tf6_kmeans_max', 2 : 'X_kmeans',}, axis=0, inplace=True)\n",
    "a.rename({0 : 'n_cluster_2', 1 : 'n_cluster_3', 2 : 'n_cluster_4', 3 : 'n_cluster_5', 4 : 'n_cluster_6', 5 : 'n_cluster_7',}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplot(nrows=7)\n",
    "for ax in axs:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own time-based feature calculators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing your own time-based feature calculators is no different from usual. Only two new properties must be set using the @set_property decorator:\n",
    "\n",
    "1) @set_property(\"input\", \"pd.Series\") tells the function that the input of the function is a pd.Series rather than a numpy array. This allows the index to be used. 2) @set_property(\"index_type\", pd.DatetimeIndex) tells the function that the input is a DatetimeIndex, allowing it to perform calculations based on time datatypes.\n",
    "\n",
    "For example, if we want to write a function that calculates the time between the first and last measurement, it could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@set_property(\"input\", \"pd.Series\")\n",
    "@set_property(\"index_type\", pd.DatetimeIndex)\n",
    "def timespan(x, param):\n",
    "    ix = x.index\n",
    "\n",
    "    # Get differences between the last timestamp and the first timestamp in seconds, then convert to hours.\n",
    "    times_seconds = (ix[-1] - ix[0]).total_seconds()\n",
    "    return times_seconds / float(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a time series container with DAtetime indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\"], \n",
    "                   \"value\": [1, 2, 3, 1, 3, 1, 0, 8],\n",
    "                   \"kind\": [\"temperature\", \"temperature\", \"pressure\", \"pressure\",\n",
    "                            \"temperature\", \"temperature\", \"pressure\", \"pressure\"]},\n",
    "                   index=pd.DatetimeIndex(\n",
    "                       ['2019-03-01 10:04:00', '2019-03-01 10:50:00', '2019-03-02 00:00:00', '2019-03-02 09:04:59',\n",
    "                        '2019-03-02 23:54:12', '2019-03-03 08:13:04', '2019-03-04 08:00:00', '2019-03-04 08:01:00']\n",
    "                   ))\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biclustering\n",
    "https://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_coclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-coclustering-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Time Series Features (min, max, mean...)\n",
    "https://nbviewer.jupyter.org/github/alexminnaar/time-series-classification-and-clustering/blob/master/Time%20Series%20Classification%20and%20Clustering.ipynb  \n",
    "  \n",
    "https://tslearn.readthedocs.io/en/stable/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.reshape(50,275)\n",
    "pd.DataFrame(X_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Romain Tavenard\n",
    "# License: BSD 3 clause\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
    "# Keep first 3 classes and 50 first time series\n",
    "X_train = X_train[y_train < 4]\n",
    "X_train = X_train[:50]\n",
    "np.random.shuffle(X_train)\n",
    "# For this method to operate properly, prior scaling is required\n",
    "X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train)\n",
    "sz = X_train.shape[1]\n",
    "\n",
    "# kShape clustering\n",
    "ks = KShape(n_clusters=3, verbose=True, random_state=seed)\n",
    "y_pred = ks.fit_predict(X_train)\n",
    "\n",
    "plt.figure()\n",
    "for yi in range(3):\n",
    "    plt.subplot(3, 1, 1 + yi)\n",
    "    for xx in X_train[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(ks.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, sz)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.title(\"Cluster %d\" % (yi + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering with SK-Learn\n",
    "only equal length time series possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "clustering_sk = [kmeans.fit(x).labels_ for x in to_cluster_sample[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering with TS-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 cluster\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=2, metric=\"dtw\")\n",
    "km_dtw_labels = [km_dtw.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_soft = TimeSeriesKMeans(n_clusters=2, metric=\"softdtw\")\n",
    "km_soft_labels = [km_soft.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_euc = TimeSeriesKMeans(n_clusters=2, metric=\"euclidean\")\n",
    "km_euc_labels = [km_euc.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "\n",
    "#3 cluster\n",
    "km_3_dtw = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\")\n",
    "km_dtw_labels_3 = [km_3_dtw.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_3_soft = TimeSeriesKMeans(n_clusters=3, metric=\"softdtw\")\n",
    "km_soft_labels_3 = [km_3_soft.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_3_euc = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\")\n",
    "km_euc_labels_3 = [km_3_euc.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "\n",
    "#4 cluster\n",
    "km_4_dtw = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\")\n",
    "km_dtw_labels_4 = [km_4_dtw.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_4_soft = TimeSeriesKMeans(n_clusters=4, metric=\"softdtw\")\n",
    "km_soft_labels_4 = [km_4_soft.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_4_euc = TimeSeriesKMeans(n_clusters=4, metric=\"euclidean\")\n",
    "km_euc_labels_4 = [km_4_euc.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "\n",
    "#5 cluster\n",
    "km_5_dtw = TimeSeriesKMeans(n_clusters=5, metric=\"dtw\")\n",
    "km_dtw_labels_5 = [km_5_dtw.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_5_soft = TimeSeriesKMeans(n_clusters=5, metric=\"softdtw\")\n",
    "km_soft_labels_5 = [km_5_soft.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_5_euc = TimeSeriesKMeans(n_clusters=5, metric=\"euclidean\")\n",
    "km_euc_labels_5 = [km_5_euc.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "\n",
    "#6 cluster\n",
    "km_6_dtw = TimeSeriesKMeans(n_clusters=6, metric=\"dtw\")\n",
    "km_dtw_labels_6= [km_6_dtw.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_6_soft = TimeSeriesKMeans(n_clusters=6, metric=\"softdtw\")\n",
    "km_soft_labels_6 = [km_6_soft.fit(x).labels_ for x in to_cluster_sample[1:]]\n",
    "km_6_euc = TimeSeriesKMeans(n_clusters=6, metric=\"euclidean\")\n",
    "km_euc_labels_6 = [km_6_euc.fit(x).labels_ for x in to_cluster_sample[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ['dtw', 'softdtw', 'euclidean']\n",
    "n_cluster = [2,3,4,5,6]\n",
    "cols = list(itertools.chain.from_iterable([['p' + str(x) for x in range(1,21)],['metric', 'n_clusters']]))\n",
    "km_dtw_table = pd.DataFrame([km_dtw_labels, km_dtw_labels_3, km_dtw_labels_4, km_dtw_labels_5, km_dtw_labels_6, metric, n_cluster])\n",
    "km_softdtw_table = pd.DataFrame([km_soft_labels, km_soft_labels_3, km_soft_labels_4, km_soft_labels_5, km_soft_labels_6,metric, n_cluster])\n",
    "km_euclidean_table = pd.DataFrame([km_euc_labels, km_euc_labels_3, km_euc_labels_4, km_euc_labels_5, km_euc_labels_6,metric, n_cluster])\n",
    "pr\n",
    "ts_labels_km_features = pd.concat([km_dtw_table, km_softdtw_table, km_euclidean_table])\n",
    "ts_labels_km_features = ts_labels_km_features.pivot(index='n_clusters', columns='metric', values=['p' + str(x) for x in range(1,21)])\n",
    "ts_labels_km_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_ts = [km.fit(x).labels_ for x in to_cluster_sample[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare & Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sk = pd.DataFrame(clustering_sk, columns=['p' + str(x) for x in range(1,21)], index = ['season_mean', 'monthly_mean', 'season_min','monthly_min', 'season_max', 'monthly_max'])\n",
    "results_ts = pd.DataFrame(clustering_ts, columns=['p' + str(x) for x in range(1,21)], index = ['season_mean', 'monthly_mean', 'season_min','monthly_min', 'season_max', 'monthly_max'])\n",
    "\n",
    "pd.concat([results_sk,results_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sk == results_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sk\n",
    "fig, axs = plt.subplots(2, 3, dpi=300)\n",
    "(ax1, ax2, ax3), (ax4, ax5, ax6) = axs\n",
    "\n",
    "[ax1.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_season_mean,results_sk.loc['season_mean'])]\n",
    "[ax2.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_season_min,results_sk.loc['season_min'])]\n",
    "[ax3.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_season_max,results_sk.loc['season_max'])]\n",
    "\n",
    "[ax4.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_monthly_mean,results_sk.loc['monthly_mean'])]\n",
    "[ax5.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_monthly_min,results_sk.loc['monthly_min'])]\n",
    "[ax6.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_monthly_max,results_sk.loc['monthly_max'])]\n",
    "\n",
    "#set x-ticks\n",
    "ax4, ax5, ax6 = [x.set_xticks([2,4,6,8,10,12]) for x in axs[1]]\n",
    "\n",
    "columns = ['mean', 'min', 'max']\n",
    "rows = ['season', 'monthly']\n",
    "for ax,col in zip(axs[0],columns):\n",
    "    ax.set_title(col)\n",
    "    \n",
    "for ax,row in zip(axs[:,0],rows):\n",
    "    ax.set_ylabel(row)\n",
    "    \n",
    "fig.suptitle('SK Learn Clustering \\n soil moisture [cm3/cm3]')\n",
    "plt.tight_layout(True)\n",
    "fig.subplots_adjust(top=0.83)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts\n",
    "fig, axs = plt.subplots(2, 3, dpi=300)\n",
    "(ax1, ax2, ax3), (ax4, ax5, ax6) = axs\n",
    "\n",
    "[ax1.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_season_mean,results_ts.loc['season_mean'])]\n",
    "[ax2.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_season_min,results_ts.loc['season_min'])]\n",
    "[ax3.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_season_max,results_ts.loc['season_max'])]\n",
    "\n",
    "[ax4.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_monthly_mean,results_ts.loc['monthly_mean'])]\n",
    "[ax5.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_monthly_min,results_ts.loc['monthly_min'])]\n",
    "[ax6.plot(line, color='C'+str(c), linewidth=0.5) for line,c in zip(sm_sample_monthly_max,results_ts.loc['monthly_max'])]\n",
    "\n",
    "#set x-ticks\n",
    "ax4, ax5, ax6 = [x.set_xticks([2,4,6,8,10,12]) for x in axs[1]]\n",
    "\n",
    "columns = ['mean', 'min', 'max']\n",
    "rows = ['season', 'monthly']\n",
    "for ax,col in zip(axs[0],columns):\n",
    "    ax.set_title(col)\n",
    "    \n",
    "for ax,row in zip(axs[:,0],rows):\n",
    "    ax.set_ylabel(row)\n",
    "    \n",
    "fig.suptitle('TS Learn Clustering with DTW \\n soil moisture [cm3/cm3]')\n",
    "plt.tight_layout(True)\n",
    "fig.subplots_adjust(top=0.83)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Resampled Daily Time Series \n",
    "https://tslearn.readthedocs.io/en/latest/variablelength.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Alignment Kernel K-means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import GlobalAlignmentKernelKMeans\n",
    "gak_km = GlobalAlignmentKernelKMeans(n_clusters=2)\n",
    "labels_gak = gak_km.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gak_km_3 = GlobalAlignmentKernelKMeans(n_clusters=3)\n",
    "labels_gak_3 = gak_km_3.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gak_km_4 = GlobalAlignmentKernelKMeans(n_clusters=4)\n",
    "labels_gak_4 = gak_km_4.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gak_km_5 = GlobalAlignmentKernelKMeans(n_clusters=5)\n",
    "labels_gak_5 = gak_km_5.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gak_km_6 = GlobalAlignmentKernelKMeans(n_clusters=6)\n",
    "labels_gak_6 = gak_km_6.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gak_km_7 = GlobalAlignmentKernelKMeans(n_clusters=7)\n",
    "labels_gak_7 = gak_km_7.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7]\n",
    "samples = ['p' + str(x) for x in range(1,21)]\n",
    "labels_gak_table = pd.DataFrame([labels_gak, labels_gak_3, labels_gak_4, labels_gak_5, labels_gak_6, labels_gak_7])\n",
    "labels_gak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering for time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=2, metric=\"dtw\")\n",
    "labels_dtw = km_dtw.fit_predict(sm_sample_values_resample)\n",
    "km_bis = TimeSeriesKMeans(n_clusters=2, metric=\"softdtw\")\n",
    "labels_bis = km_bis.fit_predict(sm_sample_values_resample)\n",
    "km_euc = TimeSeriesKMeans(n_clusters=2, metric=\"euclidean\")\n",
    "labels_euc = km_euc.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_3 = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\")\n",
    "labels_dtw_3 = km_dtw_3.fit_predict(sm_sample_values_resample)\n",
    "km_bis_3 = TimeSeriesKMeans(n_clusters=3, metric=\"softdtw\")\n",
    "labels_bis_3 = km_bis_3.fit_predict(sm_sample_values_resample)\n",
    "km_euc_3 = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\")\n",
    "labels_euc_3 = km_euc_3.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_4 = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\")\n",
    "labels_dtw_4 = km_dtw_4.fit_predict(sm_sample_values_resample)\n",
    "km_bis_4 = TimeSeriesKMeans(n_clusters=4, metric=\"softdtw\")\n",
    "labels_bis_4 = km_bis_4.fit_predict(sm_sample_values_resample)\n",
    "km_euc_4 = TimeSeriesKMeans(n_clusters=4, metric=\"euclidean\")\n",
    "labels_euc_4 = km_euc_4.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_5 = TimeSeriesKMeans(n_clusters=5, metric=\"dtw\")\n",
    "labels_dtw_5 = km_dtw_5.fit_predict(sm_sample_values_resample)\n",
    "km_bis_5 = TimeSeriesKMeans(n_clusters=5, metric=\"softdtw\")\n",
    "labels_bis_5 = km_bis_5.fit_predict(sm_sample_values_resample)\n",
    "km_euc_5 = TimeSeriesKMeans(n_clusters=5, metric=\"euclidean\")\n",
    "labels_euc_5 = km_euc_5.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_6 = TimeSeriesKMeans(n_clusters=6, metric=\"dtw\")\n",
    "labels_dtw_6 = km_dtw_6.fit_predict(sm_sample_values_resample)\n",
    "km_bis_6 = TimeSeriesKMeans(n_clusters=6, metric=\"softdtw\")\n",
    "labels_bis_6 = km_bis_6.fit_predict(sm_sample_values_resample)\n",
    "km_euc_6 = TimeSeriesKMeans(n_clusters=6, metric=\"euclidean\")\n",
    "labels_euc_6 = km_euc_6.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_7 = TimeSeriesKMeans(n_clusters=7, metric=\"dtw\")\n",
    "labels_dtw_7 = km_dtw_7.fit_predict(sm_sample_values_resample)\n",
    "km_bis_7 = TimeSeriesKMeans(n_clusters=7, metric=\"softdtw\")\n",
    "labels_bis_7 = km_bis_7.fit_predict(sm_sample_values_resample)\n",
    "km_euc_7 = TimeSeriesKMeans(n_clusters=7, metric=\"euclidean\")\n",
    "labels_euc_7 = km_euc_7.fit_predict(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary in a pivot table and save as tex table\n",
    "labels_kmean_samples = pd.DataFrame([labels_dtw, labels_bis, labels_euc, labels_dtw_3, labels_bis_3, labels_euc_3, labels_dtw_4, labels_bis_4, labels_euc_4, labels_dtw_5, labels_bis_5, labels_euc_5, labels_dtw_6, labels_bis_6, labels_euc_6,  labels_dtw_7, labels_bis_7, labels_euc_7], columns=['p' + str(x) for x in range(1,21)])\n",
    "labels_kmean_samples['metric'] = [\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\"]\n",
    "labels_kmean_samples['n_cluster'] = [2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7]\n",
    "\n",
    "labels_kmean_samples_table = labels_kmean_samples.pivot(index='metric', columns='n_cluster', values=['p' + str(x) for x in range(1,21)])\n",
    "\n",
    "table1_kmeans_timeseries_interp_summary = labels_kmean_samples_table.to_latex(buf=paths[4] + 'table1_kmeans_timeseries_interp_summary.tex')\n",
    "labels_kmean_samples_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=4, figsize=(20,12), sharex=True, sharey=True)\n",
    "fig.suptitle('K-Mean samples (20) cluster for different n_clusters', fontsize=16)# Set the ticks and ticklabels for all axes\n",
    "fig.subplots_adjust(top=0.94)\n",
    "plt.setp(axs, xticks=[2,3,4,5,6,7], yticks=[0, 1, 2, 3, 4, 5, 6])\n",
    "for ax,n in zip(axs, [1,5,9,13,17]):\n",
    "    for ax,p in zip(ax,['p' + str(x) for x in range(n,21)]):\n",
    "        labels_kmean_samples_table[p].plot(ax=ax, legend=False, grid=True, ls='-', linewidth=0.5, marker='o')\n",
    "        ax.set(title=p, ylabel='cluster')\n",
    "        #ax.xtick(labelsize=6)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette (clustering) \n",
    "https://en.wikipedia.org/wiki/Silhouette_(clustering)  \n",
    "−1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean Silhouette Coefficient of all samples\n",
    "from tslearn.clustering import silhouette_score\n",
    "s_dtw = silhouette_score(sm_sample_values_resample, labels_dtw, metric=\"dtw\")\n",
    "s_softdtw = silhouette_score(sm_sample_values_resample, labels_bis, metric=\"softdtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_euclidean = silhouette_score(sm_sample_values_resample, labels_euc, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dtw_3 = silhouette_score(sm_sample_values_resample, labels_dtw_3, metric=\"dtw\")\n",
    "s_softdtw_3 = silhouette_score(sm_sample_values_resample, labels_bis_3, metric=\"softdtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_euclidean_3 = silhouette_score(sm_sample_values_resample, labels_euc_3, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dtw_4 = silhouette_score(sm_sample_values_resample, labels_dtw_4, metric=\"dtw\")\n",
    "s_softdtw_4 = silhouette_score(sm_sample_values_resample, labels_bis_4, metric=\"softdtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_euclidean_4 = silhouette_score(sm_sample_values_resample, labels_euc_4, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dtw_5 = silhouette_score(sm_sample_values_resample, labels_dtw_5, metric=\"dtw\")\n",
    "s_softdtw_5 = silhouette_score(sm_sample_values_resample, labels_bis_5, metric=\"softdtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_euclidean_5 = silhouette_score(sm_sample_values_resample, labels_euc_5, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dtw_6 = silhouette_score(sm_sample_values_resample, labels_dtw_6, metric=\"dtw\")\n",
    "s_softdtw_6 = silhouette_score(sm_sample_values_resample, labels_bis_6, metric=\"softdtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_euclidean_6 = silhouette_score(sm_sample_values_resample, labels_euc_6, metric=\"euclidea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dtw_7 = silhouette_score(sm_sample_values_resample, labels_dtw_7, metric=\"dtw\")\n",
    "s_softdtw_7 = silhouette_score(sm_sample_values_resample, labels_bis_7, metric=\"softdtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_euclidean_7 = silhouette_score(sm_sample_values_resample, labels_euc_7, metric=\"euclidea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score = pd.DataFrame({'silhouette_score' : [s_dtw, s_softdtw,  s_dtw_3, s_softdtw_3 , s_dtw_4, s_softdtw_4,  s_dtw_5, s_softdtw_5, s_dtw_6, s_softdtw_6, s_dtw_7, s_softdtw_7],'metric' : ['dtw','softdtw','dtw','softdtw','dtw','softdtw','dtw','softdtw','dtw','softdtw','dtw','softdtw'], 'n_clusters' : [2,2,3,3,4,4,5,5,6,6,7,7]})\n",
    "silhouette_score_table = silhouette_score.pivot(index='n_clusters', columns='metric', values='silhouette_score')\n",
    "silhouette_score_table.plot(title='silhouette_score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeriesScalerMinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "sm_sample_values_resample_scaled = TimeSeriesScalerMinMax().fit_transform(sm_sample_values_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_km_dtw = TimeSeriesKMeans(n_clusters=2, metric=\"dtw\")\n",
    "scaled_labels_dtw = scaled_km_dtw.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_bis = TimeSeriesKMeans(n_clusters=2, metric=\"softdtw\")\n",
    "scaled_labels_bis = scaled_km_bis.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_euc = TimeSeriesKMeans(n_clusters=2, metric=\"euclidean\")\n",
    "scaled_labels_euc = scaled_km_euc.fit_predict(sm_sample_values_resample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_km_dtw_3 = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\")\n",
    "scaled_labels_dtw_3 = scaled_km_dtw_3.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_bis_3 = TimeSeriesKMeans(n_clusters=3, metric=\"softdtw\")\n",
    "scaled_labels_bis_3 = scaled_km_bis_3.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_euc_3 = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\")\n",
    "scaled_labels_euc_3 = scaled_km_euc_3.fit_predict(sm_sample_values_resample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_km_dtw_4 = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\")\n",
    "scaled_labels_dtw_4 = scaled_km_dtw_4.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_bis_4 = TimeSeriesKMeans(n_clusters=4, metric=\"softdtw\")\n",
    "scaled_labels_bis_4 = scaled_km_bis_4.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_euc_4 = TimeSeriesKMeans(n_clusters=4, metric=\"euclidean\")\n",
    "scaled_labels_euc_4 = scaled_km_euc_4.fit_predict(sm_sample_values_resample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_km_dtw_5 = TimeSeriesKMeans(n_clusters=5, metric=\"dtw\")\n",
    "scaled_labels_dtw_5 = scaled_km_dtw_5.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_bis_5 = TimeSeriesKMeans(n_clusters=5, metric=\"softdtw\")\n",
    "scaled_labels_bis_5 = scaled_km_bis_5.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_euc_5 = TimeSeriesKMeans(n_clusters=5, metric=\"euclidean\")\n",
    "scaled_labels_euc_5 = scaled_km_euc_5.fit_predict(sm_sample_values_resample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_km_dtw_6 = TimeSeriesKMeans(n_clusters=6, metric=\"dtw\")\n",
    "scaled_labels_dtw_6 = scaled_km_dtw_6.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_bis_6 = TimeSeriesKMeans(n_clusters=6, metric=\"softdtw\")\n",
    "scaled_labels_bis_6 = scaled_km_bis_6.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_euc_6 = TimeSeriesKMeans(n_clusters=6, metric=\"euclidean\")\n",
    "scaled_labels_euc_6 = scaled_km_euc_6.fit_predict(sm_sample_values_resample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_km_dtw_7 = TimeSeriesKMeans(n_clusters=7, metric=\"dtw\")\n",
    "scaled_labels_dtw_7 = scaled_km_dtw_7.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_bis_7 = TimeSeriesKMeans(n_clusters=7, metric=\"softdtw\")\n",
    "scaled_labels_bis_7 = scaled_km_bis_7.fit_predict(sm_sample_values_resample_scaled)\n",
    "scaled_km_euc = TimeSeriesKMeans(n_clusters=7, metric=\"euclidean\")\n",
    "scaled_labels_euc_7 = scaled_km_euc_7.fit_predict(sm_sample_values_resample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\",\"dtw\", \"softdtw\", \"euclidean\"]\n",
    "ncluster = [2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7]\n",
    "labels_minmax_kmeans_samples = pd.DataFrame([scaled_labels_dtw, scaled_labels_bis, scaled_labels_euc, scaled_labels_dtw_3, scaled_labels_bis_3, scaled_labels_euc_3, scaled_labels_dtw_4, scaled_labels_bis_4, scaled_labels_euc_4, scaled_labels_dtw_5, scaled_labels_bis_5, scaled_labels_euc_5, scaled_labels_dtw_6, scaled_labels_bis_6, scaled_labels_euc_6,  scaled_labels_dtw_7, scaled_labels_bis_7, scaled_labels_euc_7, metrics, ncluster], columns=list(itertools.chain.from_iterable([['p' + str(x) for x in range(1,21)],['metric', 'n_clusters']])))\n",
    "labels_minmax_kmeans_samples_table = labels_minmax_kmeans_samples.pivot(index='n_clusters', columns='metric', values=['p' + str(x) for x in range(1,21)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sample_values_resample_1_6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(kind='hist',data=x[0]) for x in sm_sample_values_resample_1_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ried_cfm = [[z[0][1],z[0][0],z[1]] for z in zip(list_ried_center,kmeans.labels_)]\n",
    "\n",
    "df_ried = pd.DataFrame(ried_cfm, columns=['longitude', 'latitude', 'cf'])\n",
    "\n",
    "gdf_ried = gpd.GeoDataFrame(df_ried, geometry=gpd.points_from_xy(df_ried.longitude, df_ried.latitude))\n",
    "\n",
    "gdf_ried.to_file(\"clfm/ried_cfm_29_04.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "expand_tablue = list(mcolors.TABLEAU_COLORS.keys()) + ['tab:purple']\n",
    "n = np.array([expand_tablue, expand_tablue, expand_tablue], ndmin=2)\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_plot_size(width, height, plt):\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = width\n",
    "    fig_size[1] = height\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "\n",
    "change_plot_size(20, 6, plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from sklearn.cluster import KMeans\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import itertools\n",
    "from itertools import islice \n",
    "import stumpy\n",
    "import sys\n",
    "import datetime as dt\n",
    "import urllib\n",
    "import ssl\n",
    "import io\n",
    "import os\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge --yes --prefix {sys.prefix} stumpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge --yes --prefix {sys.prefix} jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextensions_configurator enable --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nb_xxx_extension enable --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable toc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths \n",
    "paths = ['C://Users//USER//Desktop//master-thesis-master//', 'D://jupy_data//', 'C://Users//USER//Desktop//Masterarbeit//DATA//master_data//', 'C:\\\\Users\\\\USER\\\\Desktop\\\\Masterarbeit\\\\DATA\\\\master_data\\\\', 'C://Users//USER//Desktop//master-thesis-master//tables//', 'C://Users//USER//Desktop//master-thesis-master//images//'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "507px",
    "width": "516px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
